{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["############################### Base Model (https://github.com/idiap/multimodal_gaze_target_prediction)\n","\"\"\" \n","## Orignal Authors\n","@inproceedings{gupta2022modular,\n","  title={A Modular Multimodal Architecture for Gaze Target Prediction: Application to Privacy-Sensitive Settings},\n","  author={Gupta, Anshul and Tafasca, Samy and Odobez, Jean-Marc},\n","  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n","  pages={5041--5050},\n","  year={2022}\n","}\n","SPDX-FileCopyrightText: 2022 Idiap Research Institute <contact@idiap.ch>\n","SPDX-FileContributor: Anshul Gupta <anshul.gupta@idiap.ch>\n","SPDX-FileContributor: Samy Tafasca <samy.tafasca@idiap.ch>\n","SPDX-License-Identifier: GPL-3.0\n","## Modification\n","<Gazed-at-object-prediction-in-retail-environment training script>\n","Copyright (C) <2022>  <Varun Tandon> <varuntandon14@gmail.com>\n","\"\"\"\n","\n","############################### TTFNet  (https://github.com/ZJULearning/ttfnet/tree/master)\n","\"\"\" \n","Copyright 2018-2019 Open-MMLab.\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License.\n","You may obtain a copy of the License at\n","\n","    http://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS,\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","See the License for the specific language governing permissions and\n","limitations under the License.\n","\"\"\"\n","\n","############################### Gaze-on-Objects (GOO) Dataet (https://github.com/upeee/GOO-GAZE2021/tree/main/dataset)\n","\"\"\" \n","Copyright [2021] [Rowel Atienza]\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License.\n","You may obtain a copy of the License at\n","\n","    http://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS,\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","See the License for the specific language governing permissions and\n","limitations under the License.\n","\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-26T11:39:10.847118Z","iopub.status.busy":"2023-04-26T11:39:10.846542Z","iopub.status.idle":"2023-04-26T11:39:14.682079Z","shell.execute_reply":"2023-04-26T11:39:14.680680Z","shell.execute_reply.started":"2023-04-26T11:39:10.847080Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torchvision import transforms\n","import torch.nn as nn\n","torch.multiprocessing.set_sharing_strategy('file_system')\n","\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","import torchvision.models as models\n","from torchvision.ops import FeaturePyramidNetwork\n","import torchvision.transforms.functional as TF\n","\n","\n","from collections import OrderedDict\n","from torch.utils.data import Dataset, DataLoader\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import cv2\n","import warnings\n","import pickle\n","from matplotlib import pyplot as plt\n","import matplotlib.patches as patches\n","import sys\n","import csv\n","from PIL import Image, ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","from pathlib import PureWindowsPath, PurePosixPath\n","from datetime import datetime\n","from numpy.linalg import norm\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","from efficientnet_pytorch import EfficientNet\n","from multimodal_gaze_target_prediction.utils import evaluation, misc"]},{"cell_type":"markdown","metadata":{},"source":["## Config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:39:26.988822Z","iopub.status.busy":"2023-04-26T11:39:26.988465Z","iopub.status.idle":"2023-04-26T11:39:26.997093Z","shell.execute_reply":"2023-04-26T11:39:26.995783Z","shell.execute_reply.started":"2023-04-26T11:39:26.988784Z"},"trusted":true},"outputs":[],"source":["# Original Image resolution for the model\n","input_resolution = 224\n","output_resolution = 64\n","\n","WIDTH,HEIGHT = 640, 480 # Original dimension of the input image\n","cone_mode = 'early'    # {'late', 'early'} fusion of person information\n","modality_dropout = True    # only used for attention model\n","pred_inout = True    # {set True for VideoAttentionTarget}\n","privacy = False     # {set True to train/test privacy-sensitive model}\n","\n","# pytorch amp to speed up training and reduce memory usage\n","use_amp = False\n","\n","modality = 'attention' # attention modality uses all 3 modalities including image, pose and depth\n","extended_model_present = True # run inference on extended model or the base gae prediction model\n","use_sparse_dataset = False # For inference on GOO-Real we can choose from sparse vs densely packed dataset images.\n","\n","human_centric_weights = 'human-centric.pt'   # give complete path to the human centric model backbone weights\n","\n","# NMS configs\n","Bbox_topk_no_of_bboxes = 3              # Maximum No. of Bboxes that are predicted\n","Bbox_confidence_score_threshold = 0.2   # topk BBoX above this threshold score are only considered \n","\n","if use_sparse_dataset:\n","    image_dir = 'path_to_the_goo_real_test_sparse_dataset_images'\n","else:\n","    image_dir = 'path_to_the_goo_real_test_dataset_images'"]},{"cell_type":"markdown","metadata":{},"source":["## Baseline (Multi modal) Gaze detection Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:39:26.999855Z","iopub.status.busy":"2023-04-26T11:39:26.999227Z","iopub.status.idle":"2023-04-26T11:39:27.058352Z","shell.execute_reply":"2023-04-26T11:39:27.057224Z","shell.execute_reply.started":"2023-04-26T11:39:26.999813Z"},"trusted":true},"outputs":[],"source":["# returns gaze cone; resnet/efficientnet + prediction head\n","class HumanCentric(nn.Module):\n","    def __init__(self, backbone = 'resnet'):\n","        super(HumanCentric, self).__init__()\n","        \n","        self.backbone = backbone\n","        self.feature_dim = 512  # the dimension of the CNN feature to represent each frame\n","        # Build Network Base\n","        if backbone == 'resnet':\n","            self.base_head = models.resnet18(pretrained=True)\n","            self.base_head = nn.Sequential(*list(self.base_head.children())[:-1])\n","        elif backbone == 'efficientnet':\n","            self.base_head = models.efficientnet_b0(pretrained=True)\n","            self.base_head = nn.Sequential(*list(self.base_head.children())[:-1])\n","        else:\n","            assert False, 'Incorrect backbone, please choose from [resnet, efficientnet]'\n","        \n","        # Build Network Head\n","        num_outputs = 2\n","        self.num_outputs = num_outputs\n","        dummy_head = torch.empty((1, 3, 224, 224))\n","        dummy_head = self.base_head(dummy_head)            \n","        '''\n","        self.head_new = nn.Sequential(\n","                        nn.Linear(dummy_head.size(1), self.feature_dim), \n","                        nn.ReLU(inplace=True),\n","                        nn.Linear(self.feature_dim, num_outputs),\n","                        nn.Tanh())\n","        ''' \n","        self.head_new = nn.Sequential(\n","                        nn.Linear(dummy_head.size(1), self.feature_dim), \n","                        nn.GELU(),\n","                        nn.Linear(self.feature_dim, num_outputs),\n","                        nn.Tanh())\n","        \n","    def forward(self, head, gaze_field):\n","        # Model output\n","        h = self.base_head(head).squeeze(dim=-1).squeeze(dim=-1) # Nx512   \n","        head_embedding = h.clone()\n","        \n","        direction = self.head_new(h) \n","        # convert to unit vector\n","        normalized_direction = direction / direction.norm(dim=1).unsqueeze(1)\n","        \n","        # generate gaze field map\n","        batch_size, channel, height, width = gaze_field.size()\n","        gaze_field = gaze_field.permute([0, 2, 3, 1]).contiguous()\n","        gaze_field = gaze_field.view([batch_size, -1, self.num_outputs])\n","        gaze_field = torch.matmul(gaze_field, normalized_direction.view([batch_size, self.num_outputs, 1]))\n","        gaze_cone = gaze_field.view([batch_size, height, width, 1])\n","        gaze_cone = gaze_cone.permute([0, 3, 1, 2]).contiguous()\n","\n","        #gaze_cone = nn.ReLU()(gaze_cone)\n","        gaze_cone = nn.GELU()(gaze_cone)\n","        return gaze_cone, normalized_direction, head_embedding\n","\n","\n","# efficientnet followed by an FPN\n","class FeatureExtractor(nn.Module):\n","    \n","    def __init__(self, backbone_name):\n","        \n","        '''\n","        args:\n","        backbone_name: name of the backbone to be used; ex. 'efficientnet-b0'\n","        '''\n","        \n","        super(FeatureExtractor, self).__init__()\n","    \n","        self.backbone = EfficientNet.from_pretrained(backbone_name)\n","        if backbone_name=='efficientnet-b3':\n","            self.fpn = FeaturePyramidNetwork([32, 48, 136, 384], 64)\n","        elif backbone_name=='efficientnet-b2':\n","            self.fpn = FeaturePyramidNetwork([24, 48, 120, 352], 64)\n","        elif backbone_name=='efficientnet-b0' or backbone_name=='efficientnet-b1':\n","            self.fpn = FeaturePyramidNetwork([24, 40, 112, 320], 64)        \n","        \n","    def forward(self, x):\n","        \n","        features = self.backbone.extract_endpoints(x)\n","        \n","        # select features to use\n","        fpn_features = OrderedDict()\n","        fpn_features['reduction_2'] = features['reduction_2']\n","        fpn_features['reduction_3'] = features['reduction_3']\n","        fpn_features['reduction_4'] = features['reduction_4']\n","        fpn_features['reduction_5'] = features['reduction_5']\n","        \n","        # upsample features from efficientnet using an FPN to generate features at (H/4, W/4) resolution\n","        features = self.fpn(fpn_features)['reduction_2']\n","        \n","        return features\n","\n","\n","# simple prediction head that takes the features and gaze cone to regress the gaze target heatmap\n","class PredictionHead(nn.Module):\n","    \n","    def __init__(self, inchannels):\n","        super(PredictionHead, self).__init__()\n","        \n","        #self.act = nn.ReLU()\n","        self.act = nn.GELU()\n","        self.conv1 = nn.Conv2d(inchannels, inchannels, 3, padding=3, dilation=3)\n","        self.bn1 = nn.BatchNorm2d(inchannels)\n","        self.conv2 = nn.Conv2d(inchannels, inchannels, 3, padding=3, dilation=3)\n","        self.bn2 = nn.BatchNorm2d(inchannels)\n","        self.conv3 = nn.Conv2d(inchannels, inchannels, 3, padding=3, dilation=3)\n","        self.bn3 = nn.BatchNorm2d(inchannels)\n","        self.conv4 = nn.Conv2d(inchannels, inchannels, 3, padding=3, dilation=3)\n","        self.bn4 = nn.BatchNorm2d(inchannels)\n","        self.conv5 = nn.Conv2d(inchannels, inchannels//2, 3, padding=3, dilation=3)\n","        self.bn5 = nn.BatchNorm2d(inchannels//2)\n","        self.conv6 = nn.Conv2d(inchannels//2, inchannels//4, 3, padding=3, dilation=3)\n","        self.bn6 = nn.BatchNorm2d(inchannels//4)\n","        self.conv7 = nn.Conv2d(inchannels//4, 1, 1)\n","\n","    def forward(self, x):\n","                \n","        # upsample the features to 64, 64\n","        x = nn.Upsample(size=(64,64), mode='bilinear', align_corners=False)(x)\n","        x = self.act(self.bn1(self.conv1(x)))\n","        \n","        # regress the heatmap\n","        x = self.act(self.bn2(self.conv2(x)))\n","        x = self.act(self.bn3(self.conv3(x)))\n","        x = self.act(self.bn4(self.conv4(x)))\n","        x = self.act(self.bn5(self.conv5(x)))\n","        x = self.act(self.bn6(self.conv6(x)))\n","        x = self.conv7(x)\n","        \n","        return x\n","        \n","# compress modality spatially\n","class CompressModality(nn.Module):\n","    \n","    def __init__(self, in_channels):\n","        super(CompressModality, self).__init__()\n","        \n","        self.act = nn.GELU()\n","        \n","        self.conv1 = nn.Conv2d(in_channels, 128, kernel_size=3, stride=2)\n","        self.bn1 = nn.BatchNorm2d(128)\n","        self.conv2 = nn.Conv2d(128, 256, kernel_size=3, stride=2)\n","        self.bn2 = nn.BatchNorm2d(256)\n","        self.conv3 = nn.Conv2d(256, 512, kernel_size=3, stride=2)\n","        self.bn3 = nn.BatchNorm2d(512)\n","    \n","    def forward(self, x):\n","        \n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.act(x)\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.act(x)\n","        x = self.conv3(x)\n","        x = self.bn3(x)\n","        x = self.act(x)\n","        x = nn.MaxPool2d(x.shape[2])(x)\n","\n","        return x.squeeze(dim=-1).squeeze(dim=-1)\n","    \n","\n","# predicts in vs out gaze; CompressModality + Linear\n","class InvsOut(nn.Module):\n","    \n","    def __init__(self, in_channels):\n","        \n","        '''\n","        args:\n","        in_channels: number of input channels\n","        '''\n","        \n","        super(InvsOut, self).__init__()\n","        self.compress_inout = CompressModality(in_channels)\n","        \"\"\"\n","        self.inout = nn.Sequential(nn.Linear(1024, 256),\n","                                   nn.ReLU(),\n","                                   nn.Linear(256, 1),\n","                                   nn.Sigmoid())\n","        \"\"\"\n","        self.inout = nn.Sequential(nn.Linear(1024, 256),\n","                                   nn.GELU(),\n","                                   nn.Linear(256, 1),\n","                                   nn.Sigmoid())\n","    \n","    def forward(self, x, head_embedding):\n","        \n","        x = self.compress_inout(x)\n","        x = torch.cat([x, head_embedding], axis=1)\n","        x = self.inout(x)\n","        \n","        return x\n","    \n","\n","# baseline model that takes a single modality and the gaze cone as input to predict a gaze target heatmap\n","class BaselineModel(nn.Module):\n","    \n","    def __init__(self, backbone_name, modality, cone_mode='early', pred_inout=False):\n","        \n","        '''\n","        args:\n","        backbone_name: name of the backbone to be used; ex. 'efficientnet-b0'\n","        cone_mode: early or late fusion of person information {'early', 'late'}\n","        pred_inout: predict an in vs out of frame gaze label\n","        '''\n","        \n","        super(BaselineModel, self).__init__()\n","        self.feature_extractor = FeatureExtractor(backbone_name)    \n","        self.prediction_head = PredictionHead(64)\n","        self.human_centric = HumanCentric()\n","        # load weights\n","        state_dict = torch.load(human_centric_weights)['model_state_dict']\n","        self.human_centric.load_state_dict(state_dict, strict=False)\n","\n","        # add additional channels\n","        self.cone_mode = cone_mode\n","        if cone_mode=='early':\n","            input_layer = self.feature_extractor.backbone._conv_stem.weight\n","            self.feature_extractor.backbone._conv_stem.weight = torch.nn.Parameter(torch.cat([input_layer.clone(), input_layer.clone()[:,0:2,:,:]], axis=1))\n","        elif cone_mode=='late':\n","            self.cat_conv = nn.Conv2d(66, 64, 3, padding=1)\n","            \n","        # drop additional channels\n","        if modality == 'depth':\n","            self.feature_extractor.backbone._conv_stem.weight = torch.nn.Parameter(self.feature_extractor.backbone._conv_stem.weight[:,0:-2,:,:])\n","        \n","        self.pred_inout = pred_inout\n","        if pred_inout:\n","            self.in_vs_out_head = InvsOut(64)\n","    \n","    def forward(self, img, face, gaze_field, head_mask):\n","        \n","        # dummy predictions\n","        batch_size = img.shape[0]\n","        in_vs_out = torch.zeros(batch_size).cuda()\n","        direction = torch.zeros(batch_size, 2).cuda()\n","        \n","        # get gaze cone\n","        gaze_cone, direction, head_embedding = self.human_centric(face, gaze_field)\n","                \n","        if self.cone_mode=='early':\n","            x = torch.cat([img, gaze_cone, head_mask], dim=1)\n","        else:\n","            x = img\n","        \n","        # extract the features\n","        x = self.feature_extractor(x)\n","        \n","        if self.cone_mode=='late':\n","            x = torch.cat([x, gaze_cone, head_mask], dim=1)\n","            x = self.cat_conv(x)\n","            \n","        # apply the prediction head to get the heatmap\n","        hm = self.prediction_head(x)\n","        \n","        # apply the in vs out head\n","        if self.pred_inout:\n","            in_vs_out = self.in_vs_out_head(x, head_embedding)\n","        \n","        return hm, direction, in_vs_out, x\n","\n","# attention based model. multiple modalities processed separately. output feature maps are weighted and added using predicted attention weights to predict a gaze target heatmap\n","class AttentionModelCombined(nn.Module):\n","    \n","    def __init__(self, cone_mode='early', pred_inout=False):\n","        \n","        '''\n","        args:\n","        cone_mode: early or late fusion of person information {'early', 'late'}\n","        pred_inout: predict an in vs out of frame gaze label\n","        '''\n","        \n","        super(AttentionModelCombined, self).__init__()\n","        self.feature_extractor_image = FeatureExtractor('efficientnet-b1')\n","        self.feature_extractor_depth = FeatureExtractor('efficientnet-b0')\n","        self.feature_extractor_pose = FeatureExtractor('efficientnet-b0')\n","        \n","        num_modalities = 3\n","        \n","        self.bn_image = nn.BatchNorm2d(64)\n","        self.bn_depth = nn.BatchNorm2d(64)\n","        self.bn_pose = nn.BatchNorm2d(64)\n","        \n","        additional_channels = 0\n","        if cone_mode=='late':\n","            additional_channels = 2\n","        self.Wv_image = nn.Conv2d(64+additional_channels, 64, kernel_size=3, padding=1)\n","        self.Wv_depth = nn.Conv2d(64+additional_channels, 64, kernel_size=3, padding=1)\n","        self.Wv_pose = nn.Conv2d(64+additional_channels, 64, kernel_size=3, padding=1)\n","        \n","        self.compress_image = CompressModality(64)\n","        self.compress_depth = CompressModality(64)\n","        self.compress_pose = CompressModality(64)\n","        self.attention_layer = nn.Sequential(nn.Linear(512*num_modalities, num_modalities),\n","                                             nn.Softmax()\n","                                             )\n","        \n","        self.human_centric = HumanCentric()\n","        # load weights\n","        state_dict = torch.load(human_centric_weights)['model_state_dict']\n","        self.human_centric.load_state_dict(state_dict, strict=False)\n","            \n","        # add additional channels\n","        self.cone_mode = cone_mode\n","        if cone_mode=='early':\n","            input_layer = self.feature_extractor_image.backbone._conv_stem.weight\n","            self.feature_extractor_image.backbone._conv_stem.weight = torch.nn.Parameter(torch.cat([input_layer.clone(), input_layer.clone()[:,0:2,:,:]], axis=1))\n","            input_layer = self.feature_extractor_depth.backbone._conv_stem.weight\n","            self.feature_extractor_depth.backbone._conv_stem.weight = torch.nn.Parameter(torch.cat([input_layer.clone(), input_layer.clone()[:,0:2,:,:]], axis=1))\n","            input_layer = self.feature_extractor_pose.backbone._conv_stem.weight\n","            self.feature_extractor_pose.backbone._conv_stem.weight = torch.nn.Parameter(torch.cat([input_layer.clone(), input_layer.clone()[:,0:2,:,:]], axis=1))\n","        \n","        # drop additional channels\n","        self.feature_extractor_depth.backbone._conv_stem.weight = torch.nn.Parameter(self.feature_extractor_depth.backbone._conv_stem.weight[:,0:-2,:,:])\n","        \n","        self.prediction_head = PredictionHead(64)\n","        self.output_act = nn.ReLU()\n","        \n","        self.pred_inout = pred_inout\n","        if pred_inout:\n","            self.in_vs_out_head = InvsOut(64)\n","    \n","    def forward(self, x, face, gaze_field, head_mask):\n","        \n","        # dummy predictions\n","        batch_size = x[0].shape[0]\n","        in_vs_out = torch.zeros(batch_size).cuda()\n","        direction = torch.zeros(batch_size, 2).cuda()\n","                \n","        # get gaze cone\n","        gaze_cone, direction, head_embedding = self.human_centric(face, gaze_field)\n","        \n","        # extract the features\n","        if self.cone_mode=='early':\n","            x_image = torch.cat([x[0], gaze_cone, head_mask], dim=1)\n","            x_depth = torch.cat([x[1], gaze_cone, head_mask], dim=1)\n","            x_pose = torch.cat([x[2], gaze_cone, head_mask], dim=1)\n","        else:\n","            x_image = x[0]\n","            x_depth = x[1]\n","            x_pose = x[2]\n","\n","        x_image = self.feature_extractor_image(x_image)\n","        x_image = self.bn_image(x_image)\n","        x_depth = self.feature_extractor_depth(x_depth)\n","        x_depth = self.bn_depth(x_depth)\n","        x_pose = self.feature_extractor_pose(x_pose)\n","        x_pose = self.bn_pose(x_pose)\n","        \n","        if self.cone_mode=='late':\n","            x_image = torch.cat([x_image, gaze_cone, head_mask], dim=1)\n","            x_depth = torch.cat([x_depth, gaze_cone, head_mask], dim=1)\n","            x_pose = torch.cat([x_pose, gaze_cone, head_mask], dim=1)\n","        \n","        # get the values\n","        v_image = self.Wv_image(x_image)\n","        v_depth = self.Wv_depth(x_depth)\n","        v_pose = self.Wv_pose(x_pose)\n","                \n","        # get attention weights\n","        att_image = self.compress_image(v_image)\n","        att_depth = self.compress_depth(v_depth)\n","        att_pose = self.compress_pose(v_pose)\n","        att = torch.cat([att_image, att_depth, att_pose], dim=1)\n","        att = self.attention_layer(att).unsqueeze(2).unsqueeze(3).unsqueeze(4)    # add extra dimensions for weighting in the next step\n","\n","        # weight values\n","        v_image = v_image * att[:, 0]\n","        v_depth = v_depth * att[:, 1]\n","        v_pose = v_pose * att[:, 2]\n","        x = v_image + v_depth + v_pose\n","        \n","        # apply the prediction head\n","        #hm = self.prediction_head(x)\n","        hm = self.output_act(self.prediction_head(x))\n","        \n","        # apply the in vs out head\n","        if self.pred_inout:\n","            in_vs_out = self.in_vs_out_head(x, head_embedding)\n","        \n","        return hm, direction, in_vs_out, att, x\n"]},{"cell_type":"markdown","metadata":{},"source":["## Baseline Gaze detection model + TTF Net regression BBoX prediction head"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:39:27.060579Z","iopub.status.busy":"2023-04-26T11:39:27.060189Z","iopub.status.idle":"2023-04-26T11:39:27.084637Z","shell.execute_reply":"2023-04-26T11:39:27.080083Z","shell.execute_reply.started":"2023-04-26T11:39:27.060544Z"},"trusted":true},"outputs":[],"source":["class RegressionHeadBbox(nn.Module):\n","    \n","    def __init__(self, inchannels):\n","        super(RegressionHeadBbox, self).__init__()\n","        \n","        #self.act = nn.ReLU()\n","        self.act = nn.GELU()\n","        self.inchannels = inchannels\n","        self.conv1 = nn.Conv2d(inchannels, inchannels, 3, padding=1, dilation=1)\n","        self.bn1 = nn.BatchNorm2d(inchannels)\n","        self.conv2 = nn.Conv2d(inchannels, inchannels, 3, padding=1, dilation=1)\n","        self.bn2 = nn.BatchNorm2d(inchannels)\n","        self.conv3 = nn.Conv2d(inchannels, inchannels, 3, padding=1, dilation=1)\n","        self.bn3 = nn.BatchNorm2d(inchannels)\n","        self.conv4 = nn.Conv2d(inchannels, inchannels, 3, padding=1, dilation=1)\n","        self.bn4 = nn.BatchNorm2d(inchannels)\n","        self.out_conv = nn.Conv2d(inchannels, 4, 1)\n","\n","\n","    def forward(self, x):\n","                \n","        # upsample the features to 64, 64\n","        #x = nn.Upsample(size=(64,64), mode='bilinear', align_corners=False)(x)\n","        x = nn.Upsample(size=(self.inchannels,self.inchannels), mode='bilinear', align_corners=False)(x)\n","        x = self.act(self.bn1(self.conv1(x)))\n","        \n","        # regress the heatmap\n","        x = self.act(self.bn2(self.conv2(x)))\n","        x = self.act(self.bn3(self.conv3(x)))\n","        x = self.act(self.bn4(self.conv4(x)))\n","        x = self.act(self.out_conv(x))\n","        \n","        return x\n","    \n","class attentionModelBboxHead(nn.Module):\n","    def __init__(self, gaze_model, output_resolution=output_resolution):\n","        super(attentionModelBboxHead, self).__init__()\n","        self.gaze_model = gaze_model\n","        self.reg_head_bbox = RegressionHeadBbox(output_resolution)\n","        # From the paper ttfnet paper Size Regression section on page 4\n","        # this is nothing but the scalar s is a fixed scalar used \n","        # to enlarge the predicted results for easier optimization. \n","        # s = 16 is set in our experiments\n","        self.wh_offset_base = 16\n","        \n","    def forward(self,x, face, gaze_field, head_mask):\n","        \"\"\"\n","        x, face, gaze_field, head_mask:  same shape gaze prediction model\n","        \n","        Outputs:\n","        outr: shape (1,2,64,64)\n","        \"\"\"\n","        \n","        hm, direction, in_vs_out, att,x_out = self.gaze_model(x, face, gaze_field, head_mask)    \n","        outr = self.reg_head_bbox(x_out) * self.wh_offset_base\n","        return hm, direction, in_vs_out, att, outr\n","        "]},{"cell_type":"markdown","metadata":{},"source":["## Getting Test Dataset dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:39:27.086882Z","iopub.status.busy":"2023-04-26T11:39:27.086537Z","iopub.status.idle":"2023-04-26T11:39:27.376452Z","shell.execute_reply":"2023-04-26T11:39:27.375379Z","shell.execute_reply.started":"2023-04-26T11:39:27.086848Z"},"trusted":true},"outputs":[],"source":["obj_test = pd.read_pickle('path_to_goo_real_test_dataset_pickle_file', compression='infer')\n","if use_sparse_dataset:\n","    obj_test = pd.read_pickle('path_to_goo_real_test_sparse_dataset_pickle_file', compression='infer')\n","df_rg_test = pd.DataFrame.from_records(obj_test)\n","column_names = ['filename', 'left_eye_x', 'left_eye_y', 'right_eye_x', 'right_eye_y', 'pose_min_x', 'pose_min_y', 'pose_max_x', 'pose_max_y']\n","csv_path_test = 'path_to_goo_real_test_pose_dataset_csv_file'\n","if use_sparse_dataset:\n","    csv_path_test = 'path_to_goo_real_test_sparse_pose_dataset_csv_file'\n","df_pose_test = pd.read_csv(csv_path_test, sep=',', names=column_names, index_col=False, encoding=\"utf-8-sig\")\n","df_test = pd.merge(df_rg_test, df_pose_test, on=\"filename\")\n","print(len(df_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:39:27.383192Z","iopub.status.busy":"2023-04-26T11:39:27.380821Z","iopub.status.idle":"2023-04-26T11:39:27.608085Z","shell.execute_reply":"2023-04-26T11:39:27.607178Z","shell.execute_reply.started":"2023-04-26T11:39:27.383155Z"},"trusted":true},"outputs":[],"source":["df_test.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:39:27.614533Z","iopub.status.busy":"2023-04-26T11:39:27.612310Z","iopub.status.idle":"2023-04-26T11:39:27.657876Z","shell.execute_reply":"2023-04-26T11:39:27.656908Z","shell.execute_reply.started":"2023-04-26T11:39:27.614494Z"},"trusted":true},"outputs":[],"source":["WIDTH_gazeutils, HEIGHT_gazeutils = 960, 720\n","def generate_data_field(eye_point, width=WIDTH_gazeutils, height=HEIGHT_gazeutils):\n","    \"\"\"eye_point is (x, y) and between 0 and 1\"\"\"\n","    x_grid = np.array(range(width)).reshape([1, width]).repeat(height, axis=0)\n","    y_grid = np.array(range(height)).reshape([height, 1]).repeat(width, axis=1)\n","    grid = np.stack((x_grid, y_grid)).astype(np.float32)\n","\n","    x, y = eye_point\n","    x, y = x * width, y * height\n","\n","    grid -= np.array([x, y]).reshape([2, 1, 1]).astype(np.float32)\n","    grid[0] = grid[0] / width\n","    grid[1] = grid[1] / height\n","#     norm = np.sqrt(np.sum(grid ** 2, axis=0)).reshape([1, height, width])\n","#     # avoid zero norm\n","#     norm = np.maximum(norm, 0.1)\n","#     grid /= norm\n","    return grid\n","\n","\n","def generate_gaze_cone(gaze_field, normalized_direction, width=WIDTH_gazeutils, height=HEIGHT_gazeutils):\n","        \n","    gaze_field = np.ascontiguousarray(gaze_field.transpose([1, 2, 0]))\n","    gaze_field = gaze_field.reshape([-1, 2])\n","    gaze_field = np.matmul(gaze_field, normalized_direction.reshape([2, 1]))\n","    gaze_field_map = gaze_field.reshape([height, width, 1])\n","    gaze_field_map = np.ascontiguousarray(gaze_field_map.transpose([2, 0, 1]))\n","    \n","    gaze_field_map = gaze_field_map * (gaze_field_map > 0).astype(int)\n","    \n","    return gaze_field_map.squeeze()\n","\n","def _get_transform():\n","    transform_list = []\n","    transform_list.append(transforms.Resize((input_resolution, input_resolution)))\n","    transform_list.append(transforms.ToTensor())\n","    transform_list.append(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n","    return transforms.Compose(transform_list)\n","\n","def _get_transform_modality():\n","    transform_list = []\n","    transform_list.append(transforms.Resize((input_resolution, input_resolution)))\n","    transform_list.append(transforms.ToTensor())\n","    return transforms.Compose(transform_list)\n","\n","def _get_object_transform():\n","    transform_list = []\n","    transform_list.append(transforms.Resize((256,256)))\n","    transform_list.append(transforms.ToTensor())\n","    transform_list.append(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n","    return transforms.Compose(transform_list)\n","\n","def unnorm(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n","    std = np.array(std).reshape(3,1,1)\n","    mean = np.array(mean).reshape(3,1,1)\n","    return img * std + mean\n","\n","def get_head_box_channel(x_min, y_min, x_max, y_max, width, height, resolution, coordconv=False):\n","    head_box = np.array([x_min/width, y_min/height, x_max/width, y_max/height])*resolution\n","    head_box = head_box.astype(int)\n","    head_box = np.clip(head_box, 0, resolution-1)\n","    if coordconv:\n","        unit = np.array(range(0,resolution), dtype=np.float32)\n","        head_channel = []\n","        for i in unit:\n","            head_channel.append([unit+i])\n","        head_channel = np.squeeze(np.array(head_channel)) / float(np.max(head_channel))\n","        head_channel[head_box[1]:head_box[3],head_box[0]:head_box[2]] = 0\n","    else:\n","        head_channel = np.zeros((resolution,resolution), dtype=np.float32)\n","        head_channel[head_box[1]:head_box[3],head_box[0]:head_box[2]] = 1\n","    head_channel = torch.from_numpy(head_channel)\n","    return head_channel\n","\n","def to_numpy(tensor):\n","    if torch.is_tensor(tensor):\n","        return tensor.cpu().numpy()\n","    elif type(tensor).__module__ != 'numpy':\n","        raise ValueError(\"Cannot convert {} to numpy array\"\n","                         .format(type(tensor)))\n","    return tensor\n","\n","\n","def to_torch(ndarray):\n","    if type(ndarray).__module__ == 'numpy':\n","        return torch.from_numpy(ndarray)\n","    elif not torch.is_tensor(ndarray):\n","        raise ValueError(\"Cannot convert {} to torch tensor\"\n","                         .format(type(ndarray)))\n","    return ndarray\n","\n","def draw_labelmap(img, pt, sigma, type='Gaussian'):\n","    # Draw a 2D gaussian\n","    # Adopted from https://github.com/anewell/pose-hg-train/blob/master/src/pypose/draw.py\n","    img = to_numpy(img)\n","\n","    # Check that any part of the gaussian is in-bounds\n","    ul = [int(pt[0] - 3 * sigma), int(pt[1] - 3 * sigma)]\n","    br = [int(pt[0] + 3 * sigma + 1), int(pt[1] + 3 * sigma + 1)]\n","    if (ul[0] >= img.shape[1] or ul[1] >= img.shape[0] or\n","            br[0] < 0 or br[1] < 0):\n","        # If not, just return the image as is\n","        return to_torch(img)\n","\n","    # Generate gaussian\n","    size = 6 * sigma + 1\n","    x = np.arange(0, size, 1, float)\n","    y = x[:, np.newaxis]\n","    x0 = y0 = size // 2\n","    # The gaussian is not normalized, we want the center value to equal 1\n","    if type == 'Gaussian':\n","        g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n","    elif type == 'Cauchy':\n","        g = sigma / (((x - x0) ** 2 + (y - y0) ** 2 + sigma ** 2) ** 1.5)\n","\n","    # Usable gaussian range\n","    g_x = max(0, -ul[0]), min(br[0], img.shape[1]) - ul[0]\n","    g_y = max(0, -ul[1]), min(br[1], img.shape[0]) - ul[1]\n","    # Image range\n","    img_x = max(0, ul[0]), min(br[0], img.shape[1])\n","    img_y = max(0, ul[1]), min(br[1], img.shape[0])\n","\n","    img[img_y[0]:img_y[1], img_x[0]:img_x[1]] += g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n","    img = img/np.max(img) # normalize heatmap so it has max value of 1\n","    return to_torch(img)\n","\n","def multi_hot_targets(gaze_pts, out_res):\n","    w, h = out_res\n","    target_map = np.zeros((h, w))\n","    if gaze_pts[0] >= 0:\n","        x, y = map(int,[gaze_pts[0]*w.float(), gaze_pts[1]*h.float()])\n","        x = min(x, w-1)\n","        y = min(y, h-1)\n","        target_map[y, x] = 1\n","    return target_map"]},{"cell_type":"markdown","metadata":{},"source":["## TTFnet regression utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:39:27.667175Z","iopub.status.busy":"2023-04-26T11:39:27.664931Z","iopub.status.idle":"2023-04-26T11:39:27.713116Z","shell.execute_reply":"2023-04-26T11:39:27.712049Z","shell.execute_reply.started":"2023-04-26T11:39:27.667140Z"},"trusted":true},"outputs":[],"source":["# ttfnet regression \n","def bbox_areas(bboxes, keep_axis=False):\n","    x_min, y_min, x_max, y_max = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n","    areas = (y_max - y_min + 1) * (x_max - x_min + 1)\n","    if keep_axis:\n","        return areas[:, None]\n","    return areas\n","\n","def calc_region(bbox, ratio, featmap_size=None):\n","    \"\"\"Calculate a proportional bbox region.\n","    The bbox center are fixed and the new h' and w' is h * ratio and w * ratio.\n","    Args:\n","        bbox (Tensor): Bboxes to calculate regions, shape (n, 4)\n","        ratio (float): Ratio of the output region.\n","        featmap_size (tuple): Feature map size used for clipping the boundary.\n","    Returns:\n","        tuple: x1, y1, x2, y2\n","    \"\"\"\n","    x1 = torch.round((1 - ratio) * bbox[0] + ratio * bbox[2]).long()\n","    y1 = torch.round((1 - ratio) * bbox[1] + ratio * bbox[3]).long()\n","    x2 = torch.round(ratio * bbox[0] + (1 - ratio) * bbox[2]).long()\n","    y2 = torch.round(ratio * bbox[1] + (1 - ratio) * bbox[3]).long()\n","    if featmap_size is not None:\n","        x1 = x1.clamp(min=0, max=featmap_size[1] - 1)\n","        y1 = y1.clamp(min=0, max=featmap_size[0] - 1)\n","        x2 = x2.clamp(min=0, max=featmap_size[1] - 1)\n","        y2 = y2.clamp(min=0, max=featmap_size[0] - 1)\n","    return (x1, y1, x2, y2)\n","\n","def gaussian_2d(shape, sigma_x=1, sigma_y=1):\n","    m, n = [(ss - 1.) / 2. for ss in shape]\n","    y, x = np.ogrid[-m:m + 1, -n:n + 1]\n","\n","    h = np.exp(-(x * x / (2 * sigma_x * sigma_x) + y * y / (2 * sigma_y * sigma_y)))\n","    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n","    return h\n","\n","def draw_truncate_gaussian(heatmap, center, h_radius, w_radius, k=1):\n","    h, w = 2 * h_radius + 1, 2 * w_radius + 1\n","    sigma_x = w / 6\n","    sigma_y = h / 6\n","    gaussian = gaussian_2d((h, w), sigma_x=sigma_x, sigma_y=sigma_y)\n","    gaussian = heatmap.new_tensor(gaussian)\n","    \n","    x, y = int(center[0]), int(center[1])\n","\n","    height, width = heatmap.shape[0:2]\n","\n","    left, right = min(x, w_radius), min(width - x, w_radius + 1)\n","    top, bottom = min(y, h_radius), min(height - y, h_radius + 1)\n","\n","    masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\n","    masked_gaussian = gaussian[h_radius - top:h_radius + bottom,\n","                      w_radius - left:w_radius + right]\n","    if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0:\n","        torch.max(masked_heatmap, masked_gaussian * k, out=masked_heatmap)\n","    return heatmap\n","\n","def target_single_image(gt_boxes, feat_shape=(64,64)):\n","    \"\"\"\n","    Args:\n","        The scale of the gt_boxes is between 0 and 1\n","        They are converted to feature space in the func.\n","        \n","        gt_boxes: tensor, tensor <=> img, (num_gt, 4).\n","        feat_shape: tuple.\n","    Returns:\n","        heatmap: tensor, tensor <=> img, (1, h, w).\n","        box_target: tensor, tensor <=> img, (4, h, w).\n","        reg_weight: tensor, same as box_target.\n","    \"\"\"\n","    wh_area_process = 'log'\n","    output_h, output_w = feat_shape\n","    heatmap_channel = 1\n","    wh_gaussian = True\n","    wh_agnostic = True\n","    down_ratio = 4\n","    \n","    alpha = 1.5\n","    beta = 1.5\n","    \n","    heatmap = gt_boxes.new_zeros((heatmap_channel, output_h, output_w))\n","    fake_heatmap = gt_boxes.new_zeros((output_h, output_w))\n","    box_target = gt_boxes.new_ones((4, output_h, output_w)) * -1\n","    reg_weight = gt_boxes.new_zeros((1, output_h, output_w),dtype=torch.float)\n","    \n","    if wh_area_process == 'log':\n","        boxes_areas_log = bbox_areas(gt_boxes).log()\n","    elif wh_area_process == 'sqrt':\n","        boxes_areas_log = bbox_areas(gt_boxes).sqrt()\n","    else:\n","        boxes_areas_log = bbox_areas(gt_boxes)\n","    boxes_area_topk_log  = boxes_areas_log\n","\n","    if wh_area_process == 'norm':\n","        boxes_area_topk_log[:] = 1.\n","\n","    \n","    # convert gt_boxes to 64 x 64 \n","    feat_gt_boxes = gt_boxes * output_h\n","    #feat_gt_boxes = torch.tensor([gt_boxes[:,0]*64/640,gt_boxes[:,1]*64/480,gt_boxes[:,2]*64/640,gt_boxes[:,3]*64/480]).unsqueeze(0)\n","    #print(feat_gt_boxes)\n","    feat_gt_boxes[:, [0, 2]] = torch.clamp(feat_gt_boxes[:, [0, 2]], min=0,\n","                                           max=output_w - 1)\n","    feat_gt_boxes[:, [1, 3]] = torch.clamp(feat_gt_boxes[:, [1, 3]], min=0,\n","                                           max=output_h - 1)\n","    feat_hs, feat_ws = (feat_gt_boxes[:, 3] - feat_gt_boxes[:, 1],\n","                        feat_gt_boxes[:, 2] - feat_gt_boxes[:, 0])\n","\n","    # we calc the center and ignore area based on the gt-boxes of the origin scale\n","    # no peak will fall between pixels\n","    ct_ints = (torch.stack([(gt_boxes[:, 0] + gt_boxes[:, 2]) / 2,\n","                            (gt_boxes[:, 1] + gt_boxes[:, 3]) / 2],\n","                           dim=1)*output_h).to(torch.int)\n","    \n","    h_radiuses_alpha = (feat_hs / 2. * alpha).int()\n","    w_radiuses_alpha = (feat_ws / 2. * alpha).int()\n","    if wh_gaussian and alpha != beta:\n","        h_radiuses_beta = (feat_hs / 2. * beta).int()\n","        w_radiuses_beta = (feat_ws / 2. * beta).int()\n","\n","    if not wh_gaussian:\n","        # calculate positive (center) regions\n","        r1 = (1 - beta) / 2\n","        ctr_x1s, ctr_y1s, ctr_x2s, ctr_y2s = calc_region(gt_boxes.transpose(0, 1), r1)\n","        ctr_x1s, ctr_y1s, ctr_x2s, ctr_y2s = [torch.round(x.float() / down_ratio).int()\n","                                              for x in [ctr_x1s, ctr_y1s, ctr_x2s, ctr_y2s]]\n","        ctr_x1s, ctr_x2s = [torch.clamp(x, max=output_w - 1) for x in [ctr_x1s, ctr_x2s]]\n","        ctr_y1s, ctr_y2s = [torch.clamp(y, max=output_h - 1) for y in [ctr_y1s, ctr_y2s]]\n","\n","    # larger boxes have lower priority than small boxes.\n","    #for k in range(boxes_ind.shape[0]):\n","    k = 0\n","    cls_id = 0\n","    #print(ct_ints[k])\n","    #print(h_radiuses_alpha[k].item())\n","    #print(w_radiuses_alpha[k].item())\n","    fake_heatmap = fake_heatmap.zero_()\n","    draw_truncate_gaussian(fake_heatmap, ct_ints[k],\n","                                h_radiuses_alpha[k].item(), w_radiuses_alpha[k].item())\n","    heatmap[cls_id] = torch.max(heatmap[cls_id], fake_heatmap)\n","\n","    if wh_gaussian:\n","        if alpha != beta:\n","            fake_heatmap = fake_heatmap.zero_()\n","            draw_truncate_gaussian(fake_heatmap, ct_ints[k],\n","                                        h_radiuses_beta[k].item(),\n","                                        w_radiuses_beta[k].item())\n","        box_target_inds = fake_heatmap > 0\n","    else:\n","        ctr_x1, ctr_y1, ctr_x2, ctr_y2 = ctr_x1s[k], ctr_y1s[k], ctr_x2s[k], ctr_y2s[k]\n","        box_target_inds = torch.zeros_like(fake_heatmap, dtype=torch.uint8)\n","        box_target_inds[ctr_y1:ctr_y2 + 1, ctr_x1:ctr_x2 + 1] = 1\n","\n","    if wh_agnostic:\n","        box_target[:, box_target_inds] = gt_boxes[k][:, None] * output_h\n","        #box_target[:, box_target_inds] = torch.tensor([abs(ct_ints[k][0] - feat_gt_boxes[k][0, None]),abs(ct_ints[k][1] - feat_gt_boxes[k][1, None]),abs(ct_ints[k][0] - feat_gt_boxes[k][2, None]),abs(ct_ints[k][1] - feat_gt_boxes[k][3, None])])[:, None]\n","\n","    else:\n","        box_target[(cls_id * 4):((cls_id + 1) * 4), box_target_inds] = gt_boxes[k][:, None] * output_h\n","\n","    if wh_gaussian:\n","        local_heatmap = fake_heatmap[box_target_inds].float()\n","        ct_div = local_heatmap.sum()\n","        local_heatmap *= boxes_area_topk_log[k]\n","        reg_weight[cls_id, box_target_inds] = local_heatmap / ct_div\n","    else:\n","        reg_weight[cls_id, box_target_inds] = \\\n","            boxes_area_topk_log[k] / box_target_inds.sum().float()\n","\n","    return heatmap, box_target, reg_weight\n","\n","def non_max_suppression_fast(boxes, overlapThresh=0.6):\n","    boxes = boxes[0]\n","    # if there are no boxes, return an empty list\n","    if len(boxes) == 0:\n","        return []\n","\n","    # initialize the list of picked indexes\n","    pick = []\n","\n","    # grab the coordinates of the bounding boxes\n","    x1 = boxes[:,0]\n","    y1 = boxes[:,1]\n","    x2 = boxes[:,2]\n","    y2 = boxes[:,3]\n","\n","    # compute the area of the bounding boxes and sort the bounding\n","    # boxes by the bottom-right y-coordinate of the bounding box\n","    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n","\n","    idxs = np.argsort(y2)\n","\n","    # keep looping while some indexes still remain in the indexes\n","    # list\n","    while len(idxs) > 0:\n","        # grab the last index in the indexes list and add the\n","        # index value to the list of picked indexes\n","        last = len(idxs) - 1\n","        i = idxs[last]\n","        pick.append(i)\n","\n","        # find the largest (x, y) coordinates for the start of\n","        # the bounding box and the smallest (x, y) coordinates\n","        # for the end of the bounding box\n","        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n","        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n","        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n","        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n","\n","        # compute the width and height of the bounding box\n","        w = np.maximum(0, xx2 - xx1 + 1)\n","        h = np.maximum(0, yy2 - yy1 + 1)\n","\n","        # compute the ratio of overlap\n","        overlap = (w * h) / area[idxs[:last]]\n","\n","        # delete all indexes from the index list that have\n","        idxs = np.delete(idxs, np.concatenate(([last],\n","            np.where(overlap > overlapThresh)[0])))\n","\n","    # return only the bounding boxes that were picked\n","    return boxes[pick]\n","\n","def simple_nms(heat, kernel=3, out_heat=None):\n","    pad = (kernel - 1) // 2\n","    hmax = nn.functional.max_pool2d(heat, (kernel, kernel), stride=1, padding=pad)\n","    keep = (hmax == heat).float()\n","    out_heat = heat if out_heat is None else out_heat\n","    return out_heat * keep\n","\n","def _topk(scores, topk):\n","    batch, cat, height, width = scores.size()\n","\n","    # both are (batch, 1, topk)\n","    topk_scores, topk_inds = torch.topk(scores.view(batch, cat, -1), topk)\n","\n","    topk_inds = topk_inds % (height * width)\n","    topk_ys = (topk_inds / width).int().float()\n","    topk_xs = (topk_inds % width).int().float()\n","\n","    # both are (batch, topk). select topk from 80*topk\n","    topk_score, topk_ind = torch.topk(topk_scores.view(batch, -1), topk)\n","    \n","    topk_clses = (topk_ind / topk).int()\n","    topk_ind = topk_ind.unsqueeze(2)\n","    topk_inds = topk_inds.view(batch, -1, 1).gather(1, topk_ind).view(batch, topk)\n","    topk_ys = topk_ys.view(batch, -1, 1).gather(1, topk_ind).view(batch, topk)\n","    topk_xs = topk_xs.view(batch, -1, 1).gather(1, topk_ind).view(batch, topk)\n","    \n","    return topk_score, topk_inds, topk_clses, topk_ys, topk_xs\n","\n","def get_bboxes_ttfnet(pred_heatmap,pred_wh,rescale=False):\n","    if pred_heatmap.shape[1]!=1:\n","        pred_heatmap = pred_heatmap.unsqueeze(1)\n","    \n","\n","    down_ratio = 1\n","\n","    batch, cat, height, width = pred_heatmap.size()\n","    #pred_heatmap = pred_heatmap.detach().sigmoid_()\n","    pred_heatmap = pred_heatmap.detach()\n","    wh = pred_wh.detach()\n","    \n","    # perform nms on heatmaps\n","    heat = simple_nms(pred_heatmap)  # used maxpool to filter the max score\n","   \n","\n","    topk = Bbox_topk_no_of_bboxes\n","    scores, inds, clses, ys, xs = _topk(heat, topk=topk)\n","   \n","    xs = xs.view(batch, topk, 1) * down_ratio\n","    ys = ys.view(batch, topk, 1) * down_ratio\n","    \n","    #print(scores, inds, clses, xs, ys) \n","    \n","    wh = wh.permute(0, 2, 3, 1).contiguous()\n","    wh = wh.view(wh.size(0), -1, wh.size(3))\n","    \n","    inds = inds.unsqueeze(2).expand(inds.size(0), inds.size(1), wh.size(2))\n","    \n","    #print(inds)\n","    wh = wh.gather(1, inds)\n","    wh = wh.view(batch, topk, 4)\n","    \n","    clses = clses.view(batch, topk, 1).float()\n","    scores = scores.view(batch, topk, 1)\n","    \n","    bboxes = torch.cat([xs - wh[..., [0]], ys - wh[..., [1]],\n","                        xs + wh[..., [2]], ys + wh[..., [3]]], dim=2)\n","    #print(bboxes)\n","    result_list = []\n","    score_thr = Bbox_confidence_score_threshold\n","    #print(bboxes.shape[0])\n","    for batch_i in range(bboxes.shape[0]):\n","        scores_per_img = scores[batch_i]\n","        scores_keep = (scores_per_img > score_thr).squeeze(-1)\n","\n","        scores_per_img = scores_per_img[scores_keep]\n","        bboxes_per_img = bboxes[batch_i][scores_keep]\n","        labels_per_img = clses[batch_i][scores_keep]\n","        img_shape = [output_resolution, output_resolution]\n","        bboxes_per_img[:, 0::2] = bboxes_per_img[:, 0::2].clamp(min=0, max=img_shape[1] - 1)\n","        bboxes_per_img[:, 1::2] = bboxes_per_img[:, 1::2].clamp(min=0, max=img_shape[0] - 1)\n","        \n","        if rescale:\n","            scale_factor = img_metas[batch_i]['scale_factor']\n","            bboxes_per_img /= bboxes_per_img.new_tensor(scale_factor)\n","\n","        bboxes_per_img = torch.cat([bboxes_per_img, scores_per_img], dim=1)\n","        labels_per_img = labels_per_img.squeeze(-1)\n","        result_list.append((bboxes_per_img, labels_per_img))\n","\n","    return result_list\n","\n","def bbox_overlaps_ttfnet(bboxes1, bboxes2, mode='iou', is_aligned=False):\n","    \"\"\"Calculate overlap between two set of bboxes.\n","    If ``is_aligned`` is ``False``, then calculate the ious between each bbox\n","    of bboxes1 and bboxes2, otherwise the ious between each aligned pair of\n","    bboxes1 and bboxes2.\n","    Args:\n","        bboxes1 (Tensor): shape (m, 4)\n","        bboxes2 (Tensor): shape (n, 4), if is_aligned is ``True``, then m and n\n","            must be equal.\n","        mode (str): \"iou\" (intersection over union) or iof (intersection over\n","            foreground).\n","    Returns:\n","        ious(Tensor): shape (m, n) if is_aligned == False else shape (m, 1)\n","    \"\"\"\n","\n","    assert mode in ['iou', 'iof']\n","\n","    rows = bboxes1.size(0)\n","    cols = bboxes2.size(0)\n","    if is_aligned:\n","        assert rows == cols\n","\n","    if rows * cols == 0:\n","        return bboxes1.new(rows, 1) if is_aligned else bboxes1.new(rows, cols)\n","\n","    if is_aligned:\n","        lt = torch.max(bboxes1[:, :2], bboxes2[:, :2])  # [rows, 2]\n","        rb = torch.min(bboxes1[:, 2:], bboxes2[:, 2:])  # [rows, 2]\n","\n","        wh = (rb - lt + 1).clamp(min=0)  # [rows, 2]\n","        overlap = wh[:, 0] * wh[:, 1]\n","        area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (\n","            bboxes1[:, 3] - bboxes1[:, 1] + 1)\n","\n","        if mode == 'iou':\n","            area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (\n","                bboxes2[:, 3] - bboxes2[:, 1] + 1)\n","            ious = overlap / (area1 + area2 - overlap)\n","        else:\n","            ious = overlap / area1\n","    else:\n","        lt = torch.max(bboxes1[:, None, :2], bboxes2[:, :2])  # [rows, cols, 2]\n","        rb = torch.min(bboxes1[:, None, 2:], bboxes2[:, 2:])  # [rows, cols, 2]\n","\n","        wh = (rb - lt + 1).clamp(min=0)  # [rows, cols, 2]\n","        overlap = wh[:, :, 0] * wh[:, :, 1]\n","        area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (\n","            bboxes1[:, 3] - bboxes1[:, 1] + 1)\n","        \n","        if mode == 'iou':\n","            area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (\n","                bboxes2[:, 3] - bboxes2[:, 1] + 1)\n","            \n","            a_xmin = torch.min(bboxes1[:,0], bboxes2[:,0])\n","            a_ymin = torch.min(bboxes1[:,1], bboxes2[:,1])\n","            a_xmax = torch.max(bboxes1[:,2], bboxes2[:,2])\n","            a_ymax = torch.max(bboxes1[:,3], bboxes2[:,3])\n","            a_box = (a_xmax - a_xmin + 1) * (a_ymax - a_ymin + 1)\n","\n","            w = torch.min(area1 / area2, area2 / area1)\n","\n","            ious = overlap / (area1[:, None] + area2 - overlap)\n","            wuocs = (w[:,None] * ((area1[:, None] + area2 - overlap) / a_box[:, None]))\n","        else:\n","            ious = overlap / (area1[:, None])\n","    \n","    return ious,wuocs"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset Class"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:39:27.777349Z","iopub.status.busy":"2023-04-26T11:39:27.774985Z","iopub.status.idle":"2023-04-26T11:39:27.843106Z","shell.execute_reply":"2023-04-26T11:39:27.842287Z","shell.execute_reply.started":"2023-04-26T11:39:27.777304Z"},"trusted":true},"outputs":[],"source":["class GooRealDataset(Dataset):\n","    \n","    def __init__(self,df, get_transform, get_transform_modality, input_size=input_resolution, output_size=output_resolution,\n","                 test=False, modality='image', imshow=False):\n","    \n","        self.df_data =  df\n","        self.image_dir = image_dir\n","        self.transform = get_transform\n","        self.transform_modality = get_transform_modality\n","        self.get_object_transform = _get_object_transform()\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.imshow = imshow\n","        self.test = test\n","        self.modality = modality\n","    def __len__(self):\n","        return len(self.df_data.index)\n","    \n","    def __getitem__(self, index):\n","        gaze_inside = True\n","        \n","        row = self.df_data.iloc[index]\n","        \n","        path = PureWindowsPath(row['filename']).as_posix()\n","        filename_no_extension = os.path.splitext(os.path.basename(path))[0]\n","        subfolder_path_str = os.path.splitext(path)[0].split(\"/\")\n","        key_filename = subfolder_path_str[0]+'/'+subfolder_path_str[1]\n","        \n","        #path = row['filename']\n","        #filename_no_extension = path.split('.')[0]\n","        #key_filename = ''   \n","        img = Image.open(os.path.join(self.image_dir, path))\n","        img = img.convert('RGB').resize((640,480))\n","        \n","        width, height = img.size\n","        \n","        #gaze_x = row.gaze_cx / width\n","        #gaze_y = row.gaze_cy / height\n","        \n","        eye_x = row.hx / width\n","        eye_y = row.hy / height\n","        \n","        x_min, y_min, x_max, y_max = np.array(row['ann']['bboxes'][-1])\n","        \n","        # For Single object heatmap\n","        gaze_obj_x_min, gaze_obj_y_min, gaze_obj_x_max, gaze_obj_y_max = np.array(row['ann']['bboxes'][row.gazeIdx])\n","        gazed_object_class = np.array(row.gaze_item)\n","        all_object_bboxes = np.array(row['ann']['bboxes'])[:-1]\n","        all_object_bboxes_class = np.array(row['ann']['labels'])[:-1]\n","        \n","                \n","        # For CenterNet move the gaze point to center of the gazed at object bounding box\n","        #if self.test:\n","        gaze_x = ((gaze_obj_x_min+gaze_obj_x_max)/2)/ width\n","        gaze_y = ((gaze_obj_y_min+gaze_obj_y_max)/2)/ height\n","        \n","        # expand face bbox a bit\n","        k = 0.05\n","        x_min -= k * abs(x_max - x_min)\n","        y_min -= k * abs(y_max - y_min)\n","        x_max += k * abs(x_max - x_min)\n","        y_max += k * abs(y_max - y_min)\n","\n","        x_min, y_min, x_max, y_max = map(float, [x_min, y_min, x_max, y_max])\n","\n","        if self.test: \n","            \n","            self.pose_dir = 'goo-real-test-pose-directory'\n","            self.depth_dir = 'goo-real-test-depth-directory'\n","            if use_sparse_dataset:\n","                self.pose_dir = 'goo-real-sparse-test-pose-directory'\n","                self.depth_dir = 'goo-real-sparse-test-depth-directory'\n","        else:\n","            self.pose_dir = None\n","            self.depth_dir = None   \n","        # read pose\n","        if os.path.exists(os.path.join(self.pose_dir, key_filename,filename_no_extension+'-pose.png')):\n","            pose = Image.open(os.path.join(self.pose_dir, key_filename,filename_no_extension+'-pose.png'))\n","        else:\n","            pose = Image.open(os.path.join(self.pose_dir, key_filename,filename_no_extension+'-pose.jpg'))\n","        # read depth\n","        depth = Image.open(os.path.join(self.depth_dir,key_filename,filename_no_extension+'.png'))\n","        \n","\n","        if self.imshow:\n","            img.save(\"origin_img.jpg\")\n","\n","        if self.test:\n","            imsize = torch.IntTensor([width, height])\n","            if privacy:\n","                img = Image.fromarray(np.uint8(np.zeros((height, width, 3))*255))\n","        else:\n","            ## data augmentation               \n","                        \n","            # Jitter (expansion-only) bounding box size\n","            if np.random.random_sample() <= 0.5:\n","                k = np.random.random_sample() * 0.2\n","                x_min -= k * abs(x_max - x_min)\n","                y_min -= k * abs(y_max - y_min)\n","                x_max += k * abs(x_max - x_min)\n","                y_max += k * abs(y_max - y_min)\n","\n","            # Random Crop\n","            if np.random.random_sample() <= 0.5:\n","                # Calculate the minimum valid range of the crop that doesn't exclude the face and the gaze target\n","                crop_x_min = np.min([gaze_x * width, x_min, x_max])\n","                crop_y_min = np.min([gaze_y * height, y_min, y_max])\n","                crop_x_max = np.max([gaze_x * width, x_min, x_max])\n","                crop_y_max = np.max([gaze_y * height, y_min, y_max])\n","\n","                # Randomly select a random top left corner\n","                if crop_x_min >= 0:\n","                    crop_x_min = np.random.uniform(0, crop_x_min)\n","                if crop_y_min >= 0:\n","                    crop_y_min = np.random.uniform(0, crop_y_min)\n","\n","                # Find the range of valid crop width and height starting from the (crop_x_min, crop_y_min)\n","                crop_width_min = crop_x_max - crop_x_min\n","                crop_height_min = crop_y_max - crop_y_min\n","                crop_width_max = width - crop_x_min\n","                crop_height_max = height - crop_y_min\n","                # Randomly select a width and a height\n","                crop_width = np.random.uniform(crop_width_min, crop_width_max)\n","                crop_height = np.random.uniform(crop_height_min, crop_height_max)\n","\n","                # Crop it\n","                img = TF.crop(img, crop_y_min, crop_x_min, crop_height, crop_width)\n","                pose = TF.crop(pose, crop_y_min, crop_x_min, crop_height, crop_width)\n","                depth = TF.crop(depth, crop_y_min, crop_x_min, crop_height, crop_width)\n","                \n","                # Record the crop's (x, y) offset\n","                offset_x, offset_y = crop_x_min, crop_y_min\n","\n","                # convert coordinates into the cropped frame\n","                x_min, y_min, x_max, y_max = x_min - offset_x, y_min - offset_y, x_max - offset_x, y_max - offset_y\n","                # if gaze_inside:\n","                gaze_x, gaze_y = (gaze_x * width - offset_x) / float(crop_width), \\\n","                                 (gaze_y * height - offset_y) / float(crop_height)\n","                eye_x, eye_y = (eye_x * width - offset_x) / float(crop_width), \\\n","                                 (eye_y * height - offset_y) / float(crop_height)\n","                # else:\n","                #     gaze_x = -1; gaze_y = -1\n","                \n","                gaze_obj_x_min -= offset_x\n","                gaze_obj_y_min -= offset_y\n","                gaze_obj_x_max -= offset_x\n","                gaze_obj_y_max -= offset_y\n","                \n","                width, height = crop_width, crop_height\n","\n","            # Random flip\n","            if np.random.random_sample() <= 0.5:\n","                img = img.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n","                pose = pose.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n","                depth = depth.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n","                                \n","                x_max_2 = width - x_min\n","                x_min_2 = width - x_max\n","                x_max = x_max_2\n","                x_min = x_min_2\n","                gaze_x = 1 - gaze_x\n","                eye_x = 1 - eye_x\n","                \n","                # flip the GT gazed object bbox\n","                gaze_obj_x_max_2 = width - gaze_obj_x_min\n","                gaze_obj_x_min_2 = width - gaze_obj_x_max\n","                gaze_obj_x_max = gaze_obj_x_max_2\n","                gaze_obj_x_min = gaze_obj_x_min_2\n","\n","            # Random color change\n","            if np.random.random_sample() <= 0.5:\n","                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n","                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n","                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n","\n","        if cone_mode=='early':\n","            cone_resolution = input_resolution\n","        else:\n","            cone_resolution = input_resolution // 4\n","        \n","        head_channel = get_head_box_channel(x_min, y_min, x_max, y_max, width, height,\n","                                                    resolution=cone_resolution, coordconv=False).unsqueeze(0)\n","\n","        # Crop the face\n","        if privacy:\n","            face = pose.crop((int(x_min), int(y_min), int(x_max), int(y_max)))\n","        else:\n","            face = img.crop((int(x_min), int(y_min), int(x_max), int(y_max)))\n","\n","        # modality dropout\n","        height, width = int(height), int(width)\n","        num_modalities = 3\n","        dropped = np.zeros(num_modalities)\n","        if not self.test and self.modality=='attention':\n","            if modality_dropout:\n","                # keep one modality\n","                if privacy:\n","                    modality_idx = 1\n","                else:\n","                    modality_idx = 0\n","                m_keep = np.random.randint(modality_idx, num_modalities)\n","\n","                if (np.random.rand() <= 0.2) and m_keep!=0:\n","                    img = Image.fromarray(np.uint8(np.random.rand(height, width, 3)*255))\n","                    dropped[0] = 1\n","                if (np.random.rand() <= 0.2) and m_keep!=1:\n","                    depth = Image.fromarray(np.uint8(np.random.rand(height, width)*255))\n","                    dropped[1] = 1\n","                if (np.random.rand() <= 0.2) and m_keep!=2:\n","                    pose = Image.fromarray(np.uint8(np.random.rand(height, width, 3)*255))\n","                    dropped[2] = 1\n","\n","            if privacy:\n","                img = Image.fromarray(np.uint8(np.zeros((height, width, 3))))\n","                dropped[0] = 1\n","        \n","        gazed_object_bbox = np.array([gaze_obj_x_min, gaze_obj_y_min, gaze_obj_x_max, gaze_obj_y_max])\n","        \n","        # generate new gaze field (for human-centric branch)\n","        eye_point = np.array([eye_x, eye_y])\n","        gaze = np.array([gaze_x, gaze_y])\n","        gt_direction = np.array([-1.0, -1.0])\n","        if gaze_inside:\n","            gt_direction = gaze - eye_point            \n","            if gt_direction.mean()!=0:\n","                gt_direction = gt_direction / np.linalg.norm(gt_direction)\n","        \n","        gaze_field = generate_data_field(eye_point, width=cone_resolution, height=cone_resolution)\n","        # normalize\n","        norm = np.sqrt(np.sum(gaze_field ** 2, axis=0)).reshape([1, cone_resolution, cone_resolution])\n","        # avoid zero norm\n","        norm = np.maximum(norm, 0.1)\n","        gaze_field /= norm\n","          \n","        if self.transform is not None:\n","            img = self.transform(img)\n","            face = self.transform(face)\n","            \n","            pose = self.transform_modality(pose)\n","            depth = self.transform_modality(depth)\n","            #depth = depth / 65535    # depth maps are in 16 bit format\n","        \n","        ## for TTFNet\n","        gazed_object_box_for_cp = torch.tensor([gaze_obj_x_min/width,gaze_obj_y_min/height,gaze_obj_x_max/width,gaze_obj_y_max/height]).unsqueeze(0)\n","        heatmap_cp, box_target_cp, reg_weight_cp =  target_single_image(gazed_object_box_for_cp, feat_shape=(self.output_size, self.output_size)) \n","        \n","        # generate the heat map used for deconv prediction\n","        gaze_heatmap = torch.zeros(self.output_size, self.output_size)  # set the size of the output\n","        #gaze_heatmap_org = torch.zeros(self.output_size, self.output_size)  # set the size of the output\n","        if self.test:  # aggregated heatmap\n","            if gaze_x != -1:\n","                #gaze_heatmap = draw_labelmap(gaze_heatmap, [gaze_x * self.output_size, gaze_y * self.output_size], 3, type='Gaussian')\n","                gaze_heatmap = heatmap_cp.squeeze(0).float() ## for TTFNet\n","        else:\n","            #gaze_heatmap = draw_labelmap(gaze_heatmap, [gaze_x * self.output_size, gaze_y * self.output_size], 3, type='Gaussian')\n","            gaze_heatmap = heatmap_cp.squeeze(0).float() ## for TTFNet\n","        \n","        if gaze_inside:\n","            cont_gaze = [gaze_x, gaze_y]\n","        else:\n","            cont_gaze = [-1, -1]\n","        cont_gaze = torch.FloatTensor(cont_gaze)\n","        \n","        if self.imshow:\n","            fig = plt.figure(111)\n","            img = 255 - unnorm(img.numpy()) * 255\n","            img = np.clip(img, 0, 255)\n","            plt.imshow(np.transpose(img, (1, 2, 0)))\n","            plt.imshow(cv2.resize(gaze_heatmap, (self.input_size, self.input_size)), cmap='jet', alpha=0.3)\n","            plt.imshow(cv2.resize(1 - head_channel.squeeze(0), (self.input_size, self.input_size)), alpha=0.2)\n","            plt.savefig('viz_aug.png')\n","\n","        if self.test:\n","            return img, face, pose, depth, gaze_field, gt_direction, head_channel, gaze_heatmap, cont_gaze, imsize, path, eye_point, all_object_bboxes, all_object_bboxes_class, gazed_object_bbox, gazed_object_class\n","        else:\n","            return img, face, pose, depth, gaze_field, gt_direction, head_channel, gaze_heatmap, path, gaze_inside, dropped, box_target_cp, reg_weight_cp\n","\n","    "]},{"cell_type":"markdown","metadata":{},"source":["## Dataloader Sanity Check"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:39:27.910998Z","iopub.status.busy":"2023-04-26T11:39:27.908782Z","iopub.status.idle":"2023-04-26T11:39:30.224023Z","shell.execute_reply":"2023-04-26T11:39:30.222919Z","shell.execute_reply.started":"2023-04-26T11:39:27.910962Z"},"trusted":true},"outputs":[],"source":["\n","transform = _get_transform()\n","transform_modality = _get_transform_modality()\n","\n","val_dataset = GooRealDataset(df_test, transform, transform_modality, \n","                        input_size=input_resolution, output_size=output_resolution, \n","                        test=True, modality='attention' , imshow=False)\n","val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n","                                            batch_size=1,\n","                                            shuffle=True,\n","                                            num_workers=0)\n","encoded_inputs = next(iter(val_loader))\n","img, face, pose, depth, gaze_field, gt_direction, head_channel, gaze_heatmap, cont_gaze, imsize, path, eye_point, all_object_bboxes, all_object_bboxes_class, gazed_object_bbox, gazed_object_class = encoded_inputs\n","\n","plt.imshow(img.squeeze(0).numpy().transpose(1,2,0))\n","plt.show()\n","plt.imshow(depth.squeeze(0).numpy().transpose(1,2,0))\n","plt.show()\n","plt.imshow(pose.squeeze(0).numpy().transpose(1,2,0))\n","plt.show()\n","plt.imshow(face.squeeze(0).numpy().transpose(1,2,0))\n","plt.show()\n","plt.imshow(gaze_field.squeeze(0).numpy().transpose(1,2,0)[:,:,0])\n","plt.show()\n","plt.imshow(gaze_field.squeeze(0).numpy().transpose(1,2,0)[:,:,1])\n","plt.show()\n","print(gt_direction)\n","plt.imshow(head_channel.squeeze(0).numpy().transpose(1,2,0))\n","plt.show()\n","plt.imshow(gaze_heatmap.squeeze(0).numpy())\n","plt.show()\n","print(cont_gaze)\n","print(imsize)\n","print(path)\n","print(eye_point)\n","print(all_object_bboxes.shape)\n","print(all_object_bboxes_class.shape)\n","print(gazed_object_bbox)\n","print(gazed_object_class)"]},{"cell_type":"markdown","metadata":{},"source":["## Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cos_sim_func = nn.CosineSimilarity(dim=1, eps=1e-8)\n","\n","def compute_metrics(pred_hm, gt_gaze, image_size, gt_eye_point, all_object_bboxes_list, all_object_bboxes_class_list, gazed_object_class_list, eps = 1e-8):\n","    \n","    AUC = []; min_dist = []; avg_dist = []\n","    avg_ang = []\n","    count_matching_object_class = []\n","    # go through each data point and record AUC, min dist, avg dist\n","    for b_i in tqdm(range(len(gt_gaze))):\n","        # remove padding and recover valid ground truth points\n","        valid_gaze = gt_gaze[b_i]        \n","        valid_gaze = valid_gaze[valid_gaze != -1].view(-1,2)\n","        \n","        valid_eye_point = gt_eye_point[b_i]\n","        valid_eye_point = valid_eye_point[valid_eye_point != -1].view(-1,2)\n","        \n","        # AUC: area under curve of ROC\n","        pm = pred_hm[b_i]\n","        multi_hot = multi_hot_targets(gt_gaze[b_i], image_size[b_i])\n","        scaled_heatmap = cv2.resize(pm, (image_size[b_i][0].item(), image_size[b_i][1].item()))\n","        auc_score = evaluation.auc(scaled_heatmap, multi_hot)\n","        AUC.append(auc_score)\n","        # min distance: minimum among all possible pairs of <ground truth point, predicted point>\n","        pred_x, pred_y = evaluation.argmax_pts(pm)\n","        norm_p = [pred_x/float(output_resolution), pred_y/float(output_resolution)]\n","        all_distances = []\n","        for gaze in valid_gaze:\n","            all_distances.append(evaluation.L2_dist(gaze, norm_p))\n","        min_dist.append(min(all_distances))\n","        # average distance: distance between the predicted point and human average point\n","        mean_gt_gaze = torch.mean(valid_gaze, 0)\n","        avg_distance = evaluation.L2_dist(mean_gt_gaze, norm_p)\n","        avg_dist.append(avg_distance)\n","        mean_gt_gaze_direction = mean_gt_gaze - valid_eye_point\n","        mean_pred_gaze_direction = torch.tensor(norm_p) - valid_eye_point\n","        \n","        avg_ang.append(torch.rad2deg(torch.acos(cos_sim_func(mean_gt_gaze_direction,mean_pred_gaze_direction))).item())\n","        \n","        cur_all_object_bboxes_list = np.array(all_object_bboxes_list[b_i])\n","        cur_all_object_bboxes_class_list = np.array(all_object_bboxes_class_list[b_i])\n","        cur_gazed_object_class = np.array(gazed_object_class_list[b_i])\n","        count_matching_object_class.extend(\n","            match_object_cat_after_get_predicted_bbox_from_energy(\n","                scaled_heatmap,\n","                cur_all_object_bboxes_list, \n","                cur_all_object_bboxes_class_list,\n","                cur_gazed_object_class))\n","        \n","    return np.array(AUC), np.array(min_dist), np.array(avg_dist), np.abs(np.array(avg_ang)),  np.array(count_matching_object_class)\n","\n","\n","def compute_metrics_extend(pred_hm, gt_gaze, image_size, gt_eye_point, all_object_bboxes_list, \n","                    all_object_bboxes_class_list, gazed_object_class_list, \n","                    all_result_list_ttfnet, directions, gazed_object_bbox_list, eps = 1e-8):\n","    \n","    AUC = []; min_dist = []; avg_dist = []\n","    avg_ang = []\n","    avg_ang_hm = []\n","    count_matching_object_class = []\n","    count_matching_object_class_ap_50 = []\n","    avg_pred_hm_energy_in_gt_bbox = []\n","    avg_pred_hm_energy_in_gt_cat = []\n","    # go through each data point and record AUC, min dist, avg dist\n","    for b_i in tqdm(range(len(gt_gaze))):\n","        \n","        # remove padding and recover valid ground truth points\n","        valid_gaze = gt_gaze[b_i]        \n","        valid_gaze = valid_gaze[valid_gaze != -1].view(-1,2)\n","        \n","        valid_eye_point = gt_eye_point[b_i]\n","        valid_eye_point = valid_eye_point[valid_eye_point != -1].view(-1,2)\n","        \n","        valid_pred_direction = torch.tensor(directions[b_i])\n","        # AUC: area under curve of ROC\n","        pm = pred_hm[b_i]\n","        multi_hot = multi_hot_targets(gt_gaze[b_i], image_size[b_i])\n","        scaled_heatmap = cv2.resize(pm, (image_size[b_i][0].item(), image_size[b_i][1].item()))\n","        auc_score = evaluation.auc(scaled_heatmap, multi_hot)\n","        AUC.append(auc_score)\n","        # min distance: minimum among all possible pairs of <ground truth point, predicted point>\n","        pred_x, pred_y = evaluation.argmax_pts(pm)\n","        norm_p = [pred_x/float(output_resolution), pred_y/float(output_resolution)]\n","        all_distances = []\n","        for gaze in valid_gaze:\n","            all_distances.append(evaluation.L2_dist(gaze, norm_p))\n","        min_dist.append(min(all_distances))\n","        # average distance: distance between the predicted point and human average point\n","        mean_gt_gaze = torch.mean(valid_gaze, 0)\n","        avg_distance = evaluation.L2_dist(mean_gt_gaze, norm_p)\n","        avg_dist.append(avg_distance)\n","        \n","        \n","        mean_gt_gaze_direction = mean_gt_gaze - valid_eye_point\n","        mean_pred_gaze_direction = torch.tensor(norm_p) - valid_eye_point\n","        avg_ang.append(torch.rad2deg(torch.acos(cos_sim_func(mean_gt_gaze_direction,valid_pred_direction))).item())\n","        \n","        mean_pred_gaze_direction = mean_pred_gaze_direction.squeeze().numpy()\n","        mean_gt_gaze_direction = mean_gt_gaze_direction.squeeze().numpy()\n","        norm_f = (mean_pred_gaze_direction[0] ** 2 + mean_pred_gaze_direction[1] ** 2) ** 0.5\n","        norm_gt = (mean_gt_gaze_direction[0] ** 2 + mean_gt_gaze_direction[1] ** 2) ** 0.5\n","\n","        f_cos_sim = (mean_pred_gaze_direction[0] * mean_gt_gaze_direction[0] + \n","                     mean_pred_gaze_direction[1] * mean_gt_gaze_direction[1]) / \\\n","                    (norm_gt * norm_f + eps)\n","        f_cos_sim = np.maximum(np.minimum(f_cos_sim, 1.0), -1.0)\n","        f_angle = np.arccos(f_cos_sim) * 180 / np.pi\n","        avg_ang_hm.extend([f_angle])\n","                        \n","        cur_all_object_bboxes_list = np.array(all_object_bboxes_list[b_i])\n","        cur_all_object_bboxes_class_list = np.array(all_object_bboxes_class_list[b_i])\n","        cur_gazed_object_class = np.array(gazed_object_class_list[b_i])\n","        count_matching_object_class.extend(\n","            match_object_cat_after_get_predicted_bbox_from_energy(\n","                scaled_heatmap,\n","                cur_all_object_bboxes_list, \n","                cur_all_object_bboxes_class_list,\n","                cur_gazed_object_class))\n","        \n","        cur_all_result_list_ttfnet = np.array(all_result_list_ttfnet[b_i])\n","        #print(cur_all_result_list_ttfnet)\n","        count_matching_object_class_ap_50.extend(\n","            match_object_cat_from_ttffnet_regression(\n","            cur_all_result_list_ttfnet,\n","            image_size[b_i],\n","            cur_all_object_bboxes_list,\n","            cur_all_object_bboxes_class_list,\n","            cur_gazed_object_class))\n","        \n","        avg_pred_hm_energy_in_gt_bbox.extend([\n","        get_avg_energy_by_gtbox_predheatmap(\n","            gazed_object_bbox_list[b_i],\n","            scaled_heatmap)])\n","        \n","        avg_pred_hm_energy_in_gt_cat.extend([\n","        get_avg_energy_in_gtcat_predheatmap(\n","            cur_all_object_bboxes_list,\n","            scaled_heatmap,\n","            cur_gazed_object_class,\n","            cur_all_object_bboxes_class_list)])\n","        \n","            \n","    return np.array(AUC), np.array(min_dist), np.array(avg_dist), np.abs(np.array(avg_ang)),  np.array(count_matching_object_class), np.array(count_matching_object_class_ap_50), np.abs(np.array(avg_ang_hm)), np.array(avg_pred_hm_energy_in_gt_bbox), np.array(avg_pred_hm_energy_in_gt_cat)\n","\n","# Get energy aggregation loss from GaTector paper\n","def get_avg_energy_by_gtbox_predheatmap(box, heatmap, width=640, height=480):\n","    \n","    # Use ground truth box and predicted heatmap to compute the energy aggregation loss\n","    # GT bbox is passed in size (640, 480)\n","    # Check the scale factor orginal = 10 *\n","    power, total_power = 0., 0.\n","    eng = 0.\n","    cur_box = box\n","   \n","    cur_heatmap = heatmap\n","    #cur_heatmap = np.maximum(heatmap,0)\n","    # axis are flipped in the heatmap\n","    power = np.sum(cur_heatmap[cur_box[1]: cur_box[3] + 1, cur_box[0]: cur_box[2] + 1])\n","    total_power = cur_heatmap.sum()\n","    if total_power > 0:\n","        eng = (power / total_power) * 100\n","    \n","    return eng \n","\n","def get_avg_energy_in_gtcat_predheatmap(all_object_bboxes, scaled_pred_heatmap, gazed_object_class, all_object_bboxes_class, width=640, height=480):\n","    \n","    # Use ground truth boxes and predicted heatmap to compute the energy aggregation loss\n","    # Since GT Object BBoXes overlap hence it is possible that cat_power > total_power\n","    # And cat_eng becomes > 100%\n","    cat_power, total_power = 0., 0.\n","    cat_eng = 0.\n","    cat_boxes = all_object_bboxes[np.where(all_object_bboxes_class == gazed_object_class)[0]]\n","    cur_heatmap = scaled_pred_heatmap\n","    total_power = cur_heatmap.sum()\n","    \n","    #cur_heatmap = np.maximum(heatmap,0)\n","    # axis are flipped in the heatmap\n","    if total_power > 0:\n","        for index in range (len(cat_boxes)):\n","            cur_box = cat_boxes[index]\n","            cat_power += np.sum(cur_heatmap[cur_box[1]: cur_box[3] + 1, cur_box[0]: cur_box[2] + 1])\n","            \n","        cat_eng = np.minimum((cat_power / total_power) * 100 , 100)\n","    \n","    return cat_eng \n","\n","# Iterate through all GT boxes and calc mean energy of BBox. \n","# Predict the BBox with max mean energy\n","\n","def match_object_cat_after_get_predicted_bbox_from_energy(scaled_pred_heatmap,all_object_bboxes, all_object_bboxes_class, gazed_object_class):\n","    \"\"\"\n","    Use pred heatmap to find the GT object bboxes with maximum energy as gaze target\n","    \"\"\"\n","    max_energy = 0.\n","    pred_bbox = None\n","    cur_all_object_bboxes = all_object_bboxes\n","    cur_all_object_bboxes_class = all_object_bboxes_class\n","    # All Bboxes are in original image resolution\n","    cur_pred_heatmap = scaled_pred_heatmap\n","    #cur_pred_heatmap = np.maximum(scaled_pred_heatmap,0)\n","    cur_gazed_object_class = gazed_object_class\n","    \n","    for ind, cur_box in enumerate(cur_all_object_bboxes):\n","        xmin, ymin, xmax, ymax =  cur_box\n","        # axis are flipped in the heatmap\n","        no_of_pixels_in_box = (xmax+1-xmin) * (ymax+1-ymin)\n","        mean_energy = np.sum(cur_pred_heatmap[ymin: ymax + 1, xmin: xmax + 1])/(no_of_pixels_in_box)\n","        #mean_energy = torch.sum(cur_pred_heatmap[ymin: ymax + 1, xmin: xmax + 1])\n","        if mean_energy > max_energy:\n","            max_energy = mean_energy\n","            pred_bbox = cur_box\n","            pred_box_class = cur_all_object_bboxes_class[ind]\n","\n","    if pred_bbox is not None:\n","        if int(pred_box_class) == int(cur_gazed_object_class):\n","            return [1.] \n","        else:\n","            return [0.]\n","    else:\n","        return [0.]\n","    \n","def match_object_cat_from_ttffnet_regression(cur_all_result_list_ttfnet, cur_image_size, all_object_bboxes, all_object_bboxes_class, gazed_object_class):\n","    cat_wuoc = 0.0\n","    cat_ious = 0.0\n","    for ind, cur_box in enumerate(cur_all_result_list_ttfnet):\n","        \n","        # get predicted BBox in 64x64\n","        pred_gazed_object_box_from_reg = cur_box[:4]\n","        if pred_gazed_object_box_from_reg[0] == -1:\n","            break\n","        # get predicted BBox in 640x480\n","        pred_gazed_object_box_from_reg_imsize = torch.tensor([pred_gazed_object_box_from_reg[0]*cur_image_size[0]/output_resolution,\n","                                                              pred_gazed_object_box_from_reg[1]*cur_image_size[1]/output_resolution,\n","                                                              pred_gazed_object_box_from_reg[2]*cur_image_size[0]/output_resolution,\n","                                                              pred_gazed_object_box_from_reg[3]*cur_image_size[1]/output_resolution]).int()\n","        pred_gazed_object_box_from_reg_imsize = pred_gazed_object_box_from_reg_imsize.unsqueeze(dim=0)\n","        ious, wuocs = bbox_overlaps_ttfnet(torch.tensor(all_object_bboxes), pred_gazed_object_box_from_reg_imsize, mode='iou', is_aligned=False)\n","        cat_ious = (ious[np.where(all_object_bboxes_class == gazed_object_class)].sum())\n","        #print(f'cat_ious.sum() for BBoX {ind}: {cat_ious}')\n","        cat_wuoc = (wuocs[np.where(all_object_bboxes_class == gazed_object_class)].sum())\n","        #print(f'cat_wuoc.sum() for BBoX {ind}: {cat_wuoc}')\n","        if int(all_object_bboxes_class[torch.argmax(ious,axis=0)]) == int(gazed_object_class) and torch.argmax(ious,axis=0)>=0.5:\n","            return [1.]\n","    \n","    return [0.]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T11:39:30.226407Z","iopub.status.busy":"2023-04-26T11:39:30.226022Z","iopub.status.idle":"2023-04-26T11:46:42.368211Z","shell.execute_reply":"2023-04-26T11:46:42.366911Z","shell.execute_reply.started":"2023-04-26T11:39:30.226364Z"},"trusted":true},"outputs":[],"source":["def test(model_weights, val_loader, batch_size=48, device=0, mode='dict', save_path=None):\n","\n","    # Load model\n","    print(\"Constructing model\")\n","    if mode=='pt':\n","        pretrained_dict = torch.load(model_weights)\n","    elif mode=='dict':\n","        pretrained_dict = model_weights\n","    \n","    if extended_model_present:\n","        if pretrained_dict['modality'] == 'attention':\n","            model_base = AttentionModelCombined(cone_mode=pretrained_dict['cone_mode'], pred_inout=pretrained_dict['pred_inout'])\n","        else:\n","            model_base = BaselineModel(pretrained_dict['backbone_name'], pretrained_dict['modality'], cone_mode=pretrained_dict['cone_mode'], pred_inout=pretrained_dict['pred_inout'])\n","        model_base.cuda().to(device)\n","        model = attentionModelBboxHead(model_base)\n","   \n","    else:\n","        if pretrained_dict['modality'] == 'attention':\n","            model = AttentionModelCombined(cone_mode=pretrained_dict['cone_mode'], pred_inout=pretrained_dict['pred_inout'])\n","        else:\n","            model = BaselineModel(pretrained_dict['backbone_name'], pretrained_dict['modality'], cone_mode=pretrained_dict['cone_mode'], pred_inout=pretrained_dict['pred_inout'])\n","        \n","\n","    model.cuda().to(device)\n","    model_dict = model.state_dict()\n","    model_dict.update(pretrained_dict['model'])\n","    model.load_state_dict(model_dict)\n","\n","    print('Evaluation in progress ...')\n","    model.train(False)\n","    gt_gaze = []; pred_hm = []; image_size = [] ; paths = []; pred_att = []; directions = []\n","    gt_eye_point = []\n","    all_object_bboxes_list = []\n","    all_object_bboxes_class_list = []\n","    gazed_object_bbox_list = []\n","    gazed_object_class_list = []\n","    all_result_list_ttfnet = [] \n","    with torch.no_grad():\n","        for val_batch, (val_img, val_face, val_pose, val_depth, val_gaze_field, val_gt_direction, val_head_channel, val_gaze_heatmap, cont_gaze, imsize, path, eye_point, all_object_bboxes, all_object_bboxes_class, gazed_object_bbox, gazed_object_class) in tqdm(enumerate(val_loader), total=len(val_loader)):\n","            \n","            val_images = val_img.cuda().to(device)\n","            val_faces = val_face.cuda().to(device)\n","            val_head_channels = val_head_channel.cuda().to(device)\n","            val_gaze_fields = val_gaze_field.cuda().to(device)\n","            val_depth_maps = val_depth.cuda().to(device)\n","            val_pose_maps = val_pose.cuda().to(device)\n","            val_gt_direction = val_gt_direction.cuda().to(device)\n","            \n","            # choose input modality\n","            if pretrained_dict['modality'] == 'image':\n","                model_input = val_images\n","            elif pretrained_dict['modality'] == 'pose':\n","                model_input = val_pose_maps\n","            elif pretrained_dict['modality'] == 'depth':\n","                model_input = val_depth_maps\n","            elif pretrained_dict['modality'] == 'attention':\n","                model_input = [val_images, val_depth_maps, val_pose_maps]\n","            if pretrained_dict['modality'] == 'attention':\n","                if extended_model_present:\n","                    val_gaze_heatmap_pred, direction, val_inout_pred, val_att, val_pred_wh = model(model_input, val_faces, val_gaze_fields, val_head_channels)\n","                else:\n","                    val_gaze_heatmap_pred, direction, val_inout_pred, val_att = model(model_input, val_faces, val_gaze_fields, val_head_channels)\n","                pred_att.extend(val_att.cpu().numpy())\n","            else:\n","                val_gaze_heatmap_pred, direction, val_inout_pred = model(model_input, val_faces, val_gaze_fields, val_head_channels)\n","            val_gaze_heatmap_pred = val_gaze_heatmap_pred.squeeze(1)\n","            \n","            gt_gaze.extend(cont_gaze)\n","            gt_eye_point.extend(eye_point)\n","            pred_hm.extend(val_gaze_heatmap_pred.cpu().numpy())\n","            image_size.extend(imsize)\n","            paths.extend(path)\n","            directions.extend(direction.cpu().numpy())\n","            all_object_bboxes_list.extend(all_object_bboxes.cpu().numpy().astype(int)) \n","            all_object_bboxes_class_list.extend(all_object_bboxes_class.cpu().numpy().astype(int)) \n","            \n","            gazed_object_bbox_list.extend(gazed_object_bbox.cpu().numpy().astype(int))\n","            gazed_object_class_list.extend(gazed_object_class.cpu().numpy().astype(int))\n","            if extended_model_present:\n","                \n","                result_list_ttfnet = get_bboxes_ttfnet(val_gaze_heatmap_pred,val_pred_wh)\n","            \n","                if torch.numel(result_list_ttfnet[0][0]) > 0:\n","                    all_result_list_ttfnet.extend([result_list_ttfnet[0][0].cpu().numpy()])\n","                    \n","                else:\n","                    all_result_list_ttfnet.extend([np.expand_dims(np.array([-1,-1,-1,-1,-1]), axis=0)])\n","                \n","            if val_batch% 100 == 0:\n","                #th = 0.5\n","                path = str(path[0])\n","                print(path)\n","                imsize = imsize[0].int().numpy()\n","                gt_object_bbox = gazed_object_bbox.squeeze(0).cpu().numpy().astype(int)\n","                print(gt_object_bbox)\n","                print(all_result_list_ttfnet[val_batch])\n","                print(torch.numel(result_list_ttfnet[0][0])/5)\n","                \n","                pred_object_box_ttfnet = []\n","                        \n","                img1 = Image.open(os.path.join(image_dir, path))\n","                img1 = np.array(img1.convert('RGB').resize((640,480)))\n","                img2 = Image.open(os.path.join(image_dir, path))\n","                img2 = np.array(img2.convert('RGB').resize((640,480)))\n","                print(f'In vs Out: {val_inout_pred.cpu().numpy()}')\n","                if pred_att:\n","                    print(f'Attention weights image | depth | pose: {val_att[:, 0].cpu().numpy()} | {val_att[:, 1].cpu().numpy()} | {val_att[:, 2].cpu().numpy()}')\n","                fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20, 15))\n","                ax1.axis('off')\n","                ax2.axis('off')\n","                ax1.set_title('GT',size=24,fontweight=\"bold\")\n","                ax1.imshow(img1)\n","                ax1.imshow(cv2.rectangle(img1, (gt_object_bbox[0],gt_object_bbox[1]), (gt_object_bbox[2],gt_object_bbox[3]), (0,255,0), 2))\n","                ax1.imshow(cv2.resize(val_gaze_heatmap.squeeze(0).cpu().numpy(),(WIDTH,HEIGHT)), cmap='jet', alpha=0.5)\n","                \n","                ax2.set_title('Pred',size=24,fontweight=\"bold\")\n","                ax2.imshow(img2)\n","                if torch.numel(result_list_ttfnet[0][0]) > 0:\n","                    for index in range(int(torch.numel(result_list_ttfnet[0][0])/5)):\n","                        cur_box = result_list_ttfnet[0][0].cpu().numpy()[index]\n","                        # get predicted BBox in 640x480\n","                        pred_object_box_ttfnet = torch.tensor([cur_box[0]*imsize[0]/output_resolution,\n","                                                            cur_box[1]*imsize[1]/output_resolution,\n","                                                            cur_box[2]*imsize[0]/output_resolution,\n","                                                            cur_box[3]*imsize[1]/output_resolution]).int().numpy()\n","                        ax2.imshow(cv2.rectangle(img2, (pred_object_box_ttfnet[0],pred_object_box_ttfnet[1]), (pred_object_box_ttfnet[2],pred_object_box_ttfnet[3]), (255,0,0), 2))\n","                ax2.imshow(cv2.resize(val_gaze_heatmap_pred.cpu().numpy().transpose(1,2,0), (WIDTH, HEIGHT)), cmap='jet', alpha=0.5)\n","                plt.show()\n","                \n","    \n","    if extended_model_present:\n","        AUC, min_dist, avg_dist, avg_ang, count_matching_object_class, count_matching_object_class_ap_50, avg_ang_hm, avg_pred_hm_energy_in_gt_bbox, avg_pred_hm_energy_in_gt_cat = compute_metrics_extend(pred_hm, gt_gaze, image_size, gt_eye_point, all_object_bboxes_list, all_object_bboxes_class_list, gazed_object_class_list, all_result_list_ttfnet, directions, gazed_object_bbox_list)\n","    else:\n","        AUC, min_dist, avg_dist, avg_ang, count_matching_object_class = compute_metrics(pred_hm, gt_gaze, image_size, gt_eye_point, all_object_bboxes_list, all_object_bboxes_class_list, gazed_object_class_list)\n","    if save_path is not None:\n","        output = {}\n","        if pretrained_dict['modality'] == 'attention':\n","            output['pred_att'] = pred_att\n","        output['pred_hm'] = pred_hm \n","        output['gt_gaze'] = gt_gaze\n","        output['gazed_object_bbox'] = gazed_object_bbox_list\n","        output['paths'] = paths\n","        output['AUC'] = AUC \n","        output['min_dist'] = min_dist \n","        output['avg_dist'] = avg_dist \n","        output['pred_direction'] = directions\n","        output['avg_ang'] = avg_ang\n","        output['avg_ang_hm'] = avg_ang_hm\n","        output['count_matching_object_class'] = count_matching_object_class\n","        output['avg_pred_hm_energy_in_gt_bbox'] = avg_pred_hm_energy_in_gt_bbox\n","        if extended_model_present:\n","            output['count_matching_object_class_ap_50'] = count_matching_object_class_ap_50\n","            output['avg_pred_hm_energy_in_gt_cat'] = avg_pred_hm_energy_in_gt_cat\n","            output['pred_bboxes_per_img'] = all_result_list_ttfnet\n","        with open(os.path.join(save_path, 'output_gazefollow.pkl'), 'wb') as fp:\n","            pickle.dump(output, fp)    \n","    \n","    \n","    final_AUC = np.mean(AUC)\n","    final_min_dist = np.mean(min_dist)\n","    final_avg_dist = np.mean(avg_dist)\n","    final_avg_ang = np.mean(avg_ang)\n","    final_avg_ang_hm = np.mean(avg_ang_hm)\n","    final_count_matching_object_class = np.mean(count_matching_object_class) * 100\n","    if extended_model_present:\n","        final_count_matching_object_class_ap_50 = np.mean(count_matching_object_class_ap_50) * 100\n","        final_avg_pred_hm_energy_in_gt_bbox = np.mean(avg_pred_hm_energy_in_gt_bbox)\n","        final_avg_pred_hm_energy_in_gt_cat = np.mean(avg_pred_hm_energy_in_gt_cat)\n","        \n","    if pred_att:\n","        avg_attention_weights = [sum(x) / len(x) for x in zip(*pred_att)]\n","        print(f'Avg. Attention weights image | depth | pose: {avg_attention_weights[0]} | {avg_attention_weights[1]} | {avg_attention_weights[2]}')\n","    \n","    if extended_model_present:\n","        print(\"\\tAUC:{:.4f}\\t min dist:{:.4f}\\t avg dist:{:.4f}\\t avg ang:{:.4f}\\t Object prediction Acc. (%):{:.4f}\\t BBoX head Object prediction Acc. (%):{:.4f} \\t avg ang hm:{:.4f} \\t avg_pred_hm_energy_in_gt_bbox (%): {:.4f} \\t avg_pred_hm_energy_in_gt_cat (%): {:.4f}\".format(\n","          final_AUC,\n","          final_min_dist,\n","          final_avg_dist,\n","          final_avg_ang,\n","          final_count_matching_object_class,\n","          final_count_matching_object_class_ap_50,\n","          final_avg_ang_hm,\n","          final_avg_pred_hm_energy_in_gt_bbox,\n","          final_avg_pred_hm_energy_in_gt_cat))\n","    \n","        return final_AUC, final_min_dist, final_avg_dist, final_avg_ang, final_count_matching_object_class, final_count_matching_object_class_ap_50\n","   \n","    print(\"\\tAUC:{:.4f}\\t min dist:{:.4f}\\t avg dist:{:.4f}\\t avg ang:{:.4f} \\t Object prediction Acc.:{:.4f}\".format(\n","          final_AUC,\n","          final_min_dist,\n","          final_avg_dist,\n","          final_avg_ang,\n","          final_count_matching_object_class))\n","    \n","    return final_AUC, final_min_dist, final_avg_dist, final_avg_ang, final_count_matching_object_class\n","\n","\n","\n","if __name__ == \"__main__\":\n","    \n","    model_weights = 'path-to-trained-model-weights'\n","    device = ('cuda:0' if torch.cuda.is_available() else 'cpu')\n","    print(\"Loading Data\")\n","\n","    transform = _get_transform()\n","    transform_modality = _get_transform_modality()\n","    \n","    val_dataset = GooRealDataset(df_test, transform, transform_modality, \n","                             input_size=input_resolution, output_size=output_resolution,\n","                             modality=modality, test=True)\n","    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n","                                               batch_size=1,\n","                                               shuffle=True,\n","                                               num_workers=2)\n","    cos_sim_func = nn.CosineSimilarity(dim=1, eps=1e-8)\n","    if extended_model_present:\n","        final_AUC, final_min_dist, final_avg_dist, final_avg_ang, final_count_matching_object_class, final_count_matching_object_class_ap_50 = test(model_weights, val_loader, batch_size=1, device=0, mode='pt', save_path = 'path-to-save-inference-results-dic-in-pickle-file')\n","\n","    else:\n","        final_AUC, final_min_dist, final_avg_dist, final_avg_ang,final_count_matching_object_class = test(model_weights, val_loader, batch_size=1, device=0, mode='pt', save_path = 'path-to-save-inference-results-dic-in-pickle-file')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":4}
