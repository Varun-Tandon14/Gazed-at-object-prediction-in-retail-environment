{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nimport torch.nn as nn\ntorch.multiprocessing.set_sharing_strategy('file_system')\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nimport torchvision.models as models\nfrom torchvision.ops import FeaturePyramidNetwork\nimport torchvision.transforms.functional as TF\n\n\nfrom collections import OrderedDict\nfrom torch.utils.data import Dataset, DataLoader\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport cv2\nimport warnings\nimport pickle\nfrom matplotlib import pyplot as plt\nimport matplotlib.patches as patches\nimport sys\nimport csv\nfrom PIL import Image, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nfrom pathlib import PureWindowsPath, PurePosixPath\nfrom datetime import datetime\nfrom numpy.linalg import norm\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\npackage_paths = [\n    '/kaggle/input/mm-gaze-target-prediction-weights',\n    '/kaggle/input/mm-gaze-target-prediction',\n]\n\nfor pth in package_paths:\n    sys.path.append(pth)\n    \n!cp -r /kaggle/input/mm-gaze-target-prediction/ /kaggle/working/mmgazetargetprediction/","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-26T11:39:10.846542Z","iopub.execute_input":"2023-04-26T11:39:10.847118Z","iopub.status.idle":"2023-04-26T11:39:14.682079Z","shell.execute_reply.started":"2023-04-26T11:39:10.847080Z","shell.execute_reply":"2023-04-26T11:39:14.680680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install efficientnet_pytorch\nfrom efficientnet_pytorch import EfficientNet","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:39:14.684620Z","iopub.execute_input":"2023-04-26T11:39:14.685797Z","iopub.status.idle":"2023-04-26T11:39:26.652917Z","shell.execute_reply.started":"2023-04-26T11:39:14.685746Z","shell.execute_reply":"2023-04-26T11:39:26.651793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mmgazetargetprediction.utils import evaluation, misc","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:39:26.655622Z","iopub.execute_input":"2023-04-26T11:39:26.656468Z","iopub.status.idle":"2023-04-26T11:39:26.985894Z","shell.execute_reply.started":"2023-04-26T11:39:26.656422Z","shell.execute_reply":"2023-04-26T11:39:26.984913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SPDX-FileCopyrightText: 2022 Idiap Research Institute <contact@idiap.ch>\n# SPDX-FileContributor: Anshul Gupta <anshul.gupta@idiap.ch>\n# SPDX-License-Identifier: GPL-3.0\n# =============================================================================\n# model config\n# =============================================================================\ninput_resolution = 224\noutput_resolution = 64\n\ncone_mode = 'early'    # {'late', 'early'} fusion of person information\nmodality_dropout = True    # only used for attention model\npred_inout = True    # {set True for VideoAttentionTarget}\nprivacy = False     # {set True to train/test privacy-sensitive model}\n\n# pytorch amp to speed up training and reduce memory usage\nuse_amp = False\n\nmodality = 'attention'\n#modality = 'pose'\nextended_model_present = True\nuse_sparse_dataset = False\nflag_run_on_goo_dataset = True\nhuman_centric_weights = '/kaggle/input/mm-gaze-target-prediction-new-weights/human-centric.pt'\n\nBbox_topk_no_of_bboxes = 3 \nBbox_confidence_score_threshold = 0.2\n\nif use_sparse_dataset:\n    image_dir = '/kaggle/input/goorealdataset/finalrealdatasetImgsV3Sparsed/finalrealdatasetImgsV3Sparsed'\nelse:\n    image_dir = '/kaggle/input/goorealdataset/finalrealdatasetImgsV3/finalrealdatasetImgsV3'","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:39:26.988465Z","iopub.execute_input":"2023-04-26T11:39:26.988822Z","iopub.status.idle":"2023-04-26T11:39:26.997093Z","shell.execute_reply.started":"2023-04-26T11:39:26.988784Z","shell.execute_reply":"2023-04-26T11:39:26.995783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# returns gaze cone; resnet/efficientnet + prediction head\nclass HumanCentric(nn.Module):\n    def __init__(self, backbone = 'resnet'):\n        super(HumanCentric, self).__init__()\n        \n        self.backbone = backbone\n        self.feature_dim = 512  # the dimension of the CNN feature to represent each frame\n        # Build Network Base\n        if backbone == 'resnet':\n            self.base_head = models.resnet18(pretrained=True)\n            self.base_head = nn.Sequential(*list(self.base_head.children())[:-1])\n        elif backbone == 'efficientnet':\n            self.base_head = models.efficientnet_b0(pretrained=True)\n            self.base_head = nn.Sequential(*list(self.base_head.children())[:-1])\n        else:\n            assert False, 'Incorrect backbone, please choose from [resnet, efficientnet]'\n        \n        # Build Network Head\n        num_outputs = 2\n        self.num_outputs = num_outputs\n        dummy_head = torch.empty((1, 3, 224, 224))\n        dummy_head = self.base_head(dummy_head)            \n        '''\n        self.head_new = nn.Sequential(\n                        nn.Linear(dummy_head.size(1), self.feature_dim), \n                        nn.ReLU(inplace=True),\n                        nn.Linear(self.feature_dim, num_outputs),\n                        nn.Tanh())\n        ''' \n        self.head_new = nn.Sequential(\n                        nn.Linear(dummy_head.size(1), self.feature_dim), \n                        nn.GELU(),\n                        nn.Linear(self.feature_dim, num_outputs),\n                        nn.Tanh())\n        \n    def forward(self, head, gaze_field):\n        # Model output\n        h = self.base_head(head).squeeze(dim=-1).squeeze(dim=-1) # Nx512   \n        head_embedding = h.clone()\n        \n        direction = self.head_new(h) \n        # convert to unit vector\n        normalized_direction = direction / direction.norm(dim=1).unsqueeze(1)\n        \n        # generate gaze field map\n        batch_size, channel, height, width = gaze_field.size()\n        gaze_field = gaze_field.permute([0, 2, 3, 1]).contiguous()\n        gaze_field = gaze_field.view([batch_size, -1, self.num_outputs])\n        gaze_field = torch.matmul(gaze_field, normalized_direction.view([batch_size, self.num_outputs, 1]))\n        gaze_cone = gaze_field.view([batch_size, height, width, 1])\n        gaze_cone = gaze_cone.permute([0, 3, 1, 2]).contiguous()\n\n        #gaze_cone = nn.ReLU()(gaze_cone)\n        gaze_cone = nn.GELU()(gaze_cone)\n        return gaze_cone, normalized_direction, head_embedding\n\n\n# efficientnet followed by an FPN\nclass FeatureExtractor(nn.Module):\n    \n    def __init__(self, backbone_name):\n        \n        '''\n        args:\n        backbone_name: name of the backbone to be used; ex. 'efficientnet-b0'\n        '''\n        \n        super(FeatureExtractor, self).__init__()\n    \n        self.backbone = EfficientNet.from_pretrained(backbone_name)\n        if backbone_name=='efficientnet-b3':\n            self.fpn = FeaturePyramidNetwork([32, 48, 136, 384], 64)\n        elif backbone_name=='efficientnet-b2':\n            self.fpn = FeaturePyramidNetwork([24, 48, 120, 352], 64)\n        elif backbone_name=='efficientnet-b0' or backbone_name=='efficientnet-b1':\n            self.fpn = FeaturePyramidNetwork([24, 40, 112, 320], 64)        \n        \n    def forward(self, x):\n        \n        features = self.backbone.extract_endpoints(x)\n        \n        # select features to use\n        fpn_features = OrderedDict()\n        fpn_features['reduction_2'] = features['reduction_2']\n        fpn_features['reduction_3'] = features['reduction_3']\n        fpn_features['reduction_4'] = features['reduction_4']\n        fpn_features['reduction_5'] = features['reduction_5']\n        \n        # upsample features from efficientnet using an FPN to generate features at (H/4, W/4) resolution\n        features = self.fpn(fpn_features)['reduction_2']\n        \n        return features\n\n\n# simple prediction head that takes the features and gaze cone to regress the gaze target heatmap\nclass PredictionHead(nn.Module):\n    \n    def __init__(self, inchannels):\n        super(PredictionHead, self).__init__()\n        \n        #self.act = nn.ReLU()\n        self.act = nn.GELU()\n        self.conv1 = nn.Conv2d(inchannels, inchannels, 3, padding=3, dilation=3)\n        self.bn1 = nn.BatchNorm2d(inchannels)\n        self.conv2 = nn.Conv2d(inchannels, inchannels, 3, padding=3, dilation=3)\n        self.bn2 = nn.BatchNorm2d(inchannels)\n        self.conv3 = nn.Conv2d(inchannels, inchannels, 3, padding=3, dilation=3)\n        self.bn3 = nn.BatchNorm2d(inchannels)\n        self.conv4 = nn.Conv2d(inchannels, inchannels, 3, padding=3, dilation=3)\n        self.bn4 = nn.BatchNorm2d(inchannels)\n        self.conv5 = nn.Conv2d(inchannels, inchannels//2, 3, padding=3, dilation=3)\n        self.bn5 = nn.BatchNorm2d(inchannels//2)\n        self.conv6 = nn.Conv2d(inchannels//2, inchannels//4, 3, padding=3, dilation=3)\n        self.bn6 = nn.BatchNorm2d(inchannels//4)\n        self.conv7 = nn.Conv2d(inchannels//4, 1, 1)\n\n    def forward(self, x):\n                \n        # upsample the features to 64, 64\n        x = nn.Upsample(size=(64,64), mode='bilinear', align_corners=False)(x)\n        x = self.act(self.bn1(self.conv1(x)))\n        \n        # regress the heatmap\n        x = self.act(self.bn2(self.conv2(x)))\n        x = self.act(self.bn3(self.conv3(x)))\n        x = self.act(self.bn4(self.conv4(x)))\n        x = self.act(self.bn5(self.conv5(x)))\n        x = self.act(self.bn6(self.conv6(x)))\n        x = self.conv7(x)\n        \n        return x\n        \n# compress modality spatially\nclass CompressModality(nn.Module):\n    \n    def __init__(self, in_channels):\n        super(CompressModality, self).__init__()\n        \n        self.act = nn.GELU()\n        \n        self.conv1 = nn.Conv2d(in_channels, 128, kernel_size=3, stride=2)\n        self.bn1 = nn.BatchNorm2d(128)\n        self.conv2 = nn.Conv2d(128, 256, kernel_size=3, stride=2)\n        self.bn2 = nn.BatchNorm2d(256)\n        self.conv3 = nn.Conv2d(256, 512, kernel_size=3, stride=2)\n        self.bn3 = nn.BatchNorm2d(512)\n    \n    def forward(self, x):\n        \n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.act(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.act(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.act(x)\n        x = nn.MaxPool2d(x.shape[2])(x)\n\n        return x.squeeze(dim=-1).squeeze(dim=-1)\n    \n\n# predicts in vs out gaze; CompressModality + Linear\nclass InvsOut(nn.Module):\n    \n    def __init__(self, in_channels):\n        \n        '''\n        args:\n        in_channels: number of input channels\n        '''\n        \n        super(InvsOut, self).__init__()\n        self.compress_inout = CompressModality(in_channels)\n        \"\"\"\n        self.inout = nn.Sequential(nn.Linear(1024, 256),\n                                   nn.ReLU(),\n                                   nn.Linear(256, 1),\n                                   nn.Sigmoid())\n        \"\"\"\n        self.inout = nn.Sequential(nn.Linear(1024, 256),\n                                   nn.GELU(),\n                                   nn.Linear(256, 1),\n                                   nn.Sigmoid())\n    \n    def forward(self, x, head_embedding):\n        \n        x = self.compress_inout(x)\n        x = torch.cat([x, head_embedding], axis=1)\n        x = self.inout(x)\n        \n        return x\n    \n\n# baseline model that takes a single modality and the gaze cone as input to predict a gaze target heatmap\nclass BaselineModel(nn.Module):\n    \n    def __init__(self, backbone_name, modality, cone_mode='early', pred_inout=False):\n        \n        '''\n        args:\n        backbone_name: name of the backbone to be used; ex. 'efficientnet-b0'\n        cone_mode: early or late fusion of person information {'early', 'late'}\n        pred_inout: predict an in vs out of frame gaze label\n        '''\n        \n        super(BaselineModel, self).__init__()\n        self.feature_extractor = FeatureExtractor(backbone_name)    \n        self.prediction_head = PredictionHead(64)\n        self.human_centric = HumanCentric()\n        # load weights\n        state_dict = torch.load(human_centric_weights)['model_state_dict']\n        self.human_centric.load_state_dict(state_dict, strict=False)\n\n        # add additional channels\n        self.cone_mode = cone_mode\n        if cone_mode=='early':\n            input_layer = self.feature_extractor.backbone._conv_stem.weight\n            self.feature_extractor.backbone._conv_stem.weight = torch.nn.Parameter(torch.cat([input_layer.clone(), input_layer.clone()[:,0:2,:,:]], axis=1))\n        elif cone_mode=='late':\n            self.cat_conv = nn.Conv2d(66, 64, 3, padding=1)\n            \n        # drop additional channels\n        if modality == 'depth':\n            self.feature_extractor.backbone._conv_stem.weight = torch.nn.Parameter(self.feature_extractor.backbone._conv_stem.weight[:,0:-2,:,:])\n        \n        self.pred_inout = pred_inout\n        if pred_inout:\n            self.in_vs_out_head = InvsOut(64)\n    \n    def forward(self, img, face, gaze_field, head_mask):\n        \n        # dummy predictions\n        batch_size = img.shape[0]\n        in_vs_out = torch.zeros(batch_size).cuda()\n        direction = torch.zeros(batch_size, 2).cuda()\n        \n        # get gaze cone\n        gaze_cone, direction, head_embedding = self.human_centric(face, gaze_field)\n                \n        if self.cone_mode=='early':\n            x = torch.cat([img, gaze_cone, head_mask], dim=1)\n        else:\n            x = img\n        \n        # extract the features\n        x = self.feature_extractor(x)\n        \n        if self.cone_mode=='late':\n            x = torch.cat([x, gaze_cone, head_mask], dim=1)\n            x = self.cat_conv(x)\n            \n        # apply the prediction head to get the heatmap\n        hm = self.prediction_head(x)\n        \n        # apply the in vs out head\n        if self.pred_inout:\n            in_vs_out = self.in_vs_out_head(x, head_embedding)\n        \n        return hm, direction, in_vs_out\n\n\n# attention based model. multiple modalities processed separately. output feature maps are weighted and added using predicted attention weights to predict a gaze target heatmap\nclass AttentionModelCombined(nn.Module):\n    \n    def __init__(self, cone_mode='early', pred_inout=False):\n        \n        '''\n        args:\n        cone_mode: early or late fusion of person information {'early', 'late'}\n        pred_inout: predict an in vs out of frame gaze label\n        '''\n        \n        super(AttentionModelCombined, self).__init__()\n        self.feature_extractor_image = FeatureExtractor('efficientnet-b1')\n        self.feature_extractor_depth = FeatureExtractor('efficientnet-b0')\n        self.feature_extractor_pose = FeatureExtractor('efficientnet-b0')\n        #self.feature_extractor_image = FeatureExtractor('efficientnet-b2')\n        #self.feature_extractor_depth = FeatureExtractor('efficientnet-b1')\n        #self.feature_extractor_pose = FeatureExtractor('efficientnet-b1')\n        num_modalities = 3\n        \n        self.bn_image = nn.BatchNorm2d(64)\n        self.bn_depth = nn.BatchNorm2d(64)\n        self.bn_pose = nn.BatchNorm2d(64)\n        \n        additional_channels = 0\n        if cone_mode=='late':\n            additional_channels = 2\n        self.Wv_image = nn.Conv2d(64+additional_channels, 64, kernel_size=3, padding=1)\n        self.Wv_depth = nn.Conv2d(64+additional_channels, 64, kernel_size=3, padding=1)\n        self.Wv_pose = nn.Conv2d(64+additional_channels, 64, kernel_size=3, padding=1)\n        \n        self.compress_image = CompressModality(64)\n        self.compress_depth = CompressModality(64)\n        self.compress_pose = CompressModality(64)\n        self.attention_layer = nn.Sequential(nn.Linear(512*num_modalities, num_modalities),\n                                             nn.Softmax()\n                                             )\n        \n        self.human_centric = HumanCentric()\n        # load weights\n        state_dict = torch.load(human_centric_weights)['model_state_dict']\n        self.human_centric.load_state_dict(state_dict, strict=False)\n            \n        # add additional channels\n        self.cone_mode = cone_mode\n        if cone_mode=='early':\n            input_layer = self.feature_extractor_image.backbone._conv_stem.weight\n            self.feature_extractor_image.backbone._conv_stem.weight = torch.nn.Parameter(torch.cat([input_layer.clone(), input_layer.clone()[:,0:2,:,:]], axis=1))\n            input_layer = self.feature_extractor_depth.backbone._conv_stem.weight\n            self.feature_extractor_depth.backbone._conv_stem.weight = torch.nn.Parameter(torch.cat([input_layer.clone(), input_layer.clone()[:,0:2,:,:]], axis=1))\n            input_layer = self.feature_extractor_pose.backbone._conv_stem.weight\n            self.feature_extractor_pose.backbone._conv_stem.weight = torch.nn.Parameter(torch.cat([input_layer.clone(), input_layer.clone()[:,0:2,:,:]], axis=1))\n        \n        # drop additional channels\n        self.feature_extractor_depth.backbone._conv_stem.weight = torch.nn.Parameter(self.feature_extractor_depth.backbone._conv_stem.weight[:,0:-2,:,:])\n        \n        self.prediction_head = PredictionHead(64)\n        self.output_act = nn.ReLU()\n        \n        self.pred_inout = pred_inout\n        if pred_inout:\n            self.in_vs_out_head = InvsOut(64)\n    \n    def forward(self, x, face, gaze_field, head_mask):\n        \n        # dummy predictions\n        batch_size = x[0].shape[0]\n        in_vs_out = torch.zeros(batch_size).cuda()\n        direction = torch.zeros(batch_size, 2).cuda()\n                \n        # get gaze cone\n        gaze_cone, direction, head_embedding = self.human_centric(face, gaze_field)\n        \n        # extract the features\n        if self.cone_mode=='early':\n            x_image = torch.cat([x[0], gaze_cone, head_mask], dim=1)\n            x_depth = torch.cat([x[1], gaze_cone, head_mask], dim=1)\n            x_pose = torch.cat([x[2], gaze_cone, head_mask], dim=1)\n        else:\n            x_image = x[0]\n            x_depth = x[1]\n            x_pose = x[2]\n\n        x_image = self.feature_extractor_image(x_image)\n        x_image = self.bn_image(x_image)\n        x_depth = self.feature_extractor_depth(x_depth)\n        x_depth = self.bn_depth(x_depth)\n        x_pose = self.feature_extractor_pose(x_pose)\n        x_pose = self.bn_pose(x_pose)\n        \n        if self.cone_mode=='late':\n            x_image = torch.cat([x_image, gaze_cone, head_mask], dim=1)\n            x_depth = torch.cat([x_depth, gaze_cone, head_mask], dim=1)\n            x_pose = torch.cat([x_pose, gaze_cone, head_mask], dim=1)\n        \n        # get the values\n        v_image = self.Wv_image(x_image)\n        v_depth = self.Wv_depth(x_depth)\n        v_pose = self.Wv_pose(x_pose)\n                \n        # get attention weights\n        att_image = self.compress_image(v_image)\n        att_depth = self.compress_depth(v_depth)\n        att_pose = self.compress_pose(v_pose)\n        att = torch.cat([att_image, att_depth, att_pose], dim=1)\n        att = self.attention_layer(att).unsqueeze(2).unsqueeze(3).unsqueeze(4)    # add extra dimensions for weighting in the next step\n\n        # weight values\n        v_image = v_image * att[:, 0]\n        v_depth = v_depth * att[:, 1]\n        v_pose = v_pose * att[:, 2]\n        x = v_image + v_depth + v_pose\n        \n        # apply the prediction head\n        #hm = self.prediction_head(x)\n        hm = self.output_act(self.prediction_head(x))\n        \n        # apply the in vs out head\n        if self.pred_inout:\n            in_vs_out = self.in_vs_out_head(x, head_embedding)\n        \n        #return hm, direction, in_vs_out, att, x\n        return hm, direction, in_vs_out, att, x_image","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:39:26.999227Z","iopub.execute_input":"2023-04-26T11:39:26.999855Z","iopub.status.idle":"2023-04-26T11:39:27.058352Z","shell.execute_reply.started":"2023-04-26T11:39:26.999813Z","shell.execute_reply":"2023-04-26T11:39:27.057224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RegressionHeadBbox(nn.Module):\n    \n    def __init__(self, inchannels):\n        super(RegressionHeadBbox, self).__init__()\n        \n        #self.act = nn.ReLU()\n        self.act = nn.GELU()\n        self.inchannels = inchannels\n        self.conv1 = nn.Conv2d(inchannels, inchannels, 3, padding=1, dilation=1)\n        self.bn1 = nn.BatchNorm2d(inchannels)\n        self.conv2 = nn.Conv2d(inchannels, inchannels, 3, padding=1, dilation=1)\n        self.bn2 = nn.BatchNorm2d(inchannels)\n        self.conv3 = nn.Conv2d(inchannels, inchannels, 3, padding=1, dilation=1)\n        self.bn3 = nn.BatchNorm2d(inchannels)\n        self.conv4 = nn.Conv2d(inchannels, inchannels, 3, padding=1, dilation=1)\n        self.bn4 = nn.BatchNorm2d(inchannels)\n        self.out_conv = nn.Conv2d(inchannels, 4, 1)\n\n\n    def forward(self, x):\n                \n        # upsample the features to 64, 64\n        #x = nn.Upsample(size=(64,64), mode='bilinear', align_corners=False)(x)\n        x = nn.Upsample(size=(self.inchannels,self.inchannels), mode='bilinear', align_corners=False)(x)\n        x = self.act(self.bn1(self.conv1(x)))\n        \n        # regress the heatmap\n        x = self.act(self.bn2(self.conv2(x)))\n        x = self.act(self.bn3(self.conv3(x)))\n        x = self.act(self.bn4(self.conv4(x)))\n        x = self.act(self.out_conv(x))\n        \n        return x\n    \nclass attentionModelBboxHead(nn.Module):\n    def __init__(self, gaze_model, output_resolution=output_resolution):\n        super(attentionModelBboxHead, self).__init__()\n        self.gaze_model = gaze_model\n        self.reg_head_bbox = RegressionHeadBbox(output_resolution)\n        # From the paper ttfnet paper Size Regression section on page 4\n        # this is nothing but the scalar s is a fixed scalar used \n        # to enlarge the predicted results for easier optimization. \n        # s = 16 is set in our experiments\n        self.wh_offset_base = 16\n        \n    def forward(self,x, face, gaze_field, head_mask):\n        \"\"\"\n        x, face, gaze_field, head_mask:  same shape gaze prediction model\n        \n        Outputs:\n        outr: shape (1,2,64,64)\n        \"\"\"\n        \n        hm, direction, in_vs_out, att,x_out = self.gaze_model(x, face, gaze_field, head_mask)    \n        outr = self.reg_head_bbox(x_out) * self.wh_offset_base\n        return hm, direction, in_vs_out, att, outr\n        ","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:39:27.060189Z","iopub.execute_input":"2023-04-26T11:39:27.060579Z","iopub.status.idle":"2023-04-26T11:39:27.084637Z","shell.execute_reply.started":"2023-04-26T11:39:27.060544Z","shell.execute_reply":"2023-04-26T11:39:27.080083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if flag_run_on_goo_dataset:\n    \n    \"\"\"\n    obj_test = pd.read_pickle('/kaggle/input/goorealdataset/testrealhumansNew.pickle', compression='infer')\n    if use_sparse_dataset:\n        obj_test = pd.read_pickle('/kaggle/input/goorealdataset/testrealhumansSparsedNew.pickle', compression='infer')\n    df_rg_test = pd.DataFrame.from_records(obj_test)\n    #df_test = df_rg_test\n    ## print(len(df_test))\n    # merge with pose csv file\n    ## to get eye point in retailgaze dataset\n    ## to filter frames with no pose detected both for train and inf\n    column_names = ['filename', 'left_eye_x', 'left_eye_y', 'right_eye_x', 'right_eye_y', 'pose_min_x', 'pose_min_y', 'pose_max_x', 'pose_max_y']\n    csv_path_test = '/kaggle/input/goorealposeanddepth/goo-real-test-pose/kaggle/working/goo-real-pose/master-pose.csv'\n    if use_sparse_dataset:\n        csv_path_test = '/kaggle/input/goorealposeanddepth/goo-real-sparse-test-pose/kaggle/working/goo-real-sparse-test-pose/master-pose.csv'\n    df_pose_test = pd.read_csv(csv_path_test, sep=',', names=column_names, index_col=False, encoding=\"utf-8-sig\")\n    df_test = pd.merge(df_rg_test, df_pose_test, on=\"filename\")\n    print(len(df_test))\n    \"\"\"\n    \n    \"\"\"\n    obj_test = pd.read_pickle('/kaggle/input/goorealdataset/oneshotrealhumansNew.pickle', compression='infer')\n    df_rg_test = pd.DataFrame.from_records(obj_test)\n    print(len(df_rg_test))\n    \n    # merge with pose csv file\n    ## to get eye point in retailgaze dataset\n    ## to filter frames with no pose detected both for train and inf\n    column_names = ['filename', 'left_eye_x', 'left_eye_y', 'right_eye_x', 'right_eye_y', 'pose_min_x', 'pose_min_y', 'pose_max_x', 'pose_max_y']\n    csv_path_test = '/kaggle/input/goorealposeanddepth/goo-real-train-pose/kaggle/working/goo-real-pose/master-pose.csv'\n    #csv_path_train = '/kaggle/input/goosynthtestposeanddepth/goo-synth-test-pose/kaggle/working/goo-synth-test-pose/master-pose.csv'\n    df_pose_test = pd.read_csv(csv_path_test, sep=',', names=column_names, index_col=False, encoding=\"utf-8-sig\")\n    print(len(df_pose_test))\n    df_test = pd.merge(df_rg_test, df_pose_test, on=\"filename\")\n    \n    obj_test = pd.read_pickle('/kaggle/input/goosynthtestdataset/goosynth_test_v2_no_segm.pkl', compression='infer')\n    df_rg_test = pd.DataFrame.from_records(obj_test)\n    df_rg_test = df_rg_test[['filename','width','height','ann','gaze_item','gazeIdx','gaze_cx','gaze_cy','hx','hy']]\n    print(len(df_rg_test))\n\n    # merge with pose csv file\n    ## to get eye point in retailgaze dataset\n    ## to filter frames with no pose detected both for train and inf\n    column_names = ['filename', 'left_eye_x', 'left_eye_y', 'right_eye_x', 'right_eye_y', 'pose_min_x', 'pose_min_y', 'pose_max_x', 'pose_max_y']\n    csv_path_test = '/kaggle/input/goosynthtestposeanddepth/goo-synth-test-pose/kaggle/working/goo-synth-test-pose/master-pose.csv'\n    #csv_path_train = '/kaggle/input/goosynthtestposeanddepth/goo-synth-test-pose/kaggle/working/goo-synth-test-pose/master-pose.csv'\n    df_pose_test = pd.read_csv(csv_path_test, sep=',', names=column_names, index_col=False, encoding=\"utf-8-sig\")\n    print(len(df_pose_test))\n\n    df_test = pd.merge(df_rg_test, df_pose_test, on=\"filename\")\n    image_dir = '/kaggle/input/goosynthtestdataset/goo-synth-test-images/images'\n    \"\"\"\n    obj_test = pd.read_pickle('/kaggle/input/goorealdataset/valrealhumansNew.pickle', compression='infer')\n    df_rg_test = pd.DataFrame.from_records(obj_test)\n    df_rg_test = df_rg_test[['filename','width','height','ann','gaze_item','gazeIdx','gaze_cx','gaze_cy','hx','hy']]\n    csv_path_test = '/kaggle/input/goorealposeanddepth/goo-real-val-pose/kaggle/working/goo-real-pose/master-pose.csv'\n    column_names = ['filename', 'left_eye_x', 'left_eye_y', 'right_eye_x', 'right_eye_y', 'pose_min_x', 'pose_min_y', 'pose_max_x', 'pose_max_y']\n    df_pose_test = pd.read_csv(csv_path_test, sep=',', names=column_names, index_col=False, encoding=\"utf-8-sig\")\n\n    df_test = pd.merge(df_rg_test, df_pose_test, on=\"filename\")\n    df_test['dataset_id'] = np.zeros(len(df_test),dtype=np.int8) \n    print(len(df_test))\nelse:\n    obj_test = pd.read_pickle('/kaggle/input/retailgaze/RetailGaze_V2_seg/RetailGaze_V3_2_test.pickle', compression='infer')\n    df_rg_test = pd.DataFrame.from_records(obj_test)\n    # merge with pose csv file\n    column_names = ['filename', 'left_eye_x', 'left_eye_y', 'right_eye_x', 'right_eye_y', 'pose_min_x', 'pose_min_y', 'pose_max_x', 'pose_max_y']\n    csv_path_test = '/kaggle/input/retailgazedepthandpose/retailgaze-test-pose/kaggle/working/retailgaze-pose/master-pose.csv'\n    df_pose_test = pd.read_csv(csv_path_test, sep=',', names=column_names, index_col=False, encoding=\"utf-8-sig\")\n    df_test = pd.merge(df_rg_test, df_pose_test, on=\"filename\")\n    print(len(df_test))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:39:27.086537Z","iopub.execute_input":"2023-04-26T11:39:27.086882Z","iopub.status.idle":"2023-04-26T11:39:27.376452Z","shell.execute_reply.started":"2023-04-26T11:39:27.086848Z","shell.execute_reply":"2023-04-26T11:39:27.375379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:39:27.380821Z","iopub.execute_input":"2023-04-26T11:39:27.383192Z","iopub.status.idle":"2023-04-26T11:39:27.608085Z","shell.execute_reply.started":"2023-04-26T11:39:27.383155Z","shell.execute_reply":"2023-04-26T11:39:27.607178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"WIDTH = 640\nHEIGHT = 480\nWIDTH_gazeutils, HEIGHT_gazeutils = 960, 720\ndef generate_data_field(eye_point, width=WIDTH_gazeutils, height=HEIGHT_gazeutils):\n    \"\"\"eye_point is (x, y) and between 0 and 1\"\"\"\n    x_grid = np.array(range(width)).reshape([1, width]).repeat(height, axis=0)\n    y_grid = np.array(range(height)).reshape([height, 1]).repeat(width, axis=1)\n    grid = np.stack((x_grid, y_grid)).astype(np.float32)\n\n    x, y = eye_point\n    x, y = x * width, y * height\n\n    grid -= np.array([x, y]).reshape([2, 1, 1]).astype(np.float32)\n    grid[0] = grid[0] / width\n    grid[1] = grid[1] / height\n#     norm = np.sqrt(np.sum(grid ** 2, axis=0)).reshape([1, height, width])\n#     # avoid zero norm\n#     norm = np.maximum(norm, 0.1)\n#     grid /= norm\n    return grid\n\n\ndef generate_gaze_cone(gaze_field, normalized_direction, width=WIDTH_gazeutils, height=HEIGHT_gazeutils):\n        \n    gaze_field = np.ascontiguousarray(gaze_field.transpose([1, 2, 0]))\n    gaze_field = gaze_field.reshape([-1, 2])\n    gaze_field = np.matmul(gaze_field, normalized_direction.reshape([2, 1]))\n    gaze_field_map = gaze_field.reshape([height, width, 1])\n    gaze_field_map = np.ascontiguousarray(gaze_field_map.transpose([2, 0, 1]))\n    \n    gaze_field_map = gaze_field_map * (gaze_field_map > 0).astype(int)\n    \n    return gaze_field_map.squeeze()\n\ndef _get_transform():\n    transform_list = []\n    transform_list.append(transforms.Resize((input_resolution, input_resolution)))\n    transform_list.append(transforms.ToTensor())\n    transform_list.append(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n    return transforms.Compose(transform_list)\n\ndef _get_transform_modality():\n    transform_list = []\n    transform_list.append(transforms.Resize((input_resolution, input_resolution)))\n    transform_list.append(transforms.ToTensor())\n    return transforms.Compose(transform_list)\n\ndef _get_object_transform():\n    transform_list = []\n    transform_list.append(transforms.Resize((256,256)))\n    transform_list.append(transforms.ToTensor())\n    transform_list.append(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n    return transforms.Compose(transform_list)\n\ndef get_head_box_channel(x_min, y_min, x_max, y_max, width, height, resolution, coordconv=False):\n    head_box = np.array([x_min/width, y_min/height, x_max/width, y_max/height])*resolution\n    head_box = head_box.astype(int)\n    head_box = np.clip(head_box, 0, resolution-1)\n    if coordconv:\n        unit = np.array(range(0,resolution), dtype=np.float32)\n        head_channel = []\n        for i in unit:\n            head_channel.append([unit+i])\n        head_channel = np.squeeze(np.array(head_channel)) / float(np.max(head_channel))\n        head_channel[head_box[1]:head_box[3],head_box[0]:head_box[2]] = 0\n    else:\n        head_channel = np.zeros((resolution,resolution), dtype=np.float32)\n        head_channel[head_box[1]:head_box[3],head_box[0]:head_box[2]] = 1\n    head_channel = torch.from_numpy(head_channel)\n    return head_channel\n\ndef to_numpy(tensor):\n    if torch.is_tensor(tensor):\n        return tensor.cpu().numpy()\n    elif type(tensor).__module__ != 'numpy':\n        raise ValueError(\"Cannot convert {} to numpy array\"\n                         .format(type(tensor)))\n    return tensor\n\n\ndef to_torch(ndarray):\n    if type(ndarray).__module__ == 'numpy':\n        return torch.from_numpy(ndarray)\n    elif not torch.is_tensor(ndarray):\n        raise ValueError(\"Cannot convert {} to torch tensor\"\n                         .format(type(ndarray)))\n    return ndarray\n\ndef draw_labelmap(img, pt, sigma, type='Gaussian'):\n    # Draw a 2D gaussian\n    # Adopted from https://github.com/anewell/pose-hg-train/blob/master/src/pypose/draw.py\n    img = to_numpy(img)\n\n    # Check that any part of the gaussian is in-bounds\n    ul = [int(pt[0] - 3 * sigma), int(pt[1] - 3 * sigma)]\n    br = [int(pt[0] + 3 * sigma + 1), int(pt[1] + 3 * sigma + 1)]\n    if (ul[0] >= img.shape[1] or ul[1] >= img.shape[0] or\n            br[0] < 0 or br[1] < 0):\n        # If not, just return the image as is\n        return to_torch(img)\n\n    # Generate gaussian\n    size = 6 * sigma + 1\n    x = np.arange(0, size, 1, float)\n    y = x[:, np.newaxis]\n    x0 = y0 = size // 2\n    # The gaussian is not normalized, we want the center value to equal 1\n    if type == 'Gaussian':\n        g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n    elif type == 'Cauchy':\n        g = sigma / (((x - x0) ** 2 + (y - y0) ** 2 + sigma ** 2) ** 1.5)\n\n    # Usable gaussian range\n    g_x = max(0, -ul[0]), min(br[0], img.shape[1]) - ul[0]\n    g_y = max(0, -ul[1]), min(br[1], img.shape[0]) - ul[1]\n    # Image range\n    img_x = max(0, ul[0]), min(br[0], img.shape[1])\n    img_y = max(0, ul[1]), min(br[1], img.shape[0])\n\n    img[img_y[0]:img_y[1], img_x[0]:img_x[1]] += g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n    img = img/np.max(img) # normalize heatmap so it has max value of 1\n    return to_torch(img)\n\ndef multi_hot_targets(gaze_pts, out_res):\n    w, h = out_res\n    target_map = np.zeros((h, w))\n    if gaze_pts[0] >= 0:\n        x, y = map(int,[gaze_pts[0]*w.float(), gaze_pts[1]*h.float()])\n        x = min(x, w-1)\n        y = min(y, h-1)\n        target_map[y, x] = 1\n    return target_map","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:39:27.612310Z","iopub.execute_input":"2023-04-26T11:39:27.614533Z","iopub.status.idle":"2023-04-26T11:39:27.657876Z","shell.execute_reply.started":"2023-04-26T11:39:27.614494Z","shell.execute_reply":"2023-04-26T11:39:27.656908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ttfnet regression \ndef bbox_areas(bboxes, keep_axis=False):\n    x_min, y_min, x_max, y_max = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n    areas = (y_max - y_min + 1) * (x_max - x_min + 1)\n    if keep_axis:\n        return areas[:, None]\n    return areas\n\ndef calc_region(bbox, ratio, featmap_size=None):\n    \"\"\"Calculate a proportional bbox region.\n    The bbox center are fixed and the new h' and w' is h * ratio and w * ratio.\n    Args:\n        bbox (Tensor): Bboxes to calculate regions, shape (n, 4)\n        ratio (float): Ratio of the output region.\n        featmap_size (tuple): Feature map size used for clipping the boundary.\n    Returns:\n        tuple: x1, y1, x2, y2\n    \"\"\"\n    x1 = torch.round((1 - ratio) * bbox[0] + ratio * bbox[2]).long()\n    y1 = torch.round((1 - ratio) * bbox[1] + ratio * bbox[3]).long()\n    x2 = torch.round(ratio * bbox[0] + (1 - ratio) * bbox[2]).long()\n    y2 = torch.round(ratio * bbox[1] + (1 - ratio) * bbox[3]).long()\n    if featmap_size is not None:\n        x1 = x1.clamp(min=0, max=featmap_size[1] - 1)\n        y1 = y1.clamp(min=0, max=featmap_size[0] - 1)\n        x2 = x2.clamp(min=0, max=featmap_size[1] - 1)\n        y2 = y2.clamp(min=0, max=featmap_size[0] - 1)\n    return (x1, y1, x2, y2)\n\ndef gaussian_2d(shape, sigma_x=1, sigma_y=1):\n    m, n = [(ss - 1.) / 2. for ss in shape]\n    y, x = np.ogrid[-m:m + 1, -n:n + 1]\n\n    h = np.exp(-(x * x / (2 * sigma_x * sigma_x) + y * y / (2 * sigma_y * sigma_y)))\n    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n    return h\n\ndef draw_truncate_gaussian(heatmap, center, h_radius, w_radius, k=1):\n    h, w = 2 * h_radius + 1, 2 * w_radius + 1\n    sigma_x = w / 6\n    sigma_y = h / 6\n    gaussian = gaussian_2d((h, w), sigma_x=sigma_x, sigma_y=sigma_y)\n    gaussian = heatmap.new_tensor(gaussian)\n    \n    x, y = int(center[0]), int(center[1])\n\n    height, width = heatmap.shape[0:2]\n\n    left, right = min(x, w_radius), min(width - x, w_radius + 1)\n    top, bottom = min(y, h_radius), min(height - y, h_radius + 1)\n\n    masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\n    masked_gaussian = gaussian[h_radius - top:h_radius + bottom,\n                      w_radius - left:w_radius + right]\n    if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0:\n        torch.max(masked_heatmap, masked_gaussian * k, out=masked_heatmap)\n    return heatmap\n\ndef target_single_image(gt_boxes, feat_shape=(64,64)):\n    \"\"\"\n    Args:\n        The scale of the gt_boxes is between 0 and 1\n        They are converted to feature space in the func.\n        \n        gt_boxes: tensor, tensor <=> img, (num_gt, 4).\n        feat_shape: tuple.\n    Returns:\n        heatmap: tensor, tensor <=> img, (1, h, w).\n        box_target: tensor, tensor <=> img, (4, h, w).\n        reg_weight: tensor, same as box_target.\n    \"\"\"\n    wh_area_process = 'log'\n    output_h, output_w = feat_shape\n    heatmap_channel = 1\n    wh_gaussian = True\n    wh_agnostic = True\n    down_ratio = 4\n    #alpha = 0.54\n    #beta = 0.54\n    alpha = 1.5\n    beta = 1.5\n    \n    heatmap = gt_boxes.new_zeros((heatmap_channel, output_h, output_w))\n    fake_heatmap = gt_boxes.new_zeros((output_h, output_w))\n    box_target = gt_boxes.new_ones((4, output_h, output_w)) * -1\n    reg_weight = gt_boxes.new_zeros((1, output_h, output_w),dtype=torch.float)\n    \n    if wh_area_process == 'log':\n        boxes_areas_log = bbox_areas(gt_boxes).log()\n    elif wh_area_process == 'sqrt':\n        boxes_areas_log = bbox_areas(gt_boxes).sqrt()\n    else:\n        boxes_areas_log = bbox_areas(gt_boxes)\n    boxes_area_topk_log  = boxes_areas_log\n\n    if wh_area_process == 'norm':\n        boxes_area_topk_log[:] = 1.\n\n    \n    # convert gt_boxes to 64 x 64 \n    feat_gt_boxes = gt_boxes * output_h\n    #feat_gt_boxes = torch.tensor([gt_boxes[:,0]*64/640,gt_boxes[:,1]*64/480,gt_boxes[:,2]*64/640,gt_boxes[:,3]*64/480]).unsqueeze(0)\n    #print(feat_gt_boxes)\n    feat_gt_boxes[:, [0, 2]] = torch.clamp(feat_gt_boxes[:, [0, 2]], min=0,\n                                           max=output_w - 1)\n    feat_gt_boxes[:, [1, 3]] = torch.clamp(feat_gt_boxes[:, [1, 3]], min=0,\n                                           max=output_h - 1)\n    feat_hs, feat_ws = (feat_gt_boxes[:, 3] - feat_gt_boxes[:, 1],\n                        feat_gt_boxes[:, 2] - feat_gt_boxes[:, 0])\n\n    # we calc the center and ignore area based on the gt-boxes of the origin scale\n    # no peak will fall between pixels\n    ct_ints = (torch.stack([(gt_boxes[:, 0] + gt_boxes[:, 2]) / 2,\n                            (gt_boxes[:, 1] + gt_boxes[:, 3]) / 2],\n                           dim=1)*output_h).to(torch.int)\n    \n    h_radiuses_alpha = (feat_hs / 2. * alpha).int()\n    w_radiuses_alpha = (feat_ws / 2. * alpha).int()\n    if wh_gaussian and alpha != beta:\n        h_radiuses_beta = (feat_hs / 2. * beta).int()\n        w_radiuses_beta = (feat_ws / 2. * beta).int()\n\n    if not wh_gaussian:\n        # calculate positive (center) regions\n        r1 = (1 - beta) / 2\n        ctr_x1s, ctr_y1s, ctr_x2s, ctr_y2s = calc_region(gt_boxes.transpose(0, 1), r1)\n        ctr_x1s, ctr_y1s, ctr_x2s, ctr_y2s = [torch.round(x.float() / down_ratio).int()\n                                              for x in [ctr_x1s, ctr_y1s, ctr_x2s, ctr_y2s]]\n        ctr_x1s, ctr_x2s = [torch.clamp(x, max=output_w - 1) for x in [ctr_x1s, ctr_x2s]]\n        ctr_y1s, ctr_y2s = [torch.clamp(y, max=output_h - 1) for y in [ctr_y1s, ctr_y2s]]\n\n    # larger boxes have lower priority than small boxes.\n    #for k in range(boxes_ind.shape[0]):\n    k = 0\n    cls_id = 0\n    #print(ct_ints[k])\n    #print(h_radiuses_alpha[k].item())\n    #print(w_radiuses_alpha[k].item())\n    fake_heatmap = fake_heatmap.zero_()\n    draw_truncate_gaussian(fake_heatmap, ct_ints[k],\n                                h_radiuses_alpha[k].item(), w_radiuses_alpha[k].item())\n    heatmap[cls_id] = torch.max(heatmap[cls_id], fake_heatmap)\n\n    if wh_gaussian:\n        if alpha != beta:\n            fake_heatmap = fake_heatmap.zero_()\n            draw_truncate_gaussian(fake_heatmap, ct_ints[k],\n                                        h_radiuses_beta[k].item(),\n                                        w_radiuses_beta[k].item())\n        box_target_inds = fake_heatmap > 0\n    else:\n        ctr_x1, ctr_y1, ctr_x2, ctr_y2 = ctr_x1s[k], ctr_y1s[k], ctr_x2s[k], ctr_y2s[k]\n        box_target_inds = torch.zeros_like(fake_heatmap, dtype=torch.uint8)\n        box_target_inds[ctr_y1:ctr_y2 + 1, ctr_x1:ctr_x2 + 1] = 1\n\n    if wh_agnostic:\n        box_target[:, box_target_inds] = gt_boxes[k][:, None] * output_h\n        #box_target[:, box_target_inds] = torch.tensor([abs(ct_ints[k][0] - feat_gt_boxes[k][0, None]),abs(ct_ints[k][1] - feat_gt_boxes[k][1, None]),abs(ct_ints[k][0] - feat_gt_boxes[k][2, None]),abs(ct_ints[k][1] - feat_gt_boxes[k][3, None])])[:, None]\n\n    else:\n        box_target[(cls_id * 4):((cls_id + 1) * 4), box_target_inds] = gt_boxes[k][:, None] * output_h\n\n    if wh_gaussian:\n        local_heatmap = fake_heatmap[box_target_inds].float()\n        ct_div = local_heatmap.sum()\n        local_heatmap *= boxes_area_topk_log[k]\n        reg_weight[cls_id, box_target_inds] = local_heatmap / ct_div\n    else:\n        reg_weight[cls_id, box_target_inds] = \\\n            boxes_area_topk_log[k] / box_target_inds.sum().float()\n\n    return heatmap, box_target, reg_weight\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:39:27.664931Z","iopub.execute_input":"2023-04-26T11:39:27.667175Z","iopub.status.idle":"2023-04-26T11:39:27.713116Z","shell.execute_reply.started":"2023-04-26T11:39:27.667140Z","shell.execute_reply":"2023-04-26T11:39:27.712049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def non_max_suppression_fast(boxes, overlapThresh=0.6):\n    boxes = boxes[0]\n    # if there are no boxes, return an empty list\n    if len(boxes) == 0:\n        return []\n\n    # initialize the list of picked indexes\n    pick = []\n\n    # grab the coordinates of the bounding boxes\n    x1 = boxes[:,0]\n    y1 = boxes[:,1]\n    x2 = boxes[:,2]\n    y2 = boxes[:,3]\n\n    # compute the area of the bounding boxes and sort the bounding\n    # boxes by the bottom-right y-coordinate of the bounding box\n    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n\n    idxs = np.argsort(y2)\n\n    # keep looping while some indexes still remain in the indexes\n    # list\n    while len(idxs) > 0:\n        # grab the last index in the indexes list and add the\n        # index value to the list of picked indexes\n        last = len(idxs) - 1\n        i = idxs[last]\n        pick.append(i)\n\n        # find the largest (x, y) coordinates for the start of\n        # the bounding box and the smallest (x, y) coordinates\n        # for the end of the bounding box\n        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n\n        # compute the width and height of the bounding box\n        w = np.maximum(0, xx2 - xx1 + 1)\n        h = np.maximum(0, yy2 - yy1 + 1)\n\n        # compute the ratio of overlap\n        overlap = (w * h) / area[idxs[:last]]\n\n        # delete all indexes from the index list that have\n        idxs = np.delete(idxs, np.concatenate(([last],\n            np.where(overlap > overlapThresh)[0])))\n\n    # return only the bounding boxes that were picked\n    return boxes[pick]\n\ndef simple_nms(heat, kernel=3, out_heat=None):\n    pad = (kernel - 1) // 2\n    hmax = nn.functional.max_pool2d(heat, (kernel, kernel), stride=1, padding=pad)\n    keep = (hmax == heat).float()\n    out_heat = heat if out_heat is None else out_heat\n    return out_heat * keep\n\ndef _topk(scores, topk):\n    batch, cat, height, width = scores.size()\n\n    # both are (batch, 1, topk)\n    topk_scores, topk_inds = torch.topk(scores.view(batch, cat, -1), topk)\n\n    topk_inds = topk_inds % (height * width)\n    topk_ys = (topk_inds / width).int().float()\n    topk_xs = (topk_inds % width).int().float()\n\n    # both are (batch, topk). select topk from 80*topk\n    topk_score, topk_ind = torch.topk(topk_scores.view(batch, -1), topk)\n    \n    topk_clses = (topk_ind / topk).int()\n    topk_ind = topk_ind.unsqueeze(2)\n    topk_inds = topk_inds.view(batch, -1, 1).gather(1, topk_ind).view(batch, topk)\n    topk_ys = topk_ys.view(batch, -1, 1).gather(1, topk_ind).view(batch, topk)\n    topk_xs = topk_xs.view(batch, -1, 1).gather(1, topk_ind).view(batch, topk)\n    \n    return topk_score, topk_inds, topk_clses, topk_ys, topk_xs\n\ndef get_bboxes_ttfnet(pred_heatmap,pred_wh,rescale=False):\n    if pred_heatmap.shape[1]!=1:\n        pred_heatmap = pred_heatmap.unsqueeze(1)\n    \n\n    down_ratio = 1\n\n    batch, cat, height, width = pred_heatmap.size()\n    #pred_heatmap = pred_heatmap.detach().sigmoid_()\n    pred_heatmap = pred_heatmap.detach()\n    wh = pred_wh.detach()\n    \n    # perform nms on heatmaps\n    heat = simple_nms(pred_heatmap)  # used maxpool to filter the max score\n   \n\n    topk = Bbox_topk_no_of_bboxes\n    scores, inds, clses, ys, xs = _topk(heat, topk=topk)\n   \n    xs = xs.view(batch, topk, 1) * down_ratio\n    ys = ys.view(batch, topk, 1) * down_ratio\n    \n    #print(scores, inds, clses, xs, ys) \n    \n    wh = wh.permute(0, 2, 3, 1).contiguous()\n    wh = wh.view(wh.size(0), -1, wh.size(3))\n    \n    inds = inds.unsqueeze(2).expand(inds.size(0), inds.size(1), wh.size(2))\n    \n    #print(inds)\n    wh = wh.gather(1, inds)\n    wh = wh.view(batch, topk, 4)\n    \n    clses = clses.view(batch, topk, 1).float()\n    scores = scores.view(batch, topk, 1)\n    \n    bboxes = torch.cat([xs - wh[..., [0]], ys - wh[..., [1]],\n                        xs + wh[..., [2]], ys + wh[..., [3]]], dim=2)\n    #print(bboxes)\n    result_list = []\n    score_thr = Bbox_confidence_score_threshold\n    #print(bboxes.shape[0])\n    for batch_i in range(bboxes.shape[0]):\n        scores_per_img = scores[batch_i]\n        scores_keep = (scores_per_img > score_thr).squeeze(-1)\n\n        scores_per_img = scores_per_img[scores_keep]\n        bboxes_per_img = bboxes[batch_i][scores_keep]\n        labels_per_img = clses[batch_i][scores_keep]\n        img_shape = [output_resolution, output_resolution]\n        bboxes_per_img[:, 0::2] = bboxes_per_img[:, 0::2].clamp(min=0, max=img_shape[1] - 1)\n        bboxes_per_img[:, 1::2] = bboxes_per_img[:, 1::2].clamp(min=0, max=img_shape[0] - 1)\n        \n        if rescale:\n            scale_factor = img_metas[batch_i]['scale_factor']\n            bboxes_per_img /= bboxes_per_img.new_tensor(scale_factor)\n\n        bboxes_per_img = torch.cat([bboxes_per_img, scores_per_img], dim=1)\n        labels_per_img = labels_per_img.squeeze(-1)\n        result_list.append((bboxes_per_img, labels_per_img))\n\n    return result_list\n\ndef bbox_overlaps_ttfnet(bboxes1, bboxes2, mode='iou', is_aligned=False):\n    \"\"\"Calculate overlap between two set of bboxes.\n    If ``is_aligned`` is ``False``, then calculate the ious between each bbox\n    of bboxes1 and bboxes2, otherwise the ious between each aligned pair of\n    bboxes1 and bboxes2.\n    Args:\n        bboxes1 (Tensor): shape (m, 4)\n        bboxes2 (Tensor): shape (n, 4), if is_aligned is ``True``, then m and n\n            must be equal.\n        mode (str): \"iou\" (intersection over union) or iof (intersection over\n            foreground).\n    Returns:\n        ious(Tensor): shape (m, n) if is_aligned == False else shape (m, 1)\n    \"\"\"\n\n    assert mode in ['iou', 'iof']\n\n    rows = bboxes1.size(0)\n    cols = bboxes2.size(0)\n    if is_aligned:\n        assert rows == cols\n\n    if rows * cols == 0:\n        return bboxes1.new(rows, 1) if is_aligned else bboxes1.new(rows, cols)\n\n    if is_aligned:\n        lt = torch.max(bboxes1[:, :2], bboxes2[:, :2])  # [rows, 2]\n        rb = torch.min(bboxes1[:, 2:], bboxes2[:, 2:])  # [rows, 2]\n\n        wh = (rb - lt + 1).clamp(min=0)  # [rows, 2]\n        overlap = wh[:, 0] * wh[:, 1]\n        area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (\n            bboxes1[:, 3] - bboxes1[:, 1] + 1)\n\n        if mode == 'iou':\n            area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (\n                bboxes2[:, 3] - bboxes2[:, 1] + 1)\n            ious = overlap / (area1 + area2 - overlap)\n        else:\n            ious = overlap / area1\n    else:\n        lt = torch.max(bboxes1[:, None, :2], bboxes2[:, :2])  # [rows, cols, 2]\n        rb = torch.min(bboxes1[:, None, 2:], bboxes2[:, 2:])  # [rows, cols, 2]\n\n        wh = (rb - lt + 1).clamp(min=0)  # [rows, cols, 2]\n        overlap = wh[:, :, 0] * wh[:, :, 1]\n        area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (\n            bboxes1[:, 3] - bboxes1[:, 1] + 1)\n        \n        if mode == 'iou':\n            area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (\n                bboxes2[:, 3] - bboxes2[:, 1] + 1)\n            \n            a_xmin = torch.min(bboxes1[:,0], bboxes2[:,0])\n            a_ymin = torch.min(bboxes1[:,1], bboxes2[:,1])\n            a_xmax = torch.max(bboxes1[:,2], bboxes2[:,2])\n            a_ymax = torch.max(bboxes1[:,3], bboxes2[:,3])\n            a_box = (a_xmax - a_xmin + 1) * (a_ymax - a_ymin + 1)\n\n            w = torch.min(area1 / area2, area2 / area1)\n\n            ious = overlap / (area1[:, None] + area2 - overlap)\n            wuocs = (w[:,None] * ((area1[:, None] + area2 - overlap) / a_box[:, None]))\n        else:\n            ious = overlap / (area1[:, None])\n    \n    return ious,wuocs","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:39:27.717853Z","iopub.execute_input":"2023-04-26T11:39:27.720018Z","iopub.status.idle":"2023-04-26T11:39:27.770608Z","shell.execute_reply.started":"2023-04-26T11:39:27.719984Z","shell.execute_reply":"2023-04-26T11:39:27.769637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GooRealDataset(Dataset):\n    \n    def __init__(self,df, get_transform, get_transform_modality, input_size=input_resolution, output_size=output_resolution,\n                 test=False, modality='image', imshow=False):\n    \n        self.df_data =  df\n        self.image_dir = image_dir\n        self.transform = get_transform\n        self.transform_modality = get_transform_modality\n        self.get_object_transform = _get_object_transform()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.imshow = imshow\n        self.test = test\n        self.modality = modality\n    def __len__(self):\n        return len(self.df_data.index)\n    \n    def __getitem__(self, index):\n        gaze_inside = True\n        \n        row = self.df_data.iloc[index]\n        \n        path = PureWindowsPath(row['filename']).as_posix()\n        filename_no_extension = os.path.splitext(os.path.basename(path))[0]\n        subfolder_path_str = os.path.splitext(path)[0].split(\"/\")\n        key_filename = subfolder_path_str[0]+'/'+subfolder_path_str[1]\n        \n        #path = row['filename']\n        #filename_no_extension = path.split('.')[0]\n        #key_filename = ''   \n        img = Image.open(os.path.join(self.image_dir, path))\n        img = img.convert('RGB').resize((640,480))\n        \n        width, height = img.size\n        \n        #gaze_x = row.gaze_cx / width\n        #gaze_y = row.gaze_cy / height\n        \n        eye_x = row.hx / width\n        eye_y = row.hy / height\n        \n        x_min, y_min, x_max, y_max = np.array(row['ann']['bboxes'][-1])\n        \n        # For Single object heatmap\n        gaze_obj_x_min, gaze_obj_y_min, gaze_obj_x_max, gaze_obj_y_max = np.array(row['ann']['bboxes'][row.gazeIdx])\n        gazed_object_class = np.array(row.gaze_item)\n        all_object_bboxes = np.array(row['ann']['bboxes'])[:-1]\n        all_object_bboxes_class = np.array(row['ann']['labels'])[:-1]\n        \n                \n        # For CenterNet move the gaze point to center of the gazed at object bounding box\n        #if self.test:\n        gaze_x = ((gaze_obj_x_min+gaze_obj_x_max)/2)/ width\n        gaze_y = ((gaze_obj_y_min+gaze_obj_y_max)/2)/ height\n        \n        # expand face bbox a bit\n        k = 0.05\n        x_min -= k * abs(x_max - x_min)\n        y_min -= k * abs(y_max - y_min)\n        x_max += k * abs(x_max - x_min)\n        y_max += k * abs(y_max - y_min)\n\n        x_min, y_min, x_max, y_max = map(float, [x_min, y_min, x_max, y_max])\n\n        if self.test: \n            \n            #self.pose_dir = '/kaggle/input/goorealposeanddepth/goo-real-test-pose/kaggle/working/goo-real-pose'\n            #self.depth_dir = '/kaggle/input/goorealposeanddepth/goo-real-test-depth/kaggle/working/goo-real-depth'\n            #self.pose_dir = '/kaggle/input/goosynthtestposeanddepth/goo-synth-test-pose/kaggle/working/goo-synth-test-pose'\n            #self.depth_dir = '/kaggle/input/goosynthtestposeanddepth/goo-synth-test-depth/kaggle/working/goo-synth-test-depth'\n            self.pose_dir = '/kaggle/input/goorealposeanddepth/goo-real-val-pose/kaggle/working/goo-real-pose'\n            self.depth_dir = '/kaggle/input/goorealposeanddepth/goo-real-val-depth/kaggle/working/goo-real-depth'\n            if use_sparse_dataset:\n                self.pose_dir = '/kaggle/input/goorealposeanddepth/goo-real-sparse-test-pose/kaggle/working/goo-real-sparse-test-pose'\n                self.depth_dir = '/kaggle/input/goorealposeanddepth/goo-real-sparse-test-depth/kaggle/working/goo-real-sparse-depth'\n        else:\n            self.pose_dir = None\n            self.depth_dir = None   \n        # read pose\n        if os.path.exists(os.path.join(self.pose_dir, key_filename,filename_no_extension+'-pose.png')):\n            pose = Image.open(os.path.join(self.pose_dir, key_filename,filename_no_extension+'-pose.png'))\n        else:\n            pose = Image.open(os.path.join(self.pose_dir, key_filename,filename_no_extension+'-pose.jpg'))\n        # read depth\n        depth = Image.open(os.path.join(self.depth_dir,key_filename,filename_no_extension+'.png'))\n        \n\n        if self.imshow:\n            img.save(\"origin_img.jpg\")\n\n        if self.test:\n            imsize = torch.IntTensor([width, height])\n            if privacy:\n                img = Image.fromarray(np.uint8(np.zeros((height, width, 3))*255))\n        else:\n            ## data augmentation               \n                        \n            # Jitter (expansion-only) bounding box size\n            if np.random.random_sample() <= 0.5:\n                k = np.random.random_sample() * 0.2\n                x_min -= k * abs(x_max - x_min)\n                y_min -= k * abs(y_max - y_min)\n                x_max += k * abs(x_max - x_min)\n                y_max += k * abs(y_max - y_min)\n\n            # Random Crop\n            if np.random.random_sample() <= 0.5:\n                # Calculate the minimum valid range of the crop that doesn't exclude the face and the gaze target\n                crop_x_min = np.min([gaze_x * width, x_min, x_max])\n                crop_y_min = np.min([gaze_y * height, y_min, y_max])\n                crop_x_max = np.max([gaze_x * width, x_min, x_max])\n                crop_y_max = np.max([gaze_y * height, y_min, y_max])\n\n                # Randomly select a random top left corner\n                if crop_x_min >= 0:\n                    crop_x_min = np.random.uniform(0, crop_x_min)\n                if crop_y_min >= 0:\n                    crop_y_min = np.random.uniform(0, crop_y_min)\n\n                # Find the range of valid crop width and height starting from the (crop_x_min, crop_y_min)\n                crop_width_min = crop_x_max - crop_x_min\n                crop_height_min = crop_y_max - crop_y_min\n                crop_width_max = width - crop_x_min\n                crop_height_max = height - crop_y_min\n                # Randomly select a width and a height\n                crop_width = np.random.uniform(crop_width_min, crop_width_max)\n                crop_height = np.random.uniform(crop_height_min, crop_height_max)\n\n                # Crop it\n                img = TF.crop(img, crop_y_min, crop_x_min, crop_height, crop_width)\n                pose = TF.crop(pose, crop_y_min, crop_x_min, crop_height, crop_width)\n                depth = TF.crop(depth, crop_y_min, crop_x_min, crop_height, crop_width)\n                \n                # Record the crop's (x, y) offset\n                offset_x, offset_y = crop_x_min, crop_y_min\n\n                # convert coordinates into the cropped frame\n                x_min, y_min, x_max, y_max = x_min - offset_x, y_min - offset_y, x_max - offset_x, y_max - offset_y\n                # if gaze_inside:\n                gaze_x, gaze_y = (gaze_x * width - offset_x) / float(crop_width), \\\n                                 (gaze_y * height - offset_y) / float(crop_height)\n                eye_x, eye_y = (eye_x * width - offset_x) / float(crop_width), \\\n                                 (eye_y * height - offset_y) / float(crop_height)\n                # else:\n                #     gaze_x = -1; gaze_y = -1\n                \n                gaze_obj_x_min -= offset_x\n                gaze_obj_y_min -= offset_y\n                gaze_obj_x_max -= offset_x\n                gaze_obj_y_max -= offset_y\n                \n                width, height = crop_width, crop_height\n\n            # Random flip\n            if np.random.random_sample() <= 0.5:\n                img = img.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n                pose = pose.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n                depth = depth.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n                                \n                x_max_2 = width - x_min\n                x_min_2 = width - x_max\n                x_max = x_max_2\n                x_min = x_min_2\n                gaze_x = 1 - gaze_x\n                eye_x = 1 - eye_x\n                \n                # flip the GT gazed object bbox\n                gaze_obj_x_max_2 = width - gaze_obj_x_min\n                gaze_obj_x_min_2 = width - gaze_obj_x_max\n                gaze_obj_x_max = gaze_obj_x_max_2\n                gaze_obj_x_min = gaze_obj_x_min_2\n\n            # Random color change\n            if np.random.random_sample() <= 0.5:\n                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n\n        if cone_mode=='early':\n            cone_resolution = input_resolution\n        else:\n            cone_resolution = input_resolution // 4\n        \n        head_channel = get_head_box_channel(x_min, y_min, x_max, y_max, width, height,\n                                                    resolution=cone_resolution, coordconv=False).unsqueeze(0)\n\n        # Crop the face\n        if privacy:\n            face = pose.crop((int(x_min), int(y_min), int(x_max), int(y_max)))\n        else:\n            face = img.crop((int(x_min), int(y_min), int(x_max), int(y_max)))\n\n        # modality dropout\n        height, width = int(height), int(width)\n        num_modalities = 3\n        dropped = np.zeros(num_modalities)\n        if not self.test and self.modality=='attention':\n            if modality_dropout:\n                # keep one modality\n                if privacy:\n                    modality_idx = 1\n                else:\n                    modality_idx = 0\n                m_keep = np.random.randint(modality_idx, num_modalities)\n\n                if (np.random.rand() <= 0.2) and m_keep!=0:\n                    img = Image.fromarray(np.uint8(np.random.rand(height, width, 3)*255))\n                    dropped[0] = 1\n                if (np.random.rand() <= 0.2) and m_keep!=1:\n                    depth = Image.fromarray(np.uint8(np.random.rand(height, width)*255))\n                    dropped[1] = 1\n                if (np.random.rand() <= 0.2) and m_keep!=2:\n                    pose = Image.fromarray(np.uint8(np.random.rand(height, width, 3)*255))\n                    dropped[2] = 1\n\n            if privacy:\n                img = Image.fromarray(np.uint8(np.zeros((height, width, 3))))\n                dropped[0] = 1\n        \n        gazed_object_bbox = np.array([gaze_obj_x_min, gaze_obj_y_min, gaze_obj_x_max, gaze_obj_y_max])\n        \n        # generate new gaze field (for human-centric branch)\n        eye_point = np.array([eye_x, eye_y])\n        gaze = np.array([gaze_x, gaze_y])\n        gt_direction = np.array([-1.0, -1.0])\n        if gaze_inside:\n            gt_direction = gaze - eye_point            \n            if gt_direction.mean()!=0:\n                gt_direction = gt_direction / np.linalg.norm(gt_direction)\n        \n        gaze_field = generate_data_field(eye_point, width=cone_resolution, height=cone_resolution)\n        # normalize\n        norm = np.sqrt(np.sum(gaze_field ** 2, axis=0)).reshape([1, cone_resolution, cone_resolution])\n        # avoid zero norm\n        norm = np.maximum(norm, 0.1)\n        gaze_field /= norm\n          \n        if self.transform is not None:\n            img = self.transform(img)\n            face = self.transform(face)\n            \n            pose = self.transform_modality(pose)\n            depth = self.transform_modality(depth)\n            #depth = depth / 65535    # depth maps are in 16 bit format\n        \n        ## for TTFNet\n        gazed_object_box_for_cp = torch.tensor([gaze_obj_x_min/width,gaze_obj_y_min/height,gaze_obj_x_max/width,gaze_obj_y_max/height]).unsqueeze(0)\n        heatmap_cp, box_target_cp, reg_weight_cp =  target_single_image(gazed_object_box_for_cp, feat_shape=(self.output_size, self.output_size)) \n        \n        # generate the heat map used for deconv prediction\n        gaze_heatmap = torch.zeros(self.output_size, self.output_size)  # set the size of the output\n        #gaze_heatmap_org = torch.zeros(self.output_size, self.output_size)  # set the size of the output\n        if self.test:  # aggregated heatmap\n            if gaze_x != -1:\n                #gaze_heatmap = draw_labelmap(gaze_heatmap, [gaze_x * self.output_size, gaze_y * self.output_size], 3, type='Gaussian')\n                gaze_heatmap = heatmap_cp.squeeze(0).float() ## for TTFNet\n        else:\n            #gaze_heatmap = draw_labelmap(gaze_heatmap, [gaze_x * self.output_size, gaze_y * self.output_size], 3, type='Gaussian')\n            gaze_heatmap = heatmap_cp.squeeze(0).float() ## for TTFNet\n        \n        if gaze_inside:\n            cont_gaze = [gaze_x, gaze_y]\n        else:\n            cont_gaze = [-1, -1]\n        cont_gaze = torch.FloatTensor(cont_gaze)\n        \n        if self.imshow:\n            fig = plt.figure(111)\n            img = 255 - unnorm(img.numpy()) * 255\n            img = np.clip(img, 0, 255)\n            plt.imshow(np.transpose(img, (1, 2, 0)))\n            plt.imshow(cv2.resize(gaze_heatmap, (self.input_size, self.input_size)), cmap='jet', alpha=0.3)\n            plt.imshow(cv2.resize(1 - head_channel.squeeze(0), (self.input_size, self.input_size)), alpha=0.2)\n            plt.savefig('viz_aug.png')\n\n        if self.test:\n            return img, face, pose, depth, gaze_field, gt_direction, head_channel, gaze_heatmap, cont_gaze, imsize, path, eye_point, all_object_bboxes, all_object_bboxes_class, gazed_object_bbox, gazed_object_class\n        else:\n            return img, face, pose, depth, gaze_field, gt_direction, head_channel, gaze_heatmap, path, gaze_inside, dropped, box_target_cp, reg_weight_cp\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:39:27.774985Z","iopub.execute_input":"2023-04-26T11:39:27.777349Z","iopub.status.idle":"2023-04-26T11:39:27.843106Z","shell.execute_reply.started":"2023-04-26T11:39:27.777304Z","shell.execute_reply":"2023-04-26T11:39:27.842287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class retailGazeDataset(Dataset):\n    def __init__(self,df, get_transform, get_transform_modality, input_size=input_resolution, output_size=output_resolution, test=False, imshow=False):\n        self.df_data =  df\n        self.image_dir = '/kaggle/input/retailgaze/RetailGaze_V2_seg/RetailGaze_V2/'\n        self.get_transform = get_transform\n        self.transform_modality = get_transform_modality\n        self.input_size = input_size\n        self.output_size = output_size\n        self.imshow = imshow\n        self.test = test\n    def __len__(self):\n        return len(self.df_data.index)\n\n    def __getitem__(self,idx):\n         \n        gaze_inside = True\n        row = self.df_data.iloc[idx]\n        path = row.filename\n        path_seg_mask = row.seg_mask\n        filename_no_extension = os.path.splitext(os.path.basename(row.filename))[0]\n        \n        subfolder_path_str = os.path.splitext(path)[0].split(\"/\")\n        key_filename = subfolder_path_str[0]+'/'+subfolder_path_str[1]\n    \n        img = Image.open(os.path.join(self.image_dir, row.filename))\n        img = img.convert('RGB')\n        frame_org = img\n        width, height = img.size\n        \n        # Can give -1 in dataset if not know for test\n        gaze_x = row.gaze_cx / width\n        gaze_y = row.gaze_cy / height\n        \n        \"\"\"\n        From the paper section 3.2, gaze cone generator\n        The gaze cone generator produces Ico by drawing a cone from the \n        subjects eyes location p eye (i.e. eye mid-point if available \n        from the pose modality; otherwise, using a prototypal location \n        in the head bounding box) along the direction of g2D.\n        \"\"\"\n        eye_x = ((row.left_eye_x+row.right_eye_x) * 0.5)/width\n        eye_y = ((row.left_eye_y+row.right_eye_y) * 0.5)/height\n        \n        # expand face bbox a bit\n        head_box_org = list(row.ann.values())[0]\n        x_min, y_min, x_max, y_max = head_box_org\n        \n        k = 0.05\n        x_min -= k * abs(x_max - x_min)\n        y_min -= k * abs(y_max - y_min)\n        x_max += k * abs(x_max - x_min)\n        y_max += k * abs(y_max - y_min)\n        x_min, y_min, x_max, y_max = map(float, [x_min, y_min, x_max, y_max])\n        \n        # point to val dir\n        if self.test: \n            \n            self.pose_dir = '/kaggle/input/retailgazedepthandpose/retailgaze-test-pose/kaggle/working/retailgaze-pose'\n            self.depth_dir = '/kaggle/input/retailgazedepthandpose/retailgaze-test-depth/kaggle/working/retailgaze-depth'\n            \n        # point to train dir\n        else:\n            self.pose_dir = '/kaggle/input/retailgazedepthandpose/retailgaze-train-pose/kaggle/working/retailgaze-pose'\n            self.depth_dir = '/kaggle/input/retailgazedepthandpose/retailgaze-train-depth/kaggle/working/retailgaze-depth'\n            \n        # read pose\n        pose = Image.open(os.path.join(self.pose_dir, key_filename,filename_no_extension+'-pose.jpg'))\n        \n        # read depth\n        depth = Image.open(os.path.join(self.depth_dir,key_filename,filename_no_extension+'.png'))\n        \n        \n        if self.test:\n            imsize = torch.IntTensor([width, height])\n        else:\n            ## data augmentation                          \n            # Jitter (expansion-only) bounding box size\n            \n            if np.random.random_sample() <= 0.5:\n                k = np.random.random_sample() * 0.2\n                x_min -= k * abs(x_max - x_min)\n                y_min -= k * abs(y_max - y_min)\n                x_max += k * abs(x_max - x_min)\n                y_max += k * abs(y_max - y_min)\n\n            # Random Crop\n            if np.random.random_sample() <= 0.5:\n                # Calculate the minimum valid range of the crop that doesn't exclude the face and the gaze target\n                crop_x_min = np.min([gaze_x * width, x_min, x_max])\n                crop_y_min = np.min([gaze_y * height, y_min, y_max])\n                crop_x_max = np.max([gaze_x * width, x_min, x_max])\n                crop_y_max = np.max([gaze_y * height, y_min, y_max])\n\n                # Randomly select a random top left corner\n                if crop_x_min >= 0:\n                    crop_x_min = np.random.uniform(0, crop_x_min)\n                if crop_y_min >= 0:\n                    crop_y_min = np.random.uniform(0, crop_y_min)\n\n                # Find the range of valid crop width and height starting from the (crop_x_min, crop_y_min)\n                crop_width_min = crop_x_max - crop_x_min\n                crop_height_min = crop_y_max - crop_y_min\n                crop_width_max = width - crop_x_min\n                crop_height_max = height - crop_y_min\n                # Randomly select a width and a height\n                crop_width = np.random.uniform(crop_width_min, crop_width_max)\n                crop_height = np.random.uniform(crop_height_min, crop_height_max)\n\n                # Crop it\n                img = TF.crop(img, crop_y_min, crop_x_min, crop_height, crop_width)\n                pose = TF.crop(pose, crop_y_min, crop_x_min, crop_height, crop_width)\n                depth = TF.crop(depth, crop_y_min, crop_x_min, crop_height, crop_width)\n                \n                # Record the crop's (x, y) offset\n                offset_x, offset_y = crop_x_min, crop_y_min\n\n                # convert coordinates into the cropped frame\n                x_min, y_min, x_max, y_max = x_min - offset_x, y_min - offset_y, x_max - offset_x, y_max - offset_y\n                gaze_x, gaze_y = (gaze_x * width - offset_x) / float(crop_width), \\\n                                 (gaze_y * height - offset_y) / float(crop_height)\n                eye_x, eye_y = (eye_x * width - offset_x) / float(crop_width), \\\n                                 (eye_y * height - offset_y) / float(crop_height)\n                \n\n                width, height = crop_width, crop_height\n\n            \n            # Random flip\n            if np.random.random_sample() <= 0.5:\n                img = img.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n                pose = pose.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n                depth = depth.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n                                \n                x_max_2 = width - x_min\n                x_min_2 = width - x_max\n                x_max = x_max_2\n                x_min = x_min_2\n                gaze_x = 1 - gaze_x\n                eye_x = 1 - eye_x\n                \n            \n            # Random color change\n            if np.random.random_sample() <= 0.5:\n                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n        \n        if cone_mode=='early':\n            cone_resolution = input_resolution\n        else:\n            cone_resolution = input_resolution // 4\n            \n        head_box = [x_min, y_min, x_max, y_max]\n        head = img.crop((head_box)) # head crop\n        head_channel = get_head_box_channel(head_box[0], head_box[1], head_box[2], head_box[3], width, height,\n                                                    resolution=input_resolution).unsqueeze(0)\n        \n        \n        # modality dropout\n        height, width = np.int32(height), np.int32(width)\n        num_modalities = 3\n        dropped = np.zeros(num_modalities)\n        \n        if not self.test:\n            if modality_dropout:\n                # keep one modality\n                modality_idx = 0\n                m_keep = np.random.randint(modality_idx, num_modalities)\n\n                if (np.random.rand() <= 0.2) and m_keep!=0:\n                    img = Image.fromarray(np.uint8(np.random.rand(height, width, 3)*255))\n                    dropped[0] = 1\n                if (np.random.rand() <= 0.2) and m_keep!=1:\n                    depth = Image.fromarray(np.uint8(np.random.rand(height, width)*255))\n                    dropped[1] = 1\n                if (np.random.rand() <= 0.2) and m_keep!=2:\n                    pose = Image.fromarray(np.uint8(np.random.rand(height, width, 3)*255))\n                    dropped[2] = 1\n\n                    \n        # generate new gaze field (for human-centric branch)\n        eye_point = np.array([eye_x, eye_y])\n        gaze = np.array([gaze_x, gaze_y])\n        gt_direction = np.array([-1.0, -1.0])\n        if gaze_inside:\n            gt_direction = gaze - eye_point            \n            if gt_direction.mean()!=0:\n                gt_direction = gt_direction / np.linalg.norm(gt_direction)\n                \n        gaze_field = generate_data_field(eye_point, width=cone_resolution, height=cone_resolution)\n        # normalize\n        norm = np.sqrt(np.sum(gaze_field ** 2, axis=0)).reshape([1, cone_resolution, cone_resolution])\n        norm = np.maximum(norm, 0.1)\n        gaze_field /= norm\n        \n        \n        head = self.get_transform(head) # transform inputs\n        img = self.get_transform(img)\n        pose = self.transform_modality(pose)\n        depth = self.transform_modality(depth)\n        # Note: I saved depth in 8 bits not 16 bits\n        #########\n        #depth = depth / 65535    # depth maps are in 16 bit format\n        #########        \n                \n        # generate the heat map used for deconv prediction\n        gaze_heatmap = torch.zeros(self.output_size, self.output_size)  # set the size of the output\n        #gaze_heatmap = torch.zeros(self.output_size[0], self.output_size[1])  # set the size of the output\n        \n        if gaze_x != -1:\n            gaze_heatmap = draw_labelmap(gaze_heatmap, [gaze_x * self.output_size, gaze_y * self.output_size],3,type='Gaussian')\n        if gaze_inside:\n            cont_gaze = [gaze_x, gaze_y]\n        cont_gaze = torch.FloatTensor(cont_gaze)\n        \n        if self.imshow:\n            fig = plt.figure(111)\n            plt.imshow(frame_org)\n            #plt.imshow(cv2.resize(gaze_heatmap, (self.input_size, self.input_size)), cmap='jet', alpha=0.3)\n            #plt.imshow(cv2.resize(1 - head_channel.squeeze(0), (self.input_size, self.input_size)), alpha=0.2)\n            plt.imshow(cv2.resize(gaze_heatmap.numpy(), (self.input_size[0], self.input_size[1])), cmap='jet', alpha=0.3)\n            plt.imshow(cv2.resize(1 - head_channel.squeeze(0).numpy(), (self.input_size[0], self.input_size[1])), alpha=0.2)\n            plt.savefig('viz_aug.png')\n        \n        # for some images seg mask is None to avoid error give a dummy value\n        if path_seg_mask is None:\n            path_seg_mask = 0\n        if self.test:\n            return img, head, pose, depth, gaze_field, gt_direction, head_channel, gaze_heatmap, cont_gaze, imsize, gaze_inside, path, path_seg_mask\n        else:\n            return img, head, pose, depth, gaze_field, gt_direction, head_channel, gaze_heatmap, path, gaze_inside, dropped","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:39:27.847561Z","iopub.execute_input":"2023-04-26T11:39:27.849699Z","iopub.status.idle":"2023-04-26T11:39:27.904777Z","shell.execute_reply.started":"2023-04-26T11:39:27.849665Z","shell.execute_reply":"2023-04-26T11:39:27.903933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntransform = _get_transform()\ntransform_modality = _get_transform_modality()\n\nif flag_run_on_goo_dataset:\n    val_dataset = GooRealDataset(df_test, transform, transform_modality, \n                           input_size=input_resolution, output_size=output_resolution, \n                           test=True, modality='attention' , imshow=False)\n    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                               batch_size=1,\n                                               shuffle=True,\n                                               num_workers=0)\n    encoded_inputs = next(iter(val_loader))\n    img, face, pose, depth, gaze_field, gt_direction, head_channel, gaze_heatmap, cont_gaze, imsize, path, eye_point, all_object_bboxes, all_object_bboxes_class, gazed_object_bbox, gazed_object_class = encoded_inputs\n\n    plt.imshow(img.squeeze(0).numpy().transpose(1,2,0))\n    plt.show()\n    plt.imshow(depth.squeeze(0).numpy().transpose(1,2,0))\n    plt.show()\n    plt.imshow(pose.squeeze(0).numpy().transpose(1,2,0))\n    plt.show()\n    plt.imshow(face.squeeze(0).numpy().transpose(1,2,0))\n    plt.show()\n    plt.imshow(gaze_field.squeeze(0).numpy().transpose(1,2,0)[:,:,0])\n    plt.show()\n    plt.imshow(gaze_field.squeeze(0).numpy().transpose(1,2,0)[:,:,1])\n    plt.show()\n    print(gt_direction)\n    plt.imshow(head_channel.squeeze(0).numpy().transpose(1,2,0))\n    plt.show()\n    plt.imshow(gaze_heatmap.squeeze(0).numpy())\n    plt.show()\n    print(cont_gaze)\n    print(imsize)\n    print(path)\n    print(eye_point)\n    print(all_object_bboxes.shape)\n    print(all_object_bboxes_class.shape)\n    print(gazed_object_bbox)\n    print(gazed_object_class)\n    \nelse:\n    ds = retailGazeDataset(df_test, transform, transform_modality, \n                       input_size=input_resolution, output_size=output_resolution, \n                       test=True,imshow=False)\n    dl = torch.utils.data.DataLoader(ds,batch_size=1,shuffle=False,num_workers=0)\n    encoded_inputs = next(iter(dl))\n    img, head, pose, depth, gaze_field, gt_direction, head_channel, gaze_heatmap, cont_gaze, imsize, gaze_inside, path,path_seg_mask = encoded_inputs\n    plt.imshow(img.squeeze(0).numpy().transpose(1,2,0))\n    plt.show()\n    plt.imshow(head.squeeze(0).numpy().transpose(1,2,0))\n    plt.show()\n    plt.imshow(pose.squeeze(0).numpy().transpose(1,2,0))\n    plt.show()\n    plt.imshow(depth.squeeze(0).numpy().transpose(1,2,0))\n    plt.show()\n    plt.imshow(gaze_field.squeeze(0).numpy().transpose(1,2,0)[:,:,0])\n    plt.show()\n    plt.imshow(gaze_field.squeeze(0).numpy().transpose(1,2,0)[:,:,1])\n    plt.show()\n    print(gt_direction)\n    plt.imshow(head_channel.squeeze(0).numpy().transpose(1,2,0))\n    plt.show()\n    plt.imshow(gaze_heatmap.squeeze(0).numpy())\n    plt.show()\n    print(cont_gaze)\n    print(imsize)\n    print(path)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:39:27.908782Z","iopub.execute_input":"2023-04-26T11:39:27.910998Z","iopub.status.idle":"2023-04-26T11:39:30.224023Z","shell.execute_reply.started":"2023-04-26T11:39:27.910962Z","shell.execute_reply":"2023-04-26T11:39:30.222919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(model_weights, val_loader, batch_size=48, device=0, mode='dict', save_path=None):\n\n    # Load model\n    print(\"Constructing model\")\n    if mode=='pt':\n        pretrained_dict = torch.load(model_weights)\n    elif mode=='dict':\n        pretrained_dict = model_weights\n    \n    if extended_model_present:\n        if pretrained_dict['modality'] == 'attention':\n            model_base = AttentionModelCombined(cone_mode=pretrained_dict['cone_mode'], pred_inout=pretrained_dict['pred_inout'])\n        else:\n            model_base = BaselineModel(pretrained_dict['backbone_name'], pretrained_dict['modality'], cone_mode=pretrained_dict['cone_mode'], pred_inout=pretrained_dict['pred_inout'])\n        model_base.cuda().to(device)\n        model = attentionModelBboxHead(model_base)\n   \n    else:\n        if pretrained_dict['modality'] == 'attention':\n            model = AttentionModelCombined(cone_mode=pretrained_dict['cone_mode'], pred_inout=pretrained_dict['pred_inout'])\n        else:\n            model = BaselineModel(pretrained_dict['backbone_name'], pretrained_dict['modality'], cone_mode=pretrained_dict['cone_mode'], pred_inout=pretrained_dict['pred_inout'])\n        \n\n    model.cuda().to(device)\n    model_dict = model.state_dict()\n    model_dict.update(pretrained_dict['model'])\n    model.load_state_dict(model_dict)\n\n    print('Evaluation in progress ...')\n    model.train(False)\n    gt_gaze = []; pred_hm = []; image_size = [] ; paths = []; pred_att = []; directions = []\n    gt_eye_point = []\n    all_object_bboxes_list = []\n    all_object_bboxes_class_list = []\n    gazed_object_bbox_list = []\n    gazed_object_class_list = []\n    all_result_list_ttfnet = [] \n    with torch.no_grad():\n        for val_batch, (val_img, val_face, val_pose, val_depth, val_gaze_field, val_gt_direction, val_head_channel, val_gaze_heatmap, cont_gaze, imsize, path, eye_point, all_object_bboxes, all_object_bboxes_class, gazed_object_bbox, gazed_object_class) in tqdm(enumerate(val_loader), total=len(val_loader)):\n            \n            val_images = val_img.cuda().to(device)\n            val_faces = val_face.cuda().to(device)\n            val_head_channels = val_head_channel.cuda().to(device)\n            val_gaze_fields = val_gaze_field.cuda().to(device)\n            val_depth_maps = val_depth.cuda().to(device)\n            val_pose_maps = val_pose.cuda().to(device)\n            val_gt_direction = val_gt_direction.cuda().to(device)\n            \n            # choose input modality\n            if pretrained_dict['modality'] == 'image':\n                model_input = val_images\n            elif pretrained_dict['modality'] == 'pose':\n                model_input = val_pose_maps\n            elif pretrained_dict['modality'] == 'depth':\n                model_input = val_depth_maps\n            elif pretrained_dict['modality'] == 'attention':\n                model_input = [val_images, val_depth_maps, val_pose_maps]\n            if pretrained_dict['modality'] == 'attention':\n                if extended_model_present:\n                    val_gaze_heatmap_pred, direction, val_inout_pred, val_att, val_pred_wh = model(model_input, val_faces, val_gaze_fields, val_head_channels)\n                else:\n                    val_gaze_heatmap_pred, direction, val_inout_pred, val_att = model(model_input, val_faces, val_gaze_fields, val_head_channels)\n                pred_att.extend(val_att.cpu().numpy())\n            else:\n                val_gaze_heatmap_pred, direction, val_inout_pred = model(model_input, val_faces, val_gaze_fields, val_head_channels)\n            val_gaze_heatmap_pred = val_gaze_heatmap_pred.squeeze(1)\n            \n            gt_gaze.extend(cont_gaze)\n            gt_eye_point.extend(eye_point)\n            pred_hm.extend(val_gaze_heatmap_pred.cpu().numpy())\n            image_size.extend(imsize)\n            paths.extend(path)\n            directions.extend(direction.cpu().numpy())\n            all_object_bboxes_list.extend(all_object_bboxes.cpu().numpy().astype(int)) \n            all_object_bboxes_class_list.extend(all_object_bboxes_class.cpu().numpy().astype(int)) \n            \n            gazed_object_bbox_list.extend(gazed_object_bbox.cpu().numpy().astype(int))\n            gazed_object_class_list.extend(gazed_object_class.cpu().numpy().astype(int))\n            if extended_model_present:\n                \n                result_list_ttfnet = get_bboxes_ttfnet(val_gaze_heatmap_pred,val_pred_wh)\n            \n                if torch.numel(result_list_ttfnet[0][0]) > 0:\n                    all_result_list_ttfnet.extend([result_list_ttfnet[0][0].cpu().numpy()])\n                    \n                else:\n                    #all_result_list_ttfnet.extend(np.expand_dims(np.array([-1,-1,-1,-1,-1]), axis=0))\n                    all_result_list_ttfnet.extend([np.expand_dims(np.array([-1,-1,-1,-1,-1]), axis=0)])\n                \n            if val_batch% 100 == 0:\n                #th = 0.5\n                path = str(path[0])\n                print(path)\n                imsize = imsize[0].int().numpy()\n                gt_object_bbox = gazed_object_bbox.squeeze(0).cpu().numpy().astype(int)\n                print(gt_object_bbox)\n                print(all_result_list_ttfnet[val_batch])\n                print(torch.numel(result_list_ttfnet[0][0])/5)\n                \n                pred_object_box_ttfnet = []\n                        \n                img1 = Image.open(os.path.join(image_dir, path))\n                img1 = np.array(img1.convert('RGB').resize((640,480)))\n                img2 = Image.open(os.path.join(image_dir, path))\n                img2 = np.array(img2.convert('RGB').resize((640,480)))\n                print(f'In vs Out: {val_inout_pred.cpu().numpy()}')\n                if pred_att:\n                    print(f'Attention weights image | depth | pose: {val_att[:, 0].cpu().numpy()} | {val_att[:, 1].cpu().numpy()} | {val_att[:, 2].cpu().numpy()}')\n                fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20, 15))\n                ax1.axis('off')\n                ax2.axis('off')\n                ax1.set_title('GT',size=24,fontweight=\"bold\")\n                ax1.imshow(img1)\n                ax1.imshow(cv2.rectangle(img1, (gt_object_bbox[0],gt_object_bbox[1]), (gt_object_bbox[2],gt_object_bbox[3]), (0,255,0), 2))\n                ax1.imshow(cv2.resize(val_gaze_heatmap.squeeze(0).cpu().numpy(),(WIDTH,HEIGHT)), cmap='jet', alpha=0.5)\n                \n                ax2.set_title('Pred',size=24,fontweight=\"bold\")\n                ax2.imshow(img2)\n                if torch.numel(result_list_ttfnet[0][0]) > 0:\n                    for index in range(int(torch.numel(result_list_ttfnet[0][0])/5)):\n                        cur_box = result_list_ttfnet[0][0].cpu().numpy()[index]\n                        # get predicted BBox in 640x480\n                        pred_object_box_ttfnet = torch.tensor([cur_box[0]*imsize[0]/output_resolution,\n                                                            cur_box[1]*imsize[1]/output_resolution,\n                                                            cur_box[2]*imsize[0]/output_resolution,\n                                                            cur_box[3]*imsize[1]/output_resolution]).int().numpy()\n                        ax2.imshow(cv2.rectangle(img2, (pred_object_box_ttfnet[0],pred_object_box_ttfnet[1]), (pred_object_box_ttfnet[2],pred_object_box_ttfnet[3]), (255,0,0), 2))\n                ax2.imshow(cv2.resize(val_gaze_heatmap_pred.cpu().numpy().transpose(1,2,0), (WIDTH, HEIGHT)), cmap='jet', alpha=0.5)\n                plt.show()\n                \n    \n    if extended_model_present:\n        AUC, min_dist, avg_dist, avg_ang, count_matching_object_class, count_matching_object_class_ap_50, avg_ang_hm, avg_pred_hm_energy_in_gt_bbox, avg_pred_hm_energy_in_gt_cat = compute_metrics_extend(pred_hm, gt_gaze, image_size, gt_eye_point, all_object_bboxes_list, all_object_bboxes_class_list, gazed_object_class_list, all_result_list_ttfnet, directions, gazed_object_bbox_list)\n    else:\n        AUC, min_dist, avg_dist, avg_ang, count_matching_object_class = compute_metrics(pred_hm, gt_gaze, image_size, gt_eye_point, all_object_bboxes_list, all_object_bboxes_class_list, gazed_object_class_list)\n    if save_path is not None:\n        output = {}\n        if pretrained_dict['modality'] == 'attention':\n            output['pred_att'] = pred_att\n        output['pred_hm'] = pred_hm \n        output['gt_gaze'] = gt_gaze\n        output['gazed_object_bbox'] = gazed_object_bbox_list\n        output['paths'] = paths\n        output['AUC'] = AUC \n        output['min_dist'] = min_dist \n        output['avg_dist'] = avg_dist \n        output['pred_direction'] = directions\n        output['avg_ang'] = avg_ang\n        output['avg_ang_hm'] = avg_ang_hm\n        output['count_matching_object_class'] = count_matching_object_class\n        output['avg_pred_hm_energy_in_gt_bbox'] = avg_pred_hm_energy_in_gt_bbox\n        if extended_model_present:\n            output['count_matching_object_class_ap_50'] = count_matching_object_class_ap_50\n            output['avg_pred_hm_energy_in_gt_cat'] = avg_pred_hm_energy_in_gt_cat\n            output['pred_bboxes_per_img'] = all_result_list_ttfnet\n        with open(os.path.join(save_path, 'output_gazefollow.pkl'), 'wb') as fp:\n            pickle.dump(output, fp)    \n    \n    \n    final_AUC = np.mean(AUC)\n    final_min_dist = np.mean(min_dist)\n    final_avg_dist = np.mean(avg_dist)\n    final_avg_ang = np.mean(avg_ang)\n    final_avg_ang_hm = np.mean(avg_ang_hm)\n    final_count_matching_object_class = np.mean(count_matching_object_class) * 100\n    if extended_model_present:\n        final_count_matching_object_class_ap_50 = np.mean(count_matching_object_class_ap_50) * 100\n        final_avg_pred_hm_energy_in_gt_bbox = np.mean(avg_pred_hm_energy_in_gt_bbox)\n        final_avg_pred_hm_energy_in_gt_cat = np.mean(avg_pred_hm_energy_in_gt_cat)\n        \n    if pred_att:\n        avg_attention_weights = [sum(x) / len(x) for x in zip(*pred_att)]\n        print(f'Avg. Attention weights image | depth | pose: {avg_attention_weights[0]} | {avg_attention_weights[1]} | {avg_attention_weights[2]}')\n    \n    if extended_model_present:\n        print(\"\\tAUC:{:.4f}\\t min dist:{:.4f}\\t avg dist:{:.4f}\\t avg ang:{:.4f}\\t Object prediction Acc. (%):{:.4f}\\t BBoX head Object prediction Acc. (%):{:.4f} \\t avg ang hm:{:.4f} \\t avg_pred_hm_energy_in_gt_bbox (%): {:.4f} \\t avg_pred_hm_energy_in_gt_cat (%): {:.4f}\".format(\n          final_AUC,\n          final_min_dist,\n          final_avg_dist,\n          final_avg_ang,\n          final_count_matching_object_class,\n          final_count_matching_object_class_ap_50,\n          final_avg_ang_hm,\n          final_avg_pred_hm_energy_in_gt_bbox,\n          final_avg_pred_hm_energy_in_gt_cat))\n    \n        return final_AUC, final_min_dist, final_avg_dist, final_avg_ang, final_count_matching_object_class, final_count_matching_object_class_ap_50\n   \n    print(\"\\tAUC:{:.4f}\\t min dist:{:.4f}\\t avg dist:{:.4f}\\t avg ang:{:.4f} \\t Object prediction Acc.:{:.4f}\".format(\n          final_AUC,\n          final_min_dist,\n          final_avg_dist,\n          final_avg_ang,\n          final_count_matching_object_class))\n    \n    return final_AUC, final_min_dist, final_avg_dist, final_avg_ang, final_count_matching_object_class\n\ndef compute_metrics(pred_hm, gt_gaze, image_size, gt_eye_point, all_object_bboxes_list, all_object_bboxes_class_list, gazed_object_class_list, eps = 1e-8):\n    \n    AUC = []; min_dist = []; avg_dist = []\n    avg_ang = []\n    count_matching_object_class = []\n    # go through each data point and record AUC, min dist, avg dist\n    for b_i in tqdm(range(len(gt_gaze))):\n        # remove padding and recover valid ground truth points\n        valid_gaze = gt_gaze[b_i]        \n        valid_gaze = valid_gaze[valid_gaze != -1].view(-1,2)\n        \n        valid_eye_point = gt_eye_point[b_i]\n        valid_eye_point = valid_eye_point[valid_eye_point != -1].view(-1,2)\n        \n        # AUC: area under curve of ROC\n        pm = pred_hm[b_i]\n        multi_hot = multi_hot_targets(gt_gaze[b_i], image_size[b_i])\n        scaled_heatmap = cv2.resize(pm, (image_size[b_i][0].item(), image_size[b_i][1].item()))\n        auc_score = evaluation.auc(scaled_heatmap, multi_hot)\n        AUC.append(auc_score)\n        # min distance: minimum among all possible pairs of <ground truth point, predicted point>\n        pred_x, pred_y = evaluation.argmax_pts(pm)\n        norm_p = [pred_x/float(output_resolution), pred_y/float(output_resolution)]\n        all_distances = []\n        for gaze in valid_gaze:\n            all_distances.append(evaluation.L2_dist(gaze, norm_p))\n        min_dist.append(min(all_distances))\n        # average distance: distance between the predicted point and human average point\n        mean_gt_gaze = torch.mean(valid_gaze, 0)\n        avg_distance = evaluation.L2_dist(mean_gt_gaze, norm_p)\n        avg_dist.append(avg_distance)\n        mean_gt_gaze_direction = mean_gt_gaze - valid_eye_point\n        mean_pred_gaze_direction = torch.tensor(norm_p) - valid_eye_point\n        \n        #f_cos_sim = (np.dot(mean_gt_gaze_direction, mean_pred_gaze_direction)/((norm(mean_gt_gaze_direction)*norm(mean_pred_gaze_direction))+eps))\n        #f_cos_sim = np.maximum(np.minimum(f_cos_sim, 1.0), -1.0)\n        #avg_ang.append(np.rad2deg(np.arccos(f_cos_sim)))\n        avg_ang.append(torch.rad2deg(torch.acos(cos_sim_func(mean_gt_gaze_direction,mean_pred_gaze_direction))).item())\n        \n        cur_all_object_bboxes_list = np.array(all_object_bboxes_list[b_i])\n        cur_all_object_bboxes_class_list = np.array(all_object_bboxes_class_list[b_i])\n        cur_gazed_object_class = np.array(gazed_object_class_list[b_i])\n        count_matching_object_class.extend(\n            match_object_cat_after_get_predicted_bbox_from_energy(\n                scaled_heatmap,\n                cur_all_object_bboxes_list, \n                cur_all_object_bboxes_class_list,\n                cur_gazed_object_class))\n        \n    return np.array(AUC), np.array(min_dist), np.array(avg_dist), np.abs(np.array(avg_ang)),  np.array(count_matching_object_class)\n\n\ndef compute_metrics_extend(pred_hm, gt_gaze, image_size, gt_eye_point, all_object_bboxes_list, \n                    all_object_bboxes_class_list, gazed_object_class_list, \n                    all_result_list_ttfnet, directions, gazed_object_bbox_list, eps = 1e-8):\n    \n    AUC = []; min_dist = []; avg_dist = []\n    avg_ang = []\n    avg_ang_hm = []\n    count_matching_object_class = []\n    count_matching_object_class_ap_50 = []\n    avg_pred_hm_energy_in_gt_bbox = []\n    avg_pred_hm_energy_in_gt_cat = []\n    # go through each data point and record AUC, min dist, avg dist\n    for b_i in tqdm(range(len(gt_gaze))):\n        \n        # remove padding and recover valid ground truth points\n        valid_gaze = gt_gaze[b_i]        \n        valid_gaze = valid_gaze[valid_gaze != -1].view(-1,2)\n        \n        valid_eye_point = gt_eye_point[b_i]\n        valid_eye_point = valid_eye_point[valid_eye_point != -1].view(-1,2)\n        \n        valid_pred_direction = torch.tensor(directions[b_i])\n        # AUC: area under curve of ROC\n        pm = pred_hm[b_i]\n        multi_hot = multi_hot_targets(gt_gaze[b_i], image_size[b_i])\n        scaled_heatmap = cv2.resize(pm, (image_size[b_i][0].item(), image_size[b_i][1].item()))\n        auc_score = evaluation.auc(scaled_heatmap, multi_hot)\n        AUC.append(auc_score)\n        # min distance: minimum among all possible pairs of <ground truth point, predicted point>\n        pred_x, pred_y = evaluation.argmax_pts(pm)\n        norm_p = [pred_x/float(output_resolution), pred_y/float(output_resolution)]\n        all_distances = []\n        for gaze in valid_gaze:\n            all_distances.append(evaluation.L2_dist(gaze, norm_p))\n        min_dist.append(min(all_distances))\n        # average distance: distance between the predicted point and human average point\n        mean_gt_gaze = torch.mean(valid_gaze, 0)\n        avg_distance = evaluation.L2_dist(mean_gt_gaze, norm_p)\n        avg_dist.append(avg_distance)\n        \n        \n        mean_gt_gaze_direction = mean_gt_gaze - valid_eye_point\n        mean_pred_gaze_direction = torch.tensor(norm_p) - valid_eye_point\n        avg_ang.append(torch.rad2deg(torch.acos(cos_sim_func(mean_gt_gaze_direction,valid_pred_direction))).item())\n        \n        mean_pred_gaze_direction = mean_pred_gaze_direction.squeeze().numpy()\n        mean_gt_gaze_direction = mean_gt_gaze_direction.squeeze().numpy()\n        norm_f = (mean_pred_gaze_direction[0] ** 2 + mean_pred_gaze_direction[1] ** 2) ** 0.5\n        norm_gt = (mean_gt_gaze_direction[0] ** 2 + mean_gt_gaze_direction[1] ** 2) ** 0.5\n\n        f_cos_sim = (mean_pred_gaze_direction[0] * mean_gt_gaze_direction[0] + \n                     mean_pred_gaze_direction[1] * mean_gt_gaze_direction[1]) / \\\n                    (norm_gt * norm_f + eps)\n        f_cos_sim = np.maximum(np.minimum(f_cos_sim, 1.0), -1.0)\n        f_angle = np.arccos(f_cos_sim) * 180 / np.pi\n        avg_ang_hm.extend([f_angle])\n                        \n        cur_all_object_bboxes_list = np.array(all_object_bboxes_list[b_i])\n        cur_all_object_bboxes_class_list = np.array(all_object_bboxes_class_list[b_i])\n        cur_gazed_object_class = np.array(gazed_object_class_list[b_i])\n        count_matching_object_class.extend(\n            match_object_cat_after_get_predicted_bbox_from_energy(\n                scaled_heatmap,\n                cur_all_object_bboxes_list, \n                cur_all_object_bboxes_class_list,\n                cur_gazed_object_class))\n        \n        cur_all_result_list_ttfnet = np.array(all_result_list_ttfnet[b_i])\n        #print(cur_all_result_list_ttfnet)\n        count_matching_object_class_ap_50.extend(\n            match_object_cat_from_ttffnet_regression(\n            cur_all_result_list_ttfnet,\n            image_size[b_i],\n            cur_all_object_bboxes_list,\n            cur_all_object_bboxes_class_list,\n            cur_gazed_object_class))\n        \n        avg_pred_hm_energy_in_gt_bbox.extend([\n        get_avg_energy_by_gtbox_predheatmap(\n            gazed_object_bbox_list[b_i],\n            scaled_heatmap)])\n        \n        avg_pred_hm_energy_in_gt_cat.extend([\n        get_avg_energy_in_gtcat_predheatmap(\n            cur_all_object_bboxes_list,\n            scaled_heatmap,\n            cur_gazed_object_class,\n            cur_all_object_bboxes_class_list)])\n        \n            \n    return np.array(AUC), np.array(min_dist), np.array(avg_dist), np.abs(np.array(avg_ang)),  np.array(count_matching_object_class), np.array(count_matching_object_class_ap_50), np.abs(np.array(avg_ang_hm)), np.array(avg_pred_hm_energy_in_gt_bbox), np.array(avg_pred_hm_energy_in_gt_cat)\n\n# Get energy aggregation loss from GaTector paper\ndef get_avg_energy_by_gtbox_predheatmap(box, heatmap, width=640, height=480):\n    \n    # Use ground truth box and predicted heatmap to compute the energy aggregation loss\n    # GT bbox is passed in size (640, 480)\n    # Check the scale factor orginal = 10 *\n    power, total_power = 0., 0.\n    eng = 0.\n    cur_box = box\n   \n    cur_heatmap = heatmap\n    #cur_heatmap = np.maximum(heatmap,0)\n    # axis are flipped in the heatmap\n    power = np.sum(cur_heatmap[cur_box[1]: cur_box[3] + 1, cur_box[0]: cur_box[2] + 1])\n    total_power = cur_heatmap.sum()\n    if total_power > 0:\n        eng = (power / total_power) * 100\n    \n    return eng \n\ndef get_avg_energy_in_gtcat_predheatmap(all_object_bboxes, scaled_pred_heatmap, gazed_object_class, all_object_bboxes_class, width=640, height=480):\n    \n    # Use ground truth boxes and predicted heatmap to compute the energy aggregation loss\n    # Since GT Object BBoXes overlap hence it is possible that cat_power > total_power\n    # And cat_eng becomes > 100%\n    cat_power, total_power = 0., 0.\n    cat_eng = 0.\n    cat_boxes = all_object_bboxes[np.where(all_object_bboxes_class == gazed_object_class)[0]]\n    cur_heatmap = scaled_pred_heatmap\n    total_power = cur_heatmap.sum()\n    \n    #cur_heatmap = np.maximum(heatmap,0)\n    # axis are flipped in the heatmap\n    if total_power > 0:\n        for index in range (len(cat_boxes)):\n            cur_box = cat_boxes[index]\n            cat_power += np.sum(cur_heatmap[cur_box[1]: cur_box[3] + 1, cur_box[0]: cur_box[2] + 1])\n            \n        cat_eng = np.minimum((cat_power / total_power) * 100 , 100)\n    \n    return cat_eng \n\n# Iterate through all GT boxes and calc mean energy of BBox. \n# Predict the BBox with max mean energy\n\ndef match_object_cat_after_get_predicted_bbox_from_energy(scaled_pred_heatmap,all_object_bboxes, all_object_bboxes_class, gazed_object_class):\n    \"\"\"\n    Use pred heatmap to find the GT object bboxes with maximum energy as gaze target\n    \"\"\"\n    max_energy = 0.\n    pred_bbox = None\n    cur_all_object_bboxes = all_object_bboxes\n    cur_all_object_bboxes_class = all_object_bboxes_class\n    # All Bboxes are in original image resolution\n    cur_pred_heatmap = scaled_pred_heatmap\n    #cur_pred_heatmap = np.maximum(scaled_pred_heatmap,0)\n    cur_gazed_object_class = gazed_object_class\n    \n    for ind, cur_box in enumerate(cur_all_object_bboxes):\n        xmin, ymin, xmax, ymax =  cur_box\n        # axis are flipped in the heatmap\n        no_of_pixels_in_box = (xmax+1-xmin) * (ymax+1-ymin)\n        mean_energy = np.sum(cur_pred_heatmap[ymin: ymax + 1, xmin: xmax + 1])/(no_of_pixels_in_box)\n        #mean_energy = torch.sum(cur_pred_heatmap[ymin: ymax + 1, xmin: xmax + 1])\n        if mean_energy > max_energy:\n            max_energy = mean_energy\n            pred_bbox = cur_box\n            pred_box_class = cur_all_object_bboxes_class[ind]\n\n    if pred_bbox is not None:\n        if int(pred_box_class) == int(cur_gazed_object_class):\n            return [1.] \n        else:\n            return [0.]\n    else:\n        return [0.]\n\n\n\n'''\n# Iterate through all GT Object category and calc total mean energy of this category. \n# Predict the Object category with max total energy\ndef match_object_cat_after_get_predicted_bbox_from_energy(scaled_pred_heatmap,all_object_bboxes, all_object_bboxes_class, gazed_object_class):\n    \"\"\"\n    Use pred heatmap to find the GT object bboxes with maximum energy as gaze target\n    \"\"\"\n    max_energy_in_cat = 0.\n    pred_box_class = None\n    cur_all_object_bboxes = all_object_bboxes\n    cur_all_object_bboxes_class = all_object_bboxes_class\n    # All Bboxes are in original image resolution\n    cur_pred_heatmap = scaled_pred_heatmap\n    cur_gazed_object_class = gazed_object_class\n    \n    #Object categories in the scene is from 1-24\n    for cat_ind in range(1,25):\n        cat_boxes = all_object_bboxes[np.where(all_object_bboxes_class == cat_ind)[0]]\n        total_mean_energy_in_cat = 0.0\n        for index in range (len(cat_boxes)):\n            xmin, ymin, xmax, ymax = cat_boxes[index]\n         \n            # axis are flipped in the heatmap\n            no_of_pixels_in_box = (xmax+1-xmin) * (ymax+1-ymin)\n        \n            total_mean_energy_in_cat += np.sum(cur_pred_heatmap[ymin: ymax + 1, xmin: xmax + 1])/(no_of_pixels_in_box)\n            \n            \n        if total_mean_energy_in_cat > max_energy_in_cat:\n            max_energy_in_cat = total_mean_energy_in_cat\n            pred_box_class = cat_ind\n\n    if pred_box_class is not None:\n        if int(pred_box_class) == int(cur_gazed_object_class):\n            return [1.] \n        else:\n            return [0.]\n    else:\n        return [0.]\n'''\n\n    \ndef match_object_cat_from_ttffnet_regression(cur_all_result_list_ttfnet, cur_image_size, all_object_bboxes, all_object_bboxes_class, gazed_object_class):\n    cat_wuoc = 0.0\n    cat_ious = 0.0\n    for ind, cur_box in enumerate(cur_all_result_list_ttfnet):\n        \n        # get predicted BBox in 64x64\n        pred_gazed_object_box_from_reg = cur_box[:4]\n        if pred_gazed_object_box_from_reg[0] == -1:\n            break\n        # get predicted BBox in 640x480\n        pred_gazed_object_box_from_reg_imsize = torch.tensor([pred_gazed_object_box_from_reg[0]*cur_image_size[0]/output_resolution,\n                                                              pred_gazed_object_box_from_reg[1]*cur_image_size[1]/output_resolution,\n                                                              pred_gazed_object_box_from_reg[2]*cur_image_size[0]/output_resolution,\n                                                              pred_gazed_object_box_from_reg[3]*cur_image_size[1]/output_resolution]).int()\n        pred_gazed_object_box_from_reg_imsize = pred_gazed_object_box_from_reg_imsize.unsqueeze(dim=0)\n        ious, wuocs = bbox_overlaps_ttfnet(torch.tensor(all_object_bboxes), pred_gazed_object_box_from_reg_imsize, mode='iou', is_aligned=False)\n        cat_ious = (ious[np.where(all_object_bboxes_class == gazed_object_class)].sum())\n        #print(f'cat_ious.sum() for BBoX {ind}: {cat_ious}')\n        cat_wuoc = (wuocs[np.where(all_object_bboxes_class == gazed_object_class)].sum())\n        #print(f'cat_wuoc.sum() for BBoX {ind}: {cat_wuoc}')\n        if int(all_object_bboxes_class[torch.argmax(ious,axis=0)]) == int(gazed_object_class) and torch.argmax(ious,axis=0)>=0.5:\n        #if int(all_object_bboxes_class[torch.argmax(wuocs,axis=0)]) == int(gazed_object_class):\n            return [1.]\n    \n    return [0.]\n\nif __name__ == \"__main__\" and flag_run_on_goo_dataset:\n    \n    #model_weights = '/kaggle/input/mm-gaze-target-prediction-new-weights/goo-real-original-model_epoch_14.pt'\n    #model_weights = '/kaggle/input/mm-gaze-target-prediction-new-weights/goo-real-original-model-ttfnet-gaussian_epoch_7.pt'\n    #model_weights = '/kaggle/input/mm-gaze-target-prediction-new-weights/goo-real-original-model-ttfnet-gaussian-only-pose-modality_epoch_20.pt'\n    #model_weights = '/kaggle/input/mm-gaze-target-prediction-new-weights/goo-real-original-model-ttfnet-complete_2_epoch_12.pt'\n    #model_weights = '/kaggle/input/mm-gaze-target-prediction-new-weights/goo-real-original-model-ttfnet-focal_loss_only_epoch_15.pt'\n    #model_weights = '/kaggle/input/mm-gaze-target-prediction-new-weights/goo-real-original-model-energy_aggr_loss_epoch_20.pt'\n    #model_weights = '/kaggle/input/mm-gaze-target-prediction-new-weights/goo-real-plus-synth-original-model-energy_aggr_loss_epoch_10.pt'\n    #model_weights = '/kaggle/input/mm-gaze-target-prediction-new-weights/goo-real-original-model-energy_wh_loss_epoch_4.pt'\n    #model_weights = '/kaggle/input/mm-gaze-target-prediction-new-weights/goo-real-original-model-energy_aggr_loss_plus_focal_loss_epoch_13.pt'\n    model_weights = '/kaggle/input/mm-gaze-target-prediction-new-weights/goo-real-original-model-focal-ciou-regr_epoch_27.pt'\n    #model_weights = '/kaggle/input/mm-gaze-target-prediction-new-weights/goo-real-efficientnet_b2_epoch_32.pt'\n    device = ('cuda:0' if torch.cuda.is_available() else 'cpu')\n    print(\"Loading Data\")\n\n    transform = _get_transform()\n    transform_modality = _get_transform_modality()\n    \n    val_dataset = GooRealDataset(df_test, transform, transform_modality, \n                             input_size=input_resolution, output_size=output_resolution,\n                             modality=modality, test=True)\n    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                               batch_size=1,\n                                               shuffle=True,\n                                               num_workers=2)\n    cos_sim_func = nn.CosineSimilarity(dim=1, eps=1e-8)\n    if extended_model_present:\n        final_AUC, final_min_dist, final_avg_dist, final_avg_ang, final_count_matching_object_class, final_count_matching_object_class_ap_50 = test(model_weights, val_loader, batch_size=1, device=0, mode='pt', save_path = '/kaggle/working/')\n\n    else:\n        final_AUC, final_min_dist, final_avg_dist, final_avg_ang,final_count_matching_object_class = test(model_weights, val_loader, batch_size=1, device=0, mode='pt', save_path = '/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:39:30.226022Z","iopub.execute_input":"2023-04-26T11:39:30.226407Z","iopub.status.idle":"2023-04-26T11:46:42.368211Z","shell.execute_reply.started":"2023-04-26T11:39:30.226364Z","shell.execute_reply":"2023-04-26T11:46:42.366911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ngoo-real-original-model_epoch_14.pt\nAvg. Attention weights image | depth | pose: [[[0.44971883]]] | [[[0.35991648]]] | [[[0.19036518]]]\nAUC:0.9553\t min dist:0.1160\t avg dist:0.1160\t avg ang:19.0602 Object prediction Acc.:35.6011\n\"\"\"\n\n\"\"\"\ngoo-real-original-model-ttfnet-gaussian_epoch_7.pt\nAvg. Attention weights image | depth | pose: [[[0.5265263]]] | [[[0.31016862]]] | [[[0.16330457]]]\n\tAUC:0.9210\t min dist:0.1117\t avg dist:0.1117\t avg ang:18.1694 \t Object prediction Acc.:38.4902\n\nSparse dataset\nAvg. Attention weights image | depth | pose: [[[0.5288933]]] | [[[0.2894723]]] | [[[0.18163462]]]\n\tAUC:0.9206\t min dist:0.1272\t avg dist:0.1272\t avg ang:22.1647 \t Object prediction Acc.:37.4355\n\"\"\"\n\n\"\"\"\ngoo-real-original-model-ttfnet-gaussian-only-pose-modality_epoch_20.pt\n\nSparse dataset\nAUC:0.8718\t min dist:0.1497\t avg dist:0.1497\t avg ang:23.6209 \t Object prediction Acc.:31.0671\n\"\"\"\n\n\"\"\"\ngoo-real-original-model-ttfnet-complete_2_epoch_12.pt\nAvg. Attention weights image | depth | pose: [[[0.64857745]]] | [[[0.23286861]]] | [[[0.11855373]]]\nAUC:0.9849\t min dist:0.1115\t avg dist:0.1115\t avg ang:18.9836\t Object prediction Acc. (%):38.0242\t BBoX head Object prediction Acc. (%):33.4110\nSparse dataset\nAvg. Attention weights image | depth | pose: [[[0.68183804]]] | [[[0.19980803]]] | [[[0.11835343]]]\nAUC:0.9870\t min dist:0.1256\t avg dist:0.1256\t avg ang:22.4442\t Object prediction Acc. (%):36.2306\t BBoX head Object prediction Acc. (%):30.7229\n\"\"\"\n\n\"\"\"\ngoo-real-original-model-ttfnet-focal_loss_only_epoch_15.pt\nAvg. Attention weights image | depth | pose: [[[0.6077914]]] | [[[0.17469546]]] | [[[0.21751298]]]\nAUC:0.9748\t min dist:0.1069\t avg dist:0.1069\t avg ang:20.1690\t Object prediction Acc. (%):41.6123\t BBoX head Object prediction Acc. (%):38.0708 \t avg ang hm:18.3665 \t avg_pred_hm_energy_in_gt_bbox (%): 12.9769\n\nSparse dataset\nAvg. Attention weights image | depth | pose: [[[0.6220978]]] | [[[0.16036293]]] | [[[0.21753924]]]\nAUC:0.9842\t min dist:0.1232\t avg dist:0.1232\t avg ang:24.2822\t Object prediction Acc. (%):38.7263\t BBoX head Object prediction Acc. (%):33.3046 \t avg ang hm:21.7696 \t avg_pred_hm_energy_in_gt_bbox (%): 14.6222\n\"\"\"\n\n\"\"\"\ngoo-real-original-model-energy_aggr_loss_epoch_20.pt\nAvg. Attention weights image | depth | pose: [[[0.48699257]]] | [[[0.29781172]]] | [[[0.2151948]]]\nAUC:0.6280\t min dist:0.1056\t avg dist:0.1056\t avg ang:19.5412\t Object prediction Acc. (%):43.8024\t BBoX head Object prediction Acc. (%):34.5760 \t avg ang hm:18.0903 \t avg_pred_hm_energy_in_gt_bbox (%): 20.9762\n# After non-RELU trained weights and RELU inference \nAvg. Attention weights image | depth | pose: [[[0.4869935]]] | [[[0.29781204]]] | [[[0.21519478]]]\nAUC:0.7840\t min dist:0.1056\t avg dist:0.1056\t avg ang:19.5412\t Object prediction Acc. (%):43.8956\t BBoX head Object prediction Acc. (%):34.5760 \t avg ang hm:18.0903 \t avg_pred_hm_energy_in_gt_bbox (%): 20.6925 \t avg_pred_hm_energy_in_gt_cat (%): 44.6343\n\nTrain \nAvg. Attention weights image | depth | pose: [[[0.5025198]]] | [[[0.29043576]]] | [[[0.2070445]]]\nAUC:0.9966\t min dist:0.0211\t avg dist:0.0211\t avg ang:3.5655\t Object prediction Acc. (%):90.4082\t BBoX head Object prediction Acc. (%):88.3673 \t avg ang hm:2.7669 \t avg_pred_hm_energy_in_gt_bbox (%): 56.2714 \t avg_pred_hm_energy_in_gt_cat (%): 88.5585\n    \nSparse dataset\nAvg. Attention weights image | depth | pose: [[[0.51403946]]] | [[[0.2729661]]] | [[[0.21299438]]]\nAUC:0.6862\t min dist:0.1205\t avg dist:0.1205\t avg ang:23.5692\t Object prediction Acc. (%):39.5009\t BBoX head Object prediction Acc. (%):25.8176 \t avg ang hm:21.2358 \t avg_pred_hm_energy_in_gt_bbox (%): 26.0571\n\"\"\"\n\n## Check whether used output relu for hm\n\n\"\"\"\ngoo-real-plus-synth-original-model-energy_aggr_loss_epoch_10.pt\nAvg. Attention weights image | depth | pose: [[[0.5025191]]] | [[[0.29169896]]] | [[[0.2057821]]]\nAUC:0.7544\t min dist:0.1179\t avg dist:0.1179\t avg ang:22.6881\t Object prediction Acc. (%):39.3290\t BBoX head Object prediction Acc. (%):23.2992 \t avg ang hm:21.0754 \t avg_pred_hm_energy_in_gt_bbox (%): nan\n\nGOO Synth tes dataset which was used for training is used\nAvg. Attention weights image | depth | pose: [[[0.3271901]]] | [[[0.36095533]]] | [[[0.31185308]]]\nAUC:0.7144\t min dist:0.1610\t avg dist:0.1610\t avg ang:18.5566\t Object prediction Acc. (%):30.1927\t BBoX head Object prediction Acc. (%):24.5104 \t avg ang hm:41.6436 \t avg_pred_hm_energy_in_gt_bbox (%): 19.0564 \t avg_pred_hm_energy_in_gt_cat (%): 27.9017\n    \n\"\"\"\n\n\"\"\"\ngoo-real-original-model-energy_wh_loss_epoch_4.pt\nAvg. Attention weights image | depth | pose: [[[0.5299074]]] | [[[0.29873222]]] | [[[0.17135933]]]\nAUC:0.7459\t min dist:0.1104\t avg dist:0.1104\t avg ang:19.6288\t Object prediction Acc. (%):42.8705\t BBoX head Object prediction Acc. (%):42.1715 \t avg ang hm:18.8043 \t avg_pred_hm_energy_in_gt_bbox (%): 21.0518 \t avg_pred_hm_energy_in_gt_cat (%): 44.8470\n\"\"\"\n\n\"\"\"\n\nAvg. Attention weights image | depth | pose: [[[0.50555205]]] | [[[0.2576175]]] | [[[0.2368302]]]\nAUC:0.9558\t min dist:0.1102\t avg dist:0.1102\t avg ang:24.2152\t Object prediction Acc. (%):40.0746\t BBoX head Object prediction Acc. (%):6.6636 \t avg ang hm:20.9180 \t avg_pred_hm_energy_in_gt_bbox (%): 16.2313 \t avg_pred_hm_energy_in_gt_cat (%): 40.9557\n\"\"\"\n\n\"\"\"\nttfnet bbox prediction head inp -> ximg\ngoo-real-original-model-focal-ciou-regr_epoch_27.pt\nAvg. Attention weights image | depth | pose: [[[0.52287644]]] | [[[0.2664772]]] | [[[0.21064633]]]\nAUC:0.9801\t min dist:0.1093\t avg dist:0.1093\t avg ang:22.8425\t Object prediction Acc. (%):40.9133\t BBoX head Object prediction Acc. (%):41.0531 \t avg ang hm:20.0835 \t avg_pred_hm_energy_in_gt_bbox (%): 14.1286 \t avg_pred_hm_energy_in_gt_cat (%): 37.3826\n\n# Object prediction Acc. -> calc with product cat with max total mean energy\n# BBoX head Object prediction Acc.-> calc with any BBox in max BBoxes per image is 5 \n# (select all pred BBox with conf > 0.1) matches with GT BBox\nAvg. Attention weights image | depth | pose: [[[0.522876]]] | [[[0.2664773]]] | [[[0.21064635]]]\nAUC:0.9801\t min dist:0.1093\t avg dist:0.1093\t avg ang:22.8425\t Object prediction Acc. (%):39.7950\t BBoX head Object prediction Acc. (%):61.4632 \t avg ang hm:20.0835 \t avg_pred_hm_energy_in_gt_bbox (%): 14.1286 \t avg_pred_hm_energy_in_gt_cat (%): 37.3826\n\n# Test dataset -> GOO Inf\n# Object prediction Acc. -> BBox with max total mean energy\n# BBoX head Object prediction Acc.-> calc with any BBox in max BBoxes per image is 3 \n# (select all pred BBox with conf > 0.2) matches with GT BBox\nAvg. Attention weights image | depth | pose: [[[0.5275465]]] | [[[0.25919157]]] | [[[0.21326162]]]\nAUC:0.9809\t min dist:0.1060\t avg dist:0.1060\t avg ang:20.0947\t Object prediction Acc. (%):43.9682\t BBoX head Object prediction Acc. (%):53.2762 \t avg ang hm:17.9558 \t avg_pred_hm_energy_in_gt_bbox (%): 14.7562 \t avg_pred_hm_energy_in_gt_cat (%): 39.720\n\nTrain Acc\nAvg. Attention weights image | depth | pose: [[[0.5368456]]] | [[[0.2633405]]] | [[[0.19981423]]]\nAUC:0.9993\t min dist:0.0216\t avg dist:0.0216\t avg ang:4.7542\t Object prediction Acc. (%):89.3878\t BBoX head Object prediction Acc. (%):89.8367 \t avg ang hm:3.0586 \t avg_pred_hm_energy_in_gt_bbox (%): 35.1186 \t avg_pred_hm_energy_in_gt_cat (%): 71.9576\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:46:42.372035Z","iopub.execute_input":"2023-04-26T11:46:42.374282Z","iopub.status.idle":"2023-04-26T11:46:42.388337Z","shell.execute_reply.started":"2023-04-26T11:46:42.374232Z","shell.execute_reply":"2023-04-26T11:46:42.387225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(model_weights, val_loader=None, device=None, batch_size=1, mode='dict', save_path=None):\n    plot_figure = True\n    count = 0\n    # Load model\n    print(\"Constructing model\")\n    if mode=='pt':\n        pretrained_dict = torch.load(model_weights)\n    elif mode=='dict':\n        pretrained_dict = model_weights\n    \n    if extended_model_present:\n        if pretrained_dict['modality'] == 'attention':\n            model_base = AttentionModelCombined(cone_mode=pretrained_dict['cone_mode'], pred_inout=pretrained_dict['pred_inout'])\n        else:\n            model_base = BaselineModel(pretrained_dict['backbone_name'], pretrained_dict['modality'], cone_mode=pretrained_dict['cone_mode'], pred_inout=pretrained_dict['pred_inout'])\n        model_base.cuda().to(device)\n        model = attentionModelBboxHead(model_base)\n   \n    else:\n        if pretrained_dict['modality'] == 'attention':\n            model = AttentionModelCombined(cone_mode=pretrained_dict['cone_mode'], pred_inout=pretrained_dict['pred_inout'])\n        else:\n            model = BaselineModel(pretrained_dict['backbone_name'], pretrained_dict['modality'], cone_mode=pretrained_dict['cone_mode'], pred_inout=pretrained_dict['pred_inout'])\n        \n\n    model.cuda().to(device)\n    model_dict = model.state_dict()\n    model_dict.update(pretrained_dict['model'])\n    model.load_state_dict(model_dict)\n\n    print('Evaluation in progress ...')\n    model.train(False)\n    gt_gaze = [] \n    pred_hm = [] \n    gt_hm = []\n    image_size = [] \n    paths = []\n    pred_att = [] \n    directions = []\n    gt_directions = []\n    in_vs_out_groundtruth = []\n    in_vs_out_pred = []\n    with torch.no_grad():\n        for val_batch, (val_img, val_face, val_pose, val_depth, val_gaze_field, val_gt_direction, val_head_channel, val_gaze_heatmap, cont_gaze, imsize, gaze_inside, path, path_seg_mask) in tqdm(enumerate(val_loader), total=len(val_loader)):\n            count += 1\n            val_images = val_img.cuda().to(device)\n            val_faces = val_face.cuda().to(device)\n            val_head_channels = val_head_channel.cuda().to(device)\n            val_gaze_fields = val_gaze_field.cuda().to(device)\n            val_depth_maps = val_depth.cuda().to(device)\n            val_pose_maps = val_pose.cuda().to(device)\n            val_gt_direction = val_gt_direction.cuda().to(device)\n            gt_hm.extend(val_gaze_heatmap)\n            \n            # choose input modality\n            if pretrained_dict['modality'] == 'image':\n                model_input = val_images\n            elif pretrained_dict['modality'] == 'pose':\n                model_input = val_pose_maps\n            elif pretrained_dict['modality'] == 'depth':\n                model_input = val_depth_maps\n            elif pretrained_dict['modality'] == 'attention':\n                model_input = [val_images, val_depth_maps, val_pose_maps]\n            if pretrained_dict['modality'] == 'attention':\n                if extended_model_present:\n                    val_gaze_heatmap_pred, direction, val_inout_pred, val_att, val_pred_wh = model(model_input, val_faces, val_gaze_fields, val_head_channels)\n                else:\n                    val_gaze_heatmap_pred, direction, val_inout_pred, val_att = model(model_input, val_faces, val_gaze_fields, val_head_channels)\n                pred_att.extend(val_att.cpu().numpy())\n            else:\n                val_gaze_heatmap_pred, direction, val_inout_pred = model(model_input, val_faces, val_gaze_fields, val_head_channels)\n            val_gaze_heatmap_pred = val_gaze_heatmap_pred.squeeze(1)\n            \n            gt_gaze.extend(cont_gaze.cpu().numpy())\n            pred_hm.extend(val_gaze_heatmap_pred.cpu().numpy())\n            image_size.extend(imsize.cpu().numpy())\n            paths.extend(path)\n            directions.extend(direction.cpu().numpy())\n            gt_directions.extend(val_gt_direction.cpu().numpy())\n            # in vs out classification\n            in_vs_out_groundtruth.extend(gaze_inside.float().numpy())\n            in_vs_out_pred.extend(val_inout_pred.cpu().numpy())\n            \n            if plot_figure and count%50 == 0:\n                path = str(path[0])\n                print(path)\n                image_dir = '/kaggle/input/retailgaze/RetailGaze_V2_seg/RetailGaze_V2/'\n                img = Image.open(os.path.join(image_dir, path))\n                img = np.array(img.convert('RGB'))\n                \n                print(f'In vs Out: {val_inout_pred.cpu().numpy()}')\n                print(f'Attention weights image | depth | pose: {val_att[:, 0].cpu().numpy()} | {val_att[:, 1].cpu().numpy()} | {val_att[:, 2].cpu().numpy()}')\n                #ax = plt.gca()\n                #pred_x, pred_y = evaluation.argmax_pts(raw_hm)\n                fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20, 15))\n                ax1.axis('off')\n                ax2.axis('off')\n                #ax1.imshow(cv2.resize(val_images.squeeze(0).cpu().numpy().transpose(1,2,0),(WIDTH,HEIGHT)))\n                ax1.set_title('GT',size=24,fontweight=\"bold\")\n                ax1.imshow(img)\n                if path_seg_mask:\n                    path_seg_mask = str(path_seg_mask[0])\n                    print(path_seg_mask)\n                    img_mask = cv2.imread(os.path.join(image_dir, path_seg_mask))\n                    ax1.imshow(img_mask,alpha=0.25)\n                ax1.imshow(cv2.resize(val_gaze_heatmap.squeeze(0).cpu().numpy(),(WIDTH,HEIGHT)), cmap='jet', alpha=0.25)\n                \n                #circ_act = patches.Circle((np.rint(cont_gaze),np.rint(cont_gaze),height/50.0, facecolor=(1,0,0), edgecolor='none')\n                #ax.add_patch(circ_act)\n                #ax2.imshow(cv2.resize(val_images.squeeze(0).cpu().numpy().transpose(1,2,0),(WIDTH,HEIGHT)))\n                ax2.set_title('Pred',size=24,fontweight=\"bold\")\n                ax2.imshow(img)\n                if path_seg_mask:\n                    ax2.imshow(img_mask,alpha=0.25)\n                ax2.imshow(cv2.resize(val_gaze_heatmap_pred.cpu().numpy().transpose(1,2,0), (WIDTH, HEIGHT)), cmap='jet', alpha=0.25)\n                plt.show()\n                    \n    \n    \n    AUC, distance, cos_sim = compute_metrics(pred_hm, gt_hm, gt_gaze,directions,gt_directions)\n    if save_path is not None:\n        output = {}\n        output['pred_att'] = pred_att\n        output['pred_hm'] = pred_hm; output['gt_gaze'] = gt_gaze; output['paths'] = paths; output['direction'] = directions, output['gt_inout'] = in_vs_out_groundtruth\n        output['AUC'] = AUC; output['gt_directions'] = gt_directions; output['pred_inout'] = in_vs_out_pred;  output['cos_sim'] = cos_sim\n        with open(os.path.join(save_path, 'output_retailgaze.pkl'), 'wb') as fp:\n            pickle.dump(output, fp)    \n            \n    final_AUC = torch.mean(torch.tensor(AUC))\n    final_distance = torch.mean(torch.tensor(distance))\n    final_ap = evaluation.ap(in_vs_out_groundtruth, in_vs_out_pred)\n    final_avg_ang_dist = np.mean(np.rad2deg(np.arccos(cos_sim)))\n    avg_attention_weights = [sum(x) / len(x) for x in zip(*pred_att)]\n    print(f'Avg. Attention weights image | depth | pose: {avg_attention_weights[0]} | {avg_attention_weights[1]} | {avg_attention_weights[2]}')\n    print(\"\\tAUC:{:.4f}\\t Avg. L2 dist:{:.4f}\\t in vs out AP:{:.4f}\\t Avg. Angular Dist.:{:.4f}\".format(\n          final_AUC,\n          final_distance,\n          final_ap,\n          final_avg_ang_dist))\n    \n    return final_AUC, final_distance, final_ap, final_avg_ang_dist\n\n\ndef compute_metrics(pred_hm, gt_hm, gt_gaze,directions,gt_directions):\n    AUC = []; distance = []; cos_sim = []; dir_metric = []\n    # go through each data point and record AUC, min dist, avg dist\n    inout = [gt_gaze[i].mean()==-1 for i in range(len(gt_gaze))]\n    print(np.array(inout).sum())\n    for b_i in tqdm(range(len(gt_hm))):\n        if gt_gaze[b_i].mean()!=-1:\n            multi_hot = gt_hm[b_i]\n            multi_hot = (multi_hot > 0).float() * 1 # make GT heatmap as binary labels\n            multi_hot = misc.to_numpy(multi_hot)\n\n            pm = pred_hm[b_i]\n            scaled_heatmap = cv2.resize(pm, (output_resolution, output_resolution))\n            auc_score = evaluation.auc(scaled_heatmap, multi_hot)\n            AUC.append(auc_score)\n\n            gaze_x, gaze_y = gt_gaze[b_i]\n            # distance: L2 distance between ground truth and argmax point\n            pred_x, pred_y = evaluation.argmax_pts(pm)\n            norm_p = [pred_x/output_resolution, pred_y/output_resolution]\n            dist_score = evaluation.L2_dist([gaze_x, gaze_y], norm_p).item()\n            distance.append(dist_score)\n            dir_metric.append(directions[b_i]* gt_directions[b_i])\n            cos_sim.append(np.maximum(np.minimum(np.dot(directions[b_i], gt_directions[b_i])/(norm(directions[b_i])*norm(gt_directions[b_i])), 1.0), -1.0))\n            \n    print(np.rad2deg(np.arccos(np.mean(dir_metric)))) \n    return np.array(AUC), np.array(distance), np.array(cos_sim)\n\nif __name__ == \"__main__\" and not(flag_run_on_goo_dataset):\n    \n    #model_weights = '/kaggle/input/mm-gaze-target-prediction-weights/epoch_13.pt'\n    #model_weights = '/kaggle/input/mm-gaze-target-prediction-weights/attention-videoatttarget.pt'\n    #model_weights = '/kaggle/input/mm-gaze-target-prediction-new-weights/goo-real-original-model-ttfnet-focal_loss_only_epoch_15.pt'\n    #model_weights = '/kaggle/input/mm-gaze-target-prediction-new-weights/goo-real-original-model-ttfnet-gaussian_epoch_7.pt'\n    #model_weights = '/kaggle/input/mm-gaze-target-prediction-new-weights/goo-real-original-model-energy_aggr_loss_epoch_20.pt'\n    model_weights = '/kaggle/input/mm-gaze-target-prediction-new-weights/goo-real-original-model-focal-ciou-regr_epoch_27.pt'\n    \n    device = ('cuda' if torch.cuda.is_available() else 'cpu')\n    print(\"Loading Data\")\n\n    transform = _get_transform()\n    transform_modality = _get_transform_modality()\n    \n    val_dataset = retailGazeDataset(df_test, transform, transform_modality, \n                       input_size=input_resolution, output_size=output_resolution, \n                       test=True,imshow=False)\n    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                           batch_size=1,\n                                           shuffle=True,\n                                           num_workers=0)\n    \n    final_AUC, final_distance, final_ap,final_avg_ang_dist = test(model_weights, val_loader, device, batch_size=1, mode='pt')","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:46:42.390089Z","iopub.execute_input":"2023-04-26T11:46:42.390440Z","iopub.status.idle":"2023-04-26T11:46:42.429889Z","shell.execute_reply.started":"2023-04-26T11:46:42.390404Z","shell.execute_reply":"2023-04-26T11:46:42.428716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ngoo-real-original-model-energy_aggr_loss_epoch_20.pt\n64.28667002474117\nAvg. Attention weights image | depth | pose: [[[0.43945652]]] | [[[0.3071239]]] | [[[0.2534196]]]\nAUC:0.6414\t Avg. L2 dist:0.2216\t in vs out AP:1.0000\t Avg. Angular Dist.:20.8008\n\"\"\"\n\n\"\"\"\ngoo-real-original-model-focal-ciou-regr_epoch_27.pt\n64.69747255415196\nAvg. Attention weights image | depth | pose: [[[0.47156763]]] | [[[0.25737855]]] | [[[0.27105403]]]\nAUC:0.8151\t Avg. L2 dist:0.2219\t in vs out AP:1.0000\t Avg. Angular Dist.:21.3683\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:46:42.431560Z","iopub.execute_input":"2023-04-26T11:46:42.432303Z","iopub.status.idle":"2023-04-26T11:46:42.446769Z","shell.execute_reply.started":"2023-04-26T11:46:42.432262Z","shell.execute_reply":"2023-04-26T11:46:42.445702Z"},"trusted":true},"execution_count":null,"outputs":[]}]}