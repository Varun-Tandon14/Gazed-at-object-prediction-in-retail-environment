{"cells":[{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["## First time setup (uncomment)\n","\"\"\" \n","import requests\n","\n","!pip3 install efficientnet_pytorch\n","!git clone https://github.com/idiap/multimodal_gaze_target_prediction.git\n","\n","with open(\"human-centric.pt\", \"wb\") as f: \n","    f.write(requests.get(\"https://drive.switch.ch/index.php/s/5hDsBdP4OsLks5X/download\").content)\n","\n","print('Downloaded weight for backbone')\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-27T17:23:14.523164Z","iopub.status.busy":"2023-04-27T17:23:14.522587Z","iopub.status.idle":"2023-04-27T17:23:18.326413Z","shell.execute_reply":"2023-04-27T17:23:18.324789Z","shell.execute_reply.started":"2023-04-27T17:23:14.523134Z"},"trusted":true},"outputs":[],"source":["############################### Base Model (https://github.com/idiap/multimodal_gaze_target_prediction)\n","\"\"\" \n","## Orignal Authors\n","@inproceedings{gupta2022modular,\n","  title={A Modular Multimodal Architecture for Gaze Target Prediction: Application to Privacy-Sensitive Settings},\n","  author={Gupta, Anshul and Tafasca, Samy and Odobez, Jean-Marc},\n","  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n","  pages={5041--5050},\n","  year={2022}\n","}\n","SPDX-FileCopyrightText: 2022 Idiap Research Institute <contact@idiap.ch>\n","SPDX-FileContributor: Anshul Gupta <anshul.gupta@idiap.ch>\n","SPDX-FileContributor: Samy Tafasca <samy.tafasca@idiap.ch>\n","SPDX-License-Identifier: GPL-3.0\n","## Modification\n","<Gazed-at-object-prediction-in-retail-environment training script>\n","Copyright (C) <2022>  <Varun Tandon> <varuntandon14@gmail.com>\n","\"\"\"\n","\n","############################### TTFNet  (https://github.com/ZJULearning/ttfnet/tree/master)\n","\"\"\" \n","Copyright 2018-2019 Open-MMLab.\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License.\n","You may obtain a copy of the License at\n","\n","    http://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS,\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","See the License for the specific language governing permissions and\n","limitations under the License.\n","\"\"\"\n","\n","############################### Gaze-on-Objects (GOO) Dataet (https://github.com/upeee/GOO-GAZE2021/tree/main/dataset)\n","\"\"\" \n","Copyright [2021] [Rowel Atienza]\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License.\n","You may obtain a copy of the License at\n","\n","    http://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS,\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","See the License for the specific language governing permissions and\n","limitations under the License.\n","\n","\"\"\"\n","import torch\n","import torchvision\n","from torchvision import transforms\n","import torch.nn as nn\n","torch.multiprocessing.set_sharing_strategy('file_system')\n","\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","import torchvision.models as models\n","from torchvision.ops import FeaturePyramidNetwork\n","import torchvision.transforms.functional as TF\n","\n","\n","from collections import OrderedDict\n","from torch.utils.data import Dataset, DataLoader\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import cv2\n","import warnings\n","import pickle\n","from matplotlib import pyplot as plt\n","import matplotlib.patches as patches\n","import sys\n","import csv\n","from PIL import Image, ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","from pathlib import PureWindowsPath, PurePosixPath\n","from datetime import datetime\n","from numpy.linalg import norm\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","from efficientnet_pytorch import EfficientNet\n","from multimodal_gaze_target_prediction.utils import evaluation, misc"]},{"cell_type":"markdown","metadata":{},"source":["## Config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T17:23:35.556116Z","iopub.status.busy":"2023-04-27T17:23:35.555756Z","iopub.status.idle":"2023-04-27T17:23:35.564711Z","shell.execute_reply":"2023-04-27T17:23:35.562598Z","shell.execute_reply.started":"2023-04-27T17:23:35.556080Z"},"trusted":true},"outputs":[],"source":["# Original Image resolution for the model\n","input_resolution = 224\n","output_resolution = 64\n","\n","# Base model configs\n","WIDTH,HEIGHT = 640, 480 # Original dimension of the input image\n","cone_mode = 'early'    # {'late', 'early'} fusion of person information\n","modality_dropout = True    # only used for attention model\n","pred_inout = False    # {set True for VideoAttentionTarget}\n","privacy = False     # {set True to train/test privacy-sensitive model}\n","human_centric_weights = 'human-centric.pt'   # give complete path to the human centric model backbone weights\n","backbone_name = 'efficientnet-b2'\n","modality = 'attention'\n","\n","# pytorch amp to speed up training and reduce memory usage\n","use_amp = False\n","\n","# NMS configs\n","Bbox_topk_no_of_bboxes = 3              # Maximum No. of Bboxes pred\n","Bbox_confidence_score_threshold = 0.2   \n","\n","\"\"\" \n","use_only_real_dataset_for_train = True \n","Uses only GOO Real for train/val \n","\n","use_only_real_dataset_for_train = False\n","Uses GOO Real Train split + GOO Synth Test split for train (because we don't have resourses for GOO Synth train)\n","Uses GOO Real Val split for Validation\n","The splits used are same as provided in the original GOO Datasets\n","The inference is in a seperate script an always uses GOO Real Test split\n","\"\"\"\n","use_only_real_dataset_for_train = True \n","\n","\n","image_dir = 'goorealdataset\\\\finalrealdatasetImgsV3\\\\finalrealdatasetImgsV3' # Full path to dataset \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Baseline (Multi modal) Gaze detection Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T17:23:35.567910Z","iopub.status.busy":"2023-04-27T17:23:35.566944Z","iopub.status.idle":"2023-04-27T17:23:35.627764Z","shell.execute_reply":"2023-04-27T17:23:35.626665Z","shell.execute_reply.started":"2023-04-27T17:23:35.567868Z"},"trusted":true},"outputs":[],"source":["# returns gaze cone; resnet/efficientnet + prediction head\n","class HumanCentric(nn.Module):\n","    def __init__(self, backbone = 'resnet'):\n","        super(HumanCentric, self).__init__()\n","        \n","        self.backbone = backbone\n","        self.feature_dim = 512  # the dimension of the CNN feature to represent each frame\n","        # Build Network Base\n","        if backbone == 'resnet':\n","            self.base_head = models.resnet18(pretrained=True)\n","            self.base_head = nn.Sequential(*list(self.base_head.children())[:-1])\n","        elif backbone == 'efficientnet':\n","            self.base_head = models.efficientnet_b0(pretrained=True)\n","            self.base_head = nn.Sequential(*list(self.base_head.children())[:-1])\n","        else:\n","            assert False, 'Incorrect backbone, please choose from [resnet, efficientnet]'\n","        \n","        # Build Network Head\n","        num_outputs = 2\n","        self.num_outputs = num_outputs\n","        dummy_head = torch.empty((1, 3, 224, 224))\n","        dummy_head = self.base_head(dummy_head)\n","        \n","        # original head      \n","        '''\n","        self.head_new = nn.Sequential(\n","                        nn.Linear(dummy_head.size(1), self.feature_dim), \n","                        nn.ReLU(inplace=True),\n","                        nn.Linear(self.feature_dim, num_outputs),\n","                        nn.Tanh())\n","        ''' \n","        # Modification\n","        self.head_new = nn.Sequential(\n","                        nn.Linear(dummy_head.size(1), self.feature_dim), \n","                        nn.GELU(),\n","                        nn.Linear(self.feature_dim, num_outputs),\n","                        nn.Tanh())\n","        \n","    def forward(self, head, gaze_field):\n","        # Model output\n","        h = self.base_head(head).squeeze(dim=-1).squeeze(dim=-1) # Nx512   \n","        head_embedding = h.clone()\n","        \n","        direction = self.head_new(h) \n","        # convert to unit vector\n","        normalized_direction = direction / direction.norm(dim=1).unsqueeze(1)\n","        \n","        # generate gaze field map\n","        batch_size, channel, height, width = gaze_field.size()\n","        gaze_field = gaze_field.permute([0, 2, 3, 1]).contiguous()\n","        gaze_field = gaze_field.view([batch_size, -1, self.num_outputs])\n","        gaze_field = torch.matmul(gaze_field, normalized_direction.view([batch_size, self.num_outputs, 1]))\n","        gaze_cone = gaze_field.view([batch_size, height, width, 1])\n","        gaze_cone = gaze_cone.permute([0, 3, 1, 2]).contiguous()\n","\n","        #gaze_cone = nn.ReLU()(gaze_cone)\n","        gaze_cone = nn.GELU()(gaze_cone)\n","        return gaze_cone, normalized_direction, head_embedding\n","\n","\n","# efficientnet followed by an FPN\n","class FeatureExtractor(nn.Module):\n","    \n","    def __init__(self, backbone_name):\n","        \n","        '''\n","        args:\n","        backbone_name: name of the backbone to be used; ex. 'efficientnet-b0'\n","        '''\n","        \n","        super(FeatureExtractor, self).__init__()\n","    \n","        self.backbone = EfficientNet.from_pretrained(backbone_name)\n","        if backbone_name=='efficientnet-b3':\n","            self.fpn = FeaturePyramidNetwork([32, 48, 136, 384], output_resolution)\n","        elif backbone_name=='efficientnet-b2':\n","            self.fpn = FeaturePyramidNetwork([24, 48, 120, 352], output_resolution)\n","        elif backbone_name=='efficientnet-b0' or backbone_name=='efficientnet-b1':\n","            self.fpn = FeaturePyramidNetwork([24, 40, 112, 320], output_resolution)        \n","        \n","    def forward(self, x):\n","        \n","        features = self.backbone.extract_endpoints(x)\n","        \n","        # select features to use\n","        fpn_features = OrderedDict()\n","        fpn_features['reduction_2'] = features['reduction_2']\n","        fpn_features['reduction_3'] = features['reduction_3']\n","        fpn_features['reduction_4'] = features['reduction_4']\n","        fpn_features['reduction_5'] = features['reduction_5']\n","        \n","        # upsample features from efficientnet using an FPN to generate features at (H/4, W/4) resolution\n","        features = self.fpn(fpn_features)['reduction_2']\n","        \n","        return features\n","\n","\n","# simple prediction head that takes the features and gaze cone to regress the gaze target heatmap\n","class PredictionHead(nn.Module):\n","    \n","    def __init__(self, inchannels):\n","        super(PredictionHead, self).__init__()\n","        \n","        #self.act = nn.ReLU()\n","        self.act = nn.GELU()\n","        self.conv1 = nn.Conv2d(inchannels, inchannels, 3, padding=3, dilation=3)\n","        self.bn1 = nn.BatchNorm2d(inchannels)\n","        self.conv2 = nn.Conv2d(inchannels, inchannels, 3, padding=3, dilation=3)\n","        self.bn2 = nn.BatchNorm2d(inchannels)\n","        self.conv3 = nn.Conv2d(inchannels, inchannels, 3, padding=3, dilation=3)\n","        self.bn3 = nn.BatchNorm2d(inchannels)\n","        self.conv4 = nn.Conv2d(inchannels, inchannels, 3, padding=3, dilation=3)\n","        self.bn4 = nn.BatchNorm2d(inchannels)\n","        self.conv5 = nn.Conv2d(inchannels, inchannels//2, 3, padding=3, dilation=3)\n","        self.bn5 = nn.BatchNorm2d(inchannels//2)\n","        self.conv6 = nn.Conv2d(inchannels//2, inchannels//4, 3, padding=3, dilation=3)\n","        self.bn6 = nn.BatchNorm2d(inchannels//4)\n","        self.conv7 = nn.Conv2d(inchannels//4, 1, 1)\n","\n","    def forward(self, x):\n","                \n","        # upsample the features to output_resolution, output_resolution\n","        x = nn.Upsample(size=(output_resolution,output_resolution), mode='bilinear', align_corners=False)(x)\n","        x = self.act(self.bn1(self.conv1(x)))\n","        \n","        # regress the heatmap\n","        x = self.act(self.bn2(self.conv2(x)))\n","        x = self.act(self.bn3(self.conv3(x)))\n","        x = self.act(self.bn4(self.conv4(x)))\n","        x = self.act(self.bn5(self.conv5(x)))\n","        x = self.act(self.bn6(self.conv6(x)))\n","        x = self.conv7(x)\n","        \n","        return x\n","        \n","# compress modality spatially\n","class CompressModality(nn.Module):\n","    \n","    def __init__(self, in_channels):\n","        super(CompressModality, self).__init__()\n","        \n","        self.act = nn.GELU()\n","        \n","        self.conv1 = nn.Conv2d(in_channels, 128, kernel_size=3, stride=2)\n","        self.bn1 = nn.BatchNorm2d(128)\n","        self.conv2 = nn.Conv2d(128, 256, kernel_size=3, stride=2)\n","        self.bn2 = nn.BatchNorm2d(256)\n","        self.conv3 = nn.Conv2d(256, 512, kernel_size=3, stride=2)\n","        self.bn3 = nn.BatchNorm2d(512)\n","    \n","    def forward(self, x):\n","        \n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.act(x)\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.act(x)\n","        x = self.conv3(x)\n","        x = self.bn3(x)\n","        x = self.act(x)\n","        x = nn.MaxPool2d(x.shape[2])(x)\n","\n","        return x.squeeze(dim=-1).squeeze(dim=-1)\n","    \n","\n","# predicts in vs out gaze; CompressModality + Linear\n","class InvsOut(nn.Module):\n","    \n","    def __init__(self, in_channels):\n","        \n","        '''\n","        args:\n","        in_channels: number of input channels\n","        '''\n","        \n","        super(InvsOut, self).__init__()\n","        self.compress_inout = CompressModality(in_channels)\n","        \"\"\"\n","        self.inout = nn.Sequential(nn.Linear(1024, 256),\n","                                   nn.ReLU(),\n","                                   nn.Linear(256, 1),\n","                                   nn.Sigmoid())\n","        \"\"\"\n","        self.inout = nn.Sequential(nn.Linear(1024, 256),\n","                                   nn.GELU(),\n","                                   nn.Linear(256, 1),\n","                                   nn.Sigmoid())\n","    \n","    def forward(self, x, head_embedding):\n","        \n","        x = self.compress_inout(x)\n","        x = torch.cat([x, head_embedding], axis=1)\n","        x = self.inout(x)\n","        \n","        return x\n","    \n","\n","# baseline model that takes a single modality and the gaze cone as input to predict a gaze target heatmap\n","class BaselineModel(nn.Module):\n","    \n","    def __init__(self, backbone_name, modality, cone_mode='early', pred_inout=False):\n","        \n","        '''\n","        args:\n","        backbone_name: name of the backbone to be used; ex. 'efficientnet-b0'\n","        cone_mode: early or late fusion of person information {'early', 'late'}\n","        pred_inout: predict an in vs out of frame gaze label\n","        '''\n","        \n","        super(BaselineModel, self).__init__()\n","        self.feature_extractor = FeatureExtractor(backbone_name)    \n","        self.prediction_head = PredictionHead(output_resolution)\n","        self.human_centric = HumanCentric()\n","        # load weights\n","        state_dict = torch.load(human_centric_weights)['model_state_dict']\n","        self.human_centric.load_state_dict(state_dict, strict=False)\n","\n","        # add additional channels\n","        self.cone_mode = cone_mode\n","        if cone_mode=='early':\n","            input_layer = self.feature_extractor.backbone._conv_stem.weight\n","            self.feature_extractor.backbone._conv_stem.weight = torch.nn.Parameter(torch.cat([input_layer.clone(), input_layer.clone()[:,0:2,:,:]], axis=1))\n","        elif cone_mode=='late':\n","            self.cat_conv = nn.Conv2d(output_resolution+2, output_resolution, 3, padding=1)\n","            \n","        # drop additional channels\n","        if modality == 'depth':\n","            self.feature_extractor.backbone._conv_stem.weight = torch.nn.Parameter(self.feature_extractor.backbone._conv_stem.weight[:,0:-2,:,:])\n","        \n","        self.pred_inout = pred_inout\n","        if pred_inout:\n","            self.in_vs_out_head = InvsOut(output_resolution)\n","    \n","    def forward(self, img, face, gaze_field, head_mask):\n","        \n","        # dummy predictions\n","        batch_size = img.shape[0]\n","        in_vs_out = torch.zeros(batch_size).cuda()\n","        direction = torch.zeros(batch_size, 2).cuda()\n","        \n","        # get gaze cone\n","        gaze_cone, direction, head_embedding = self.human_centric(face, gaze_field)\n","                \n","        if self.cone_mode=='early':\n","            x = torch.cat([img, gaze_cone, head_mask], dim=1)\n","        else:\n","            x = img\n","        \n","        # extract the features\n","        x = self.feature_extractor(x)\n","        \n","        if self.cone_mode=='late':\n","            x = torch.cat([x, gaze_cone, head_mask], dim=1)\n","            x = self.cat_conv(x)\n","            \n","        # apply the prediction head to get the heatmap\n","        hm = self.prediction_head(x)\n","        \n","        # apply the in vs out head\n","        if self.pred_inout:\n","            in_vs_out = self.in_vs_out_head(x, head_embedding)\n","        \n","        return hm, direction, in_vs_out, x\n","\n","\n","# attention based model. multiple modalities processed separately. output feature maps are weighted and added using predicted attention weights to predict a gaze target heatmap\n","class AttentionModelCombined(nn.Module):\n","    \n","    def __init__(self, cone_mode='early', pred_inout=False):\n","        \n","        '''\n","        args:\n","        cone_mode: early or late fusion of person information {'early', 'late'}\n","        pred_inout: predict an in vs out of frame gaze label\n","        '''\n","        \n","        super(AttentionModelCombined, self).__init__()\n","        self.feature_extractor_image = FeatureExtractor('efficientnet-b1')\n","        self.feature_extractor_depth = FeatureExtractor('efficientnet-b0')\n","        self.feature_extractor_pose = FeatureExtractor('efficientnet-b0')\n","        num_modalities = 3\n","        \n","        self.bn_image = nn.BatchNorm2d(output_resolution)\n","        self.bn_depth = nn.BatchNorm2d(output_resolution)\n","        self.bn_pose = nn.BatchNorm2d(output_resolution)\n","        \n","        additional_channels = 0\n","        if cone_mode=='late':\n","            additional_channels = 2\n","        self.Wv_image = nn.Conv2d(output_resolution+additional_channels, output_resolution, kernel_size=3, padding=1)\n","        self.Wv_depth = nn.Conv2d(output_resolution+additional_channels, output_resolution, kernel_size=3, padding=1)\n","        self.Wv_pose = nn.Conv2d(output_resolution+additional_channels, output_resolution, kernel_size=3, padding=1)\n","        \n","        self.compress_image = CompressModality(output_resolution)\n","        self.compress_depth = CompressModality(output_resolution)\n","        self.compress_pose = CompressModality(output_resolution)\n","        self.attention_layer = nn.Sequential(nn.Linear(512*num_modalities, num_modalities),\n","                                             nn.Softmax()\n","                                             )\n","        \n","        self.human_centric = HumanCentric()\n","        # load weights\n","        state_dict = torch.load(human_centric_weights)['model_state_dict']\n","        self.human_centric.load_state_dict(state_dict, strict=False)\n","            \n","        # add additional channels\n","        self.cone_mode = cone_mode\n","        if cone_mode=='early':\n","            input_layer = self.feature_extractor_image.backbone._conv_stem.weight\n","            self.feature_extractor_image.backbone._conv_stem.weight = torch.nn.Parameter(torch.cat([input_layer.clone(), input_layer.clone()[:,0:2,:,:]], axis=1))\n","            input_layer = self.feature_extractor_depth.backbone._conv_stem.weight\n","            self.feature_extractor_depth.backbone._conv_stem.weight = torch.nn.Parameter(torch.cat([input_layer.clone(), input_layer.clone()[:,0:2,:,:]], axis=1))\n","            input_layer = self.feature_extractor_pose.backbone._conv_stem.weight\n","            self.feature_extractor_pose.backbone._conv_stem.weight = torch.nn.Parameter(torch.cat([input_layer.clone(), input_layer.clone()[:,0:2,:,:]], axis=1))\n","        \n","        # drop additional channels\n","        self.feature_extractor_depth.backbone._conv_stem.weight = torch.nn.Parameter(self.feature_extractor_depth.backbone._conv_stem.weight[:,0:-2,:,:])\n","        \n","        self.prediction_head = PredictionHead(output_resolution)\n","        #self.output_act = nn.ReLU()\n","        self.output_act = nn.GELU()\n","        #self.output_act = nn.Softmax(dim = 1)\n","        self.pred_inout = pred_inout\n","        if pred_inout:\n","            self.in_vs_out_head = InvsOut(output_resolution)\n","    \n","    def forward(self, x, face, gaze_field, head_mask):\n","        \n","        # dummy predictions\n","        batch_size = x[0].shape[0]\n","        in_vs_out = torch.zeros(batch_size).cuda()\n","        direction = torch.zeros(batch_size, 2).cuda()\n","                \n","        # get gaze cone\n","        gaze_cone, direction, head_embedding = self.human_centric(face, gaze_field)\n","        \n","        # extract the features\n","        if self.cone_mode=='early':\n","            x_image = torch.cat([x[0], gaze_cone, head_mask], dim=1)\n","            x_depth = torch.cat([x[1], gaze_cone, head_mask], dim=1)\n","            x_pose = torch.cat([x[2], gaze_cone, head_mask], dim=1)\n","        else:\n","            x_image = x[0]\n","            x_depth = x[1]\n","            x_pose = x[2]\n","\n","        x_image = self.feature_extractor_image(x_image)\n","        x_image = self.bn_image(x_image)\n","        x_depth = self.feature_extractor_depth(x_depth)\n","        x_depth = self.bn_depth(x_depth)\n","        x_pose = self.feature_extractor_pose(x_pose)\n","        x_pose = self.bn_pose(x_pose)\n","        \n","        if self.cone_mode=='late':\n","            x_image = torch.cat([x_image, gaze_cone, head_mask], dim=1)\n","            x_depth = torch.cat([x_depth, gaze_cone, head_mask], dim=1)\n","            x_pose = torch.cat([x_pose, gaze_cone, head_mask], dim=1)\n","        \n","        # get the values\n","        v_image = self.Wv_image(x_image)\n","        v_depth = self.Wv_depth(x_depth)\n","        v_pose = self.Wv_pose(x_pose)\n","                \n","        # get attention weights\n","        att_image = self.compress_image(v_image)\n","        att_depth = self.compress_depth(v_depth)\n","        att_pose = self.compress_pose(v_pose)\n","        att = torch.cat([att_image, att_depth, att_pose], dim=1)\n","        att = self.attention_layer(att).unsqueeze(2).unsqueeze(3).unsqueeze(4)    # add extra dimensions for weighting in the next step\n","\n","        # weight values\n","        v_image = v_image * att[:, 0]\n","        v_depth = v_depth * att[:, 1]\n","        v_pose = v_pose * att[:, 2]\n","        \n","        x = v_image + v_depth + v_pose\n","        \n","        # apply the prediction head\n","        hm = self.output_act(self.prediction_head(x))\n","        \n","        # apply the in vs out head\n","        if self.pred_inout:\n","            in_vs_out = self.in_vs_out_head(x, head_embedding)\n","        \n","        return hm, direction, in_vs_out, att, x\n","    \n","    # Solution from : https://github.com/Lightning-AI/lightning/issues/4690\n","    def on_load_checkpoint(self, checkpoint: dict) -> None:\n","        state_dict = checkpoint[\"model\"]\n","        model_state_dict = self.state_dict()\n","        is_changed = False\n","        for k in state_dict:\n","            if k in model_state_dict:\n","                if state_dict[k].shape != model_state_dict[k].shape:\n","                    print(f\"Skip loading parameter: {k}, \"\n","                                f\"required shape: {model_state_dict[k].shape}, \"\n","                                f\"loaded shape: {state_dict[k].shape}\")\n","                    state_dict[k] = model_state_dict[k]\n","                    is_changed = True\n","            else:\n","                print(f\"Dropping parameter {k}\")\n","                is_changed = True\n","\n","        if is_changed:\n","            checkpoint.pop(\"optimizer\", None)"]},{"cell_type":"markdown","metadata":{},"source":["## Baseline Gaze detection model + TTF Net regression BBoX prediction head"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T17:23:35.630069Z","iopub.status.busy":"2023-04-27T17:23:35.629284Z","iopub.status.idle":"2023-04-27T17:23:35.646277Z","shell.execute_reply":"2023-04-27T17:23:35.645280Z","shell.execute_reply.started":"2023-04-27T17:23:35.630032Z"},"trusted":true},"outputs":[],"source":["class RegressionHeadBbox(nn.Module):\n","    \n","    def __init__(self, inchannels):\n","        super(RegressionHeadBbox, self).__init__()\n","        \n","        #self.act = nn.ReLU()\n","        self.act = nn.GELU()\n","        self.inchannels = inchannels\n","        self.conv1 = nn.Conv2d(inchannels, inchannels, 3, padding=1, dilation=1)\n","        self.bn1 = nn.BatchNorm2d(inchannels)\n","        self.conv2 = nn.Conv2d(inchannels, inchannels, 3, padding=1, dilation=1)\n","        self.bn2 = nn.BatchNorm2d(inchannels)\n","        self.conv3 = nn.Conv2d(inchannels, inchannels, 3, padding=1, dilation=1)\n","        self.bn3 = nn.BatchNorm2d(inchannels)\n","        self.conv4 = nn.Conv2d(inchannels, inchannels, 3, padding=1, dilation=1)\n","        self.bn4 = nn.BatchNorm2d(inchannels)\n","        self.out_conv = nn.Conv2d(inchannels, 4, 1)\n","\n","\n","    def forward(self, x):\n","                \n","        # upsample the features to output_resolution, output_resolution\n","        #x = nn.Upsample(size=(output_resolution,output_resolution), mode='bilinear', align_corners=False)(x)\n","        x = nn.Upsample(size=(self.inchannels,self.inchannels), mode='bilinear', align_corners=False)(x)\n","        x = self.act(self.bn1(self.conv1(x)))\n","        \n","        # regress the heatmap\n","        x = self.act(self.bn2(self.conv2(x)))\n","        x = self.act(self.bn3(self.conv3(x)))\n","        x = self.act(self.bn4(self.conv4(x)))\n","        x = self.act(self.out_conv(x))\n","        \n","        return x\n","\n","# Just like the two variants in the original paper we also provide \n","# 1. baselineModelBboxHead -> Single modality gaze prediction model with BBoX head\n","# 2. attentionModelBboxHead -> Multi modal gaze prediction model with BBoX head\n","\n","class baselineModelBboxHead(nn.Module):\n","    def __init__(self, gaze_model, output_resolution=output_resolution):\n","        super(baselineModelBboxHead, self).__init__()\n","        self.gaze_model = gaze_model\n","        self.reg_head_bbox = RegressionHeadBbox(output_resolution)\n","        # From the paper ttfnet paper Size Regression section on page 4\n","        # this is nothing but the scalar s is a fixed scalar used \n","        # to enlarge the predicted results for easier optimization. \n","        # s = 16 is set in their implementation\n","        self.wh_offset_base = 16\n","        \n","    def forward(self,x , face, gaze_field, head_mask):\n","        \"\"\"\n","        x, face, gaze_field, head_mask:  same shape gaze prediction model\n","        \n","        Outputs:\n","        outr: shape (1,2,64,64)\n","        \"\"\"\n","        \n","        hm, direction, in_vs_out, x_out = self.gaze_model(x, face, gaze_field, head_mask)    \n","        outr = self.reg_head_bbox(x_out) * self.wh_offset_base\n","        return hm, direction, in_vs_out, outr\n","    \n","class attentionModelBboxHead(nn.Module):\n","    def __init__(self, gaze_model, output_resolution=output_resolution):\n","        super(attentionModelBboxHead, self).__init__()\n","        self.gaze_model = gaze_model\n","        self.reg_head_bbox = RegressionHeadBbox(output_resolution)\n","        # From the paper ttfnet paper Size Regression section on page 4\n","        # this is nothing but the scalar s is a fixed scalar used \n","        # to enlarge the predicted results for easier optimization. \n","        # s = 16 is set in our experiments\n","        self.wh_offset_base = 16\n","        \n","    def forward(self,x, face, gaze_field, head_mask):\n","        \"\"\"\n","        x, face, gaze_field, head_mask:  same shape gaze prediction model\n","        \n","        Outputs:\n","        outr: shape (1,2,64,64)\n","        \"\"\"\n","        \n","        hm, direction, in_vs_out, att,x_out = self.gaze_model(x, face, gaze_field, head_mask)    \n","        outr = self.reg_head_bbox(x_out) * self.wh_offset_base\n","        return hm, direction, in_vs_out, att, outr\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Getting Train/Val Dataset dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T17:23:35.648569Z","iopub.status.busy":"2023-04-27T17:23:35.648137Z","iopub.status.idle":"2023-04-27T17:23:36.494607Z","shell.execute_reply":"2023-04-27T17:23:36.493325Z","shell.execute_reply.started":"2023-04-27T17:23:35.648533Z"},"trusted":true},"outputs":[],"source":["## For Train: GOO Real Train \n","##     Val: GOO Real Val\n","if use_only_real_dataset_for_train:\n","    \n","    obj_train = pd.read_pickle('/kaggle/input/goorealdataset/oneshotrealhumansNew.pickle', compression='infer')\n","    obj_val = pd.read_pickle('/kaggle/input/goorealdataset/valrealhumansNew.pickle', compression='infer')\n","    df_rg_train = pd.DataFrame.from_records(obj_train)\n","    df_rg_val = pd.DataFrame.from_records(obj_val)\n","    print(len(df_rg_train))\n","    print(len(df_rg_val))\n","\n","    # merge with pose csv file\n","    ## to get eye point in retailgaze dataset\n","    ## to filter frames with no pose detected both for train and inf\n","    column_names = ['filename', 'left_eye_x', 'left_eye_y', 'right_eye_x', 'right_eye_y', 'pose_min_x', 'pose_min_y', 'pose_max_x', 'pose_max_y']\n","    csv_path_train = '/kaggle/input/goorealposeanddepth/goo-real-train-pose/kaggle/working/goo-real-pose/master-pose.csv'\n","    #csv_path_train = '/kaggle/input/goosynthtestposeanddepth/goo-synth-test-pose/kaggle/working/goo-synth-test-pose/master-pose.csv'\n","    df_pose_train = pd.read_csv(csv_path_train, sep=',', names=column_names, index_col=False, encoding=\"utf-8-sig\")\n","    csv_path_val = '/kaggle/input/goorealposeanddepth/goo-real-val-pose/kaggle/working/goo-real-pose/master-pose.csv'\n","    df_pose_val = pd.read_csv(csv_path_val, sep=',', names=column_names, index_col=False, encoding=\"utf-8-sig\")\n","    print(len(df_pose_train))\n","    print(len(df_pose_val))\n","    df_train = pd.merge(df_rg_train, df_pose_train, on=\"filename\")\n","    df_val = pd.merge(df_rg_val, df_pose_val, on=\"filename\")\n","    \n","else:\n","    ## For Train: GOO Real Train + GOO Synth Test (because it is smaller dataset and we have no memory for GOO synth Train :-( )\n","    ##     Val: GOO Real Val\n","\n","    obj_train = pd.read_pickle('/kaggle/input/goosynthtestdataset/goosynth_test_v2_no_segm.pkl', compression='infer')\n","    df_rg_train = pd.DataFrame.from_records(obj_train)\n","    df_rg_train = df_rg_train[['filename','width','height','ann','gaze_item','gazeIdx','gaze_cx','gaze_cy','hx','hy']]\n","    print(len(df_rg_train))\n","\n","    # merge with pose csv file\n","    ## to get eye point in retailgaze dataset\n","    ## to filter frames with no pose detected both for train and inf\n","    column_names = ['filename', 'left_eye_x', 'left_eye_y', 'right_eye_x', 'right_eye_y', 'pose_min_x', 'pose_min_y', 'pose_max_x', 'pose_max_y']\n","    csv_path_train = '/kaggle/input/goosynthtestposeanddepth/goo-synth-test-pose/kaggle/working/goo-synth-test-pose/master-pose.csv'\n","    df_pose_train = pd.read_csv(csv_path_train, sep=',', names=column_names, index_col=False, encoding=\"utf-8-sig\")\n","    print(len(df_pose_train))\n","\n","    df_train_synth = pd.merge(df_rg_train, df_pose_train, on=\"filename\")\n","\n","    obj_train = pd.read_pickle('/kaggle/input/goorealdataset/oneshotrealhumansNew.pickle', compression='infer')\n","    df_rg_train = pd.DataFrame.from_records(obj_train)\n","    print(len(df_rg_train))\n","    df_rg_train = df_rg_train[['filename','width','height','ann','gaze_item','gazeIdx','gaze_cx','gaze_cy','hx','hy']]\n","    column_names = ['filename', 'left_eye_x', 'left_eye_y', 'right_eye_x', 'right_eye_y', 'pose_min_x', 'pose_min_y', 'pose_max_x', 'pose_max_y']\n","    csv_path_train = '/kaggle/input/goorealposeanddepth/goo-real-train-pose/kaggle/working/goo-real-pose/master-pose.csv'\n","    df_pose_train = pd.read_csv(csv_path_train, sep=',', names=column_names, index_col=False, encoding=\"utf-8-sig\")\n","    print(len(df_pose_train))\n","    df_train_real = pd.merge(df_rg_train, df_pose_train, on=\"filename\")\n","\n","    ## goo real id -> 0 & goo synth id -> 1\n","    df_train_real['dataset_id'] = np.zeros(len(df_train_real),dtype=np.int8)\n","    df_train_synth['dataset_id'] = np.ones(len(df_train_synth),dtype=np.int8)\n","    df_train = df_train_real.append(df_train_synth, ignore_index=True)\n","    print(len(df_train))\n","\n","\n","    obj_val = pd.read_pickle('/kaggle/input/goorealdataset/valrealhumansNew.pickle', compression='infer')\n","    df_rg_val = pd.DataFrame.from_records(obj_val)\n","    df_rg_val = df_rg_val[['filename','width','height','ann','gaze_item','gazeIdx','gaze_cx','gaze_cy','hx','hy']]\n","    csv_path_val = '/kaggle/input/goorealposeanddepth/goo-real-val-pose/kaggle/working/goo-real-pose/master-pose.csv'\n","    df_pose_val = pd.read_csv(csv_path_val, sep=',', names=column_names, index_col=False, encoding=\"utf-8-sig\")\n","\n","    df_val = pd.merge(df_rg_val, df_pose_val, on=\"filename\")\n","    df_val['dataset_id'] = np.zeros(len(df_val),dtype=np.int8) \n","    print(len(df_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T17:23:36.496908Z","iopub.status.busy":"2023-04-27T17:23:36.496389Z","iopub.status.idle":"2023-04-27T17:23:36.639904Z","shell.execute_reply":"2023-04-27T17:23:36.638378Z","shell.execute_reply.started":"2023-04-27T17:23:36.496867Z"},"trusted":true},"outputs":[],"source":["df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T17:23:36.646726Z","iopub.status.busy":"2023-04-27T17:23:36.646082Z","iopub.status.idle":"2023-04-27T17:23:36.815754Z","shell.execute_reply":"2023-04-27T17:23:36.814396Z","shell.execute_reply.started":"2023-04-27T17:23:36.646673Z"},"trusted":true},"outputs":[],"source":["df_val.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T17:23:36.828556Z","iopub.status.busy":"2023-04-27T17:23:36.827805Z","iopub.status.idle":"2023-04-27T17:23:37.971184Z","shell.execute_reply":"2023-04-27T17:23:37.970192Z","shell.execute_reply.started":"2023-04-27T17:23:36.828518Z"},"trusted":true},"outputs":[],"source":["df_train_plot = df_train\n","df_train_plot['person_ID'] = [os.path.splitext(PureWindowsPath(x).as_posix())[0].split(\"/\")[0] for x in df_train_plot['filename']]\n","df_train_plot['cam_ID'] = [os.path.splitext(PureWindowsPath(x).as_posix())[0].split(\"/\")[1] for x in df_train_plot['filename']]\n","train_enteries_per_person = df_train_plot['person_ID'].value_counts()\n","train_enteries_per_camID = df_train_plot['cam_ID'].value_counts()\n","train_enteries_per_gaze_item = df_train_plot['gaze_item'].value_counts()\n","train_enteries_per_person.plot.bar()\n","plt.show()\n","train_enteries_per_camID.plot.bar()\n","plt.show()\n","train_enteries_per_gaze_item.plot.bar()\n","plt.show()\n","\n","df_val_plot = df_val\n","df_val_plot['person_ID'] = [os.path.splitext(PureWindowsPath(x).as_posix())[0].split(\"/\")[0] for x in df_val_plot['filename']]\n","df_val_plot['cam_ID'] = [os.path.splitext(PureWindowsPath(x).as_posix())[0].split(\"/\")[1] for x in df_val_plot['filename']]\n","val_enteries_per_person = df_val_plot['person_ID'].value_counts()\n","val_enteries_per_camID = df_val_plot['cam_ID'].value_counts()\n","val_enteries_per_gaze_item = df_val_plot['gaze_item'].value_counts()\n","val_enteries_per_person.plot.bar()\n","plt.show()\n","val_enteries_per_camID.plot.bar()\n","plt.show()\n","val_enteries_per_gaze_item.plot.bar()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T17:23:38.849739Z","iopub.status.busy":"2023-04-27T17:23:38.849448Z","iopub.status.idle":"2023-04-27T17:23:38.877317Z","shell.execute_reply":"2023-04-27T17:23:38.875882Z","shell.execute_reply.started":"2023-04-27T17:23:38.849712Z"},"trusted":true},"outputs":[],"source":["WIDTH_gazeutils, HEIGHT_gazeutils = 960, 720\n","\n","def generate_data_field(eye_point, width = WIDTH_gazeutils, height = HEIGHT_gazeutils):\n","    \"\"\"eye_point is (x, y) and between 0 and 1\"\"\"\n","    x_grid = np.array(range(width)).reshape([1, width]).repeat(height, axis=0)\n","    y_grid = np.array(range(height)).reshape([height, 1]).repeat(width, axis=1)\n","    grid = np.stack((x_grid, y_grid)).astype(np.float32)\n","\n","    x, y = eye_point\n","    x, y = x * width, y * height\n","\n","    grid -= np.array([x, y]).reshape([2, 1, 1]).astype(np.float32)\n","    grid[0] = grid[0] / width\n","    grid[1] = grid[1] / height\n","#     norm = np.sqrt(np.sum(grid ** 2, axis=0)).reshape([1, height, width])\n","#     # avoid zero norm\n","#     norm = np.maximum(norm, 0.1)\n","#     grid /= norm\n","    return grid\n","\n","\n","def generate_gaze_cone(gaze_field, normalized_direction, width=WIDTH_gazeutils, height=HEIGHT_gazeutils):\n","        \n","    gaze_field = np.ascontiguousarray(gaze_field.transpose([1, 2, 0]))\n","    gaze_field = gaze_field.reshape([-1, 2])\n","    gaze_field = np.matmul(gaze_field, normalized_direction.reshape([2, 1]))\n","    gaze_field_map = gaze_field.reshape([height, width, 1])\n","    gaze_field_map = np.ascontiguousarray(gaze_field_map.transpose([2, 0, 1]))\n","    \n","    gaze_field_map = gaze_field_map * (gaze_field_map > 0).astype(int)\n","    \n","    return gaze_field_map.squeeze()\n","\n","def _get_transform():\n","    transform_list = []\n","    transform_list.append(transforms.Resize((input_resolution, input_resolution)))\n","    transform_list.append(transforms.ToTensor())\n","    transform_list.append(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n","    return transforms.Compose(transform_list)\n","\n","def _get_transform_modality():\n","    transform_list = []\n","    transform_list.append(transforms.Resize((input_resolution, input_resolution)))\n","    transform_list.append(transforms.ToTensor())\n","    return transforms.Compose(transform_list)\n","\n","def _get_object_transform():\n","    transform_list = []\n","    transform_list.append(transforms.Resize((256,256)))\n","    transform_list.append(transforms.ToTensor())\n","    transform_list.append(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n","    return transforms.Compose(transform_list)\n","\n","def unnorm(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n","    std = np.array(std).reshape(3,1,1)\n","    mean = np.array(mean).reshape(3,1,1)\n","    return img * std + mean\n","\n","def get_head_box_channel(x_min, y_min, x_max, y_max, width, height, resolution, coordconv=False):\n","    head_box = np.array([x_min/width, y_min/height, x_max/width, y_max/height])*resolution\n","    head_box = head_box.astype(int)\n","    head_box = np.clip(head_box, 0, resolution-1)\n","    if coordconv:\n","        unit = np.array(range(0,resolution), dtype=np.float32)\n","        head_channel = []\n","        for i in unit:\n","            head_channel.append([unit+i])\n","        head_channel = np.squeeze(np.array(head_channel)) / float(np.max(head_channel))\n","        head_channel[head_box[1]:head_box[3],head_box[0]:head_box[2]] = 0\n","    else:\n","        head_channel = np.zeros((resolution,resolution), dtype=np.float32)\n","        head_channel[head_box[1]:head_box[3],head_box[0]:head_box[2]] = 1\n","    head_channel = torch.from_numpy(head_channel)\n","    return head_channel\n","\n","def to_numpy(tensor):\n","    if torch.is_tensor(tensor):\n","        return tensor.cpu().numpy()\n","    elif type(tensor).__module__ != 'numpy':\n","        raise ValueError(\"Cannot convert {} to numpy array\"\n","                         .format(type(tensor)))\n","    return tensor\n","\n","\n","def to_torch(ndarray):\n","    if type(ndarray).__module__ == 'numpy':\n","        return torch.from_numpy(ndarray)\n","    elif not torch.is_tensor(ndarray):\n","        raise ValueError(\"Cannot convert {} to torch tensor\"\n","                         .format(type(ndarray)))\n","    return ndarray\n","\n","def draw_labelmap(img, pt, sigma, type='Gaussian'):\n","    # Draw a 2D gaussian\n","    # Adopted from https://github.com/anewell/pose-hg-train/blob/master/src/pypose/draw.py\n","    img = to_numpy(img)\n","\n","    # Check that any part of the gaussian is in-bounds\n","    ul = [int(pt[0] - 3 * sigma), int(pt[1] - 3 * sigma)]\n","    br = [int(pt[0] + 3 * sigma + 1), int(pt[1] + 3 * sigma + 1)]\n","    if (ul[0] >= img.shape[1] or ul[1] >= img.shape[0] or\n","            br[0] < 0 or br[1] < 0):\n","        # If not, just return the image as is\n","        return to_torch(img)\n","\n","    # Generate gaussian\n","    size = 6 * sigma + 1\n","    x = np.arange(0, size, 1, float)\n","    y = x[:, np.newaxis]\n","    x0 = y0 = size // 2\n","    # The gaussian is not normalized, we want the center value to equal 1\n","    if type == 'Gaussian':\n","        g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n","    elif type == 'Cauchy':\n","        g = sigma / (((x - x0) ** 2 + (y - y0) ** 2 + sigma ** 2) ** 1.5)\n","\n","    # Usable gaussian range\n","    g_x = max(0, -ul[0]), min(br[0], img.shape[1]) - ul[0]\n","    g_y = max(0, -ul[1]), min(br[1], img.shape[0]) - ul[1]\n","    # Image range\n","    img_x = max(0, ul[0]), min(br[0], img.shape[1])\n","    img_y = max(0, ul[1]), min(br[1], img.shape[0])\n","\n","    img[img_y[0]:img_y[1], img_x[0]:img_x[1]] += g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n","    img = img/np.max(img) # normalize heatmap so it has max value of 1\n","    return to_torch(img)\n","\n","def multi_hot_targets(gaze_pts, out_res):\n","    w, h = out_res\n","    target_map = np.zeros((h, w))\n","    if gaze_pts[0] >= 0:\n","        x, y = map(int,[gaze_pts[0]*w.float(), gaze_pts[1]*h.float()])\n","        x = min(x, w-1)\n","        y = min(y, h-1)\n","        target_map[y, x] = 1\n","    return target_map"]},{"cell_type":"markdown","metadata":{},"source":["## TTFnet regression utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T17:23:38.879455Z","iopub.status.busy":"2023-04-27T17:23:38.879091Z","iopub.status.idle":"2023-04-27T17:23:38.911382Z","shell.execute_reply":"2023-04-27T17:23:38.910091Z","shell.execute_reply.started":"2023-04-27T17:23:38.879421Z"},"trusted":true},"outputs":[],"source":["def bbox_areas(bboxes, keep_axis=False):\n","    x_min, y_min, x_max, y_max = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n","    areas = (y_max - y_min + 1) * (x_max - x_min + 1)\n","    if keep_axis:\n","        return areas[:, None]\n","    return areas\n","\n","def calc_region(bbox, ratio, featmap_size=None):\n","    \"\"\"Calculate a proportional bbox region.\n","    The bbox center are fixed and the new h' and w' is h * ratio and w * ratio.\n","    Args:\n","        bbox (Tensor): Bboxes to calculate regions, shape (n, 4)\n","        ratio (float): Ratio of the output region.\n","        featmap_size (tuple): Feature map size used for clipping the boundary.\n","    Returns:\n","        tuple: x1, y1, x2, y2\n","    \"\"\"\n","    x1 = torch.round((1 - ratio) * bbox[0] + ratio * bbox[2]).long()\n","    y1 = torch.round((1 - ratio) * bbox[1] + ratio * bbox[3]).long()\n","    x2 = torch.round(ratio * bbox[0] + (1 - ratio) * bbox[2]).long()\n","    y2 = torch.round(ratio * bbox[1] + (1 - ratio) * bbox[3]).long()\n","    if featmap_size is not None:\n","        x1 = x1.clamp(min=0, max=featmap_size[1] - 1)\n","        y1 = y1.clamp(min=0, max=featmap_size[0] - 1)\n","        x2 = x2.clamp(min=0, max=featmap_size[1] - 1)\n","        y2 = y2.clamp(min=0, max=featmap_size[0] - 1)\n","    return (x1, y1, x2, y2)\n","\n","def gaussian_2d(shape, sigma_x=1, sigma_y=1):\n","    m, n = [(ss - 1.) / 2. for ss in shape]\n","    y, x = np.ogrid[-m:m + 1, -n:n + 1]\n","\n","    h = np.exp(-(x * x / (2 * sigma_x * sigma_x) + y * y / (2 * sigma_y * sigma_y)))\n","    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n","    return h\n","\n","def draw_truncate_gaussian(heatmap, center, h_radius, w_radius, k=1):\n","    h, w = 2 * h_radius + 1, 2 * w_radius + 1\n","    sigma_x = w / 6\n","    sigma_y = h / 6\n","    gaussian = gaussian_2d((h, w), sigma_x=sigma_x, sigma_y=sigma_y)\n","    gaussian = heatmap.new_tensor(gaussian)\n","    \n","    x, y = int(center[0]), int(center[1])\n","\n","    height, width = heatmap.shape[0:2]\n","\n","    left, right = min(x, w_radius), min(width - x, w_radius + 1)\n","    top, bottom = min(y, h_radius), min(height - y, h_radius + 1)\n","\n","    masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\n","    masked_gaussian = gaussian[h_radius - top:h_radius + bottom,\n","                      w_radius - left:w_radius + right]\n","    if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0:\n","        torch.max(masked_heatmap, masked_gaussian * k, out=masked_heatmap)\n","    return heatmap\n","\n","def target_single_image(gt_boxes, feat_shape=(output_resolution,output_resolution)):\n","    \"\"\"\n","    Args:\n","        The scale of the gt_boxes is between 0 and 1\n","        They are converted to feature space in the func.\n","        \n","        gt_boxes: tensor, tensor <=> img, (num_gt, 4).\n","        feat_shape: tuple.\n","    Returns:\n","        heatmap: tensor, tensor <=> img, (1, h, w).\n","        box_target: tensor, tensor <=> img, (4, h, w).\n","        reg_weight: tensor, same as box_target.\n","    \"\"\"\n","    wh_area_process = 'log'\n","    output_h, output_w = feat_shape\n","    heatmap_channel = 1\n","    wh_gaussian = True\n","    wh_agnostic = True\n","    down_ratio = 4\n","    alpha = 0.54\n","    beta = 0.54\n","   \n","    heatmap = gt_boxes.new_zeros((heatmap_channel, output_h, output_w))\n","    fake_heatmap = gt_boxes.new_zeros((output_h, output_w))\n","    box_target = gt_boxes.new_ones((4, output_h, output_w)) * -1\n","    reg_weight = gt_boxes.new_zeros((1, output_h, output_w),dtype=torch.float)\n","    \n","    if wh_area_process == 'log':\n","        boxes_areas_log = bbox_areas(gt_boxes).log()\n","    elif wh_area_process == 'sqrt':\n","        boxes_areas_log = bbox_areas(gt_boxes).sqrt()\n","    else:\n","        boxes_areas_log = bbox_areas(gt_boxes)\n","    boxes_area_topk_log  = boxes_areas_log\n","    \n","    if wh_area_process == 'norm':\n","        boxes_area_topk_log[:] = 1.\n","\n","    \n","    # convert gt_boxes to output_resolution x output_resolution \n","    feat_gt_boxes = gt_boxes * output_h\n","    feat_gt_boxes[:, [0, 2]] = torch.clamp(feat_gt_boxes[:, [0, 2]], min=0,\n","                                           max=output_w - 1)\n","    feat_gt_boxes[:, [1, 3]] = torch.clamp(feat_gt_boxes[:, [1, 3]], min=0,\n","                                           max=output_h - 1)\n","    feat_hs, feat_ws = (feat_gt_boxes[:, 3] - feat_gt_boxes[:, 1],\n","                        feat_gt_boxes[:, 2] - feat_gt_boxes[:, 0])\n","\n","    # we calc the center and ignore area based on the gt-boxes of the origin scale\n","    # no peak will fall between pixels\n","    ct_ints = (torch.stack([(feat_gt_boxes[:, 0] + feat_gt_boxes[:, 2]) / 2,\n","                            (feat_gt_boxes[:, 1] + feat_gt_boxes[:, 3]) / 2],\n","                           dim=1)).to(torch.int)\n","    \n","    h_radiuses_alpha = (feat_hs / 2. * alpha).int()\n","    w_radiuses_alpha = (feat_ws / 2. * alpha).int()\n","    if wh_gaussian and alpha != beta:\n","        h_radiuses_beta = (feat_hs / 2. * beta).int()\n","        w_radiuses_beta = (feat_ws / 2. * beta).int()\n","\n","    if not wh_gaussian:\n","        # calculate positive (center) regions\n","        r1 = (1 - beta) / 2\n","        ctr_x1s, ctr_y1s, ctr_x2s, ctr_y2s = calc_region(gt_boxes.transpose(0, 1), r1)\n","        ctr_x1s, ctr_y1s, ctr_x2s, ctr_y2s = [torch.round(x.float() / down_ratio).int()\n","                                              for x in [ctr_x1s, ctr_y1s, ctr_x2s, ctr_y2s]]\n","        ctr_x1s, ctr_x2s = [torch.clamp(x, max=output_w - 1) for x in [ctr_x1s, ctr_x2s]]\n","        ctr_y1s, ctr_y2s = [torch.clamp(y, max=output_h - 1) for y in [ctr_y1s, ctr_y2s]]\n","\n","    # larger boxes have lower priority than small boxes.\n","    #for k in range(boxes_ind.shape[0]):\n","    k = 0\n","    cls_id = 0\n","    fake_heatmap = fake_heatmap.zero_()\n","    draw_truncate_gaussian(fake_heatmap, ct_ints[k],\n","                                h_radiuses_alpha[k].item(), w_radiuses_alpha[k].item())\n","    heatmap[cls_id] = torch.max(heatmap[cls_id], fake_heatmap)\n","\n","    if wh_gaussian:\n","        if alpha != beta:\n","            fake_heatmap = fake_heatmap.zero_()\n","            draw_truncate_gaussian(fake_heatmap, ct_ints[k],\n","                                        h_radiuses_beta[k].item(),\n","                                        w_radiuses_beta[k].item())\n","        box_target_inds = fake_heatmap > 0\n","    else:\n","        ctr_x1, ctr_y1, ctr_x2, ctr_y2 = ctr_x1s[k], ctr_y1s[k], ctr_x2s[k], ctr_y2s[k]\n","        box_target_inds = torch.zeros_like(fake_heatmap, dtype=torch.uint8)\n","        box_target_inds[ctr_y1:ctr_y2 + 1, ctr_x1:ctr_x2 + 1] = 1\n","\n","    if wh_agnostic:\n","        box_target[:, box_target_inds] = gt_boxes[k][:, None] * output_h\n","\n","    else:\n","        box_target[(cls_id * 4):((cls_id + 1) * 4), box_target_inds] = gt_boxes[k][:, None] * output_h\n","\n","    if wh_gaussian:\n","        local_heatmap = fake_heatmap[box_target_inds].float()\n","        ct_div = local_heatmap.sum()\n","        local_heatmap *= boxes_area_topk_log[k]\n","        reg_weight[cls_id, box_target_inds] = local_heatmap / ct_div\n","    else:\n","        reg_weight[cls_id, box_target_inds] = \\\n","            boxes_area_topk_log[k] / box_target_inds.sum().float()\n","\n","    return heatmap, box_target, reg_weight\n","\n","def simple_nms(heat, kernel=3, out_heat=None):\n","    pad = (kernel - 1) // 2\n","    hmax = nn.functional.max_pool2d(heat, (kernel, kernel), stride=1, padding=pad)\n","    keep = (hmax == heat).float()\n","    out_heat = heat if out_heat is None else out_heat\n","    return out_heat * keep\n","\n","def _topk(scores, topk):\n","    batch, cat, height, width = scores.size()\n","\n","    # both are (batch, 1, topk)\n","    topk_scores, topk_inds = torch.topk(scores.view(batch, cat, -1), topk)\n","\n","    topk_inds = topk_inds % (height * width)\n","    topk_ys = (topk_inds / width).int().float()\n","    topk_xs = (topk_inds % width).int().float()\n","\n","    # both are (batch, topk)\n","    topk_score, topk_ind = torch.topk(topk_scores.view(batch, -1), topk)\n","    topk_clses = (topk_ind / topk).int()\n","    topk_ind = topk_ind.unsqueeze(2)\n","    topk_inds = topk_inds.view(batch, -1, 1).gather(1, topk_ind).view(batch, topk)\n","    topk_ys = topk_ys.view(batch, -1, 1).gather(1, topk_ind).view(batch, topk)\n","    topk_xs = topk_xs.view(batch, -1, 1).gather(1, topk_ind).view(batch, topk)\n","\n","    return topk_score, topk_inds, topk_clses, topk_ys, topk_xs\n","\n","def get_bboxes_ttfnet(pred_heatmap,pred_wh):\n","    if pred_heatmap.shape[1]!=1:\n","        pred_heatmap = pred_heatmap.unsqueeze(1)\n","    \n","\n","    down_ratio = 1\n","\n","    batch, cat, height, width = pred_heatmap.size()\n","    pred_heatmap = pred_heatmap.detach()\n","    wh = pred_wh.detach()\n","    \n","    # perform nms on heatmaps\n","    heat = simple_nms(pred_heatmap)  # used maxpool to filter the max score\n","    \n","    topk = Bbox_topk_no_of_bboxes \n","    scores, inds, clses, ys, xs = _topk(heat, topk=topk)\n","   \n","    xs = xs.view(batch, topk, 1) * down_ratio\n","    ys = ys.view(batch, topk, 1) * down_ratio\n","        \n","    wh = wh.permute(0, 2, 3, 1).contiguous()\n","    wh = wh.view(wh.size(0), -1, wh.size(3))\n","    \n","    inds = inds.unsqueeze(2).expand(inds.size(0), inds.size(1), wh.size(2))\n","    \n","    #print(inds)\n","    wh = wh.gather(1, inds)\n","    wh = wh.view(batch, topk, 4)\n","    \n","    clses = clses.view(batch, topk, 1).float()\n","    scores = scores.view(batch, topk, 1)\n","    #print(clses, scores)\n","    bboxes = torch.cat([xs - wh[..., [0]], ys - wh[..., [1]],\n","                        xs + wh[..., [2]], ys + wh[..., [3]]], dim=2)\n","    #print(bboxes)\n","    result_list = []\n","    score_thr = Bbox_confidence_score_threshold\n","    #print(bboxes.shape[0])\n","    for batch_i in range(bboxes.shape[0]):\n","        scores_per_img = scores[batch_i]\n","        scores_keep = (scores_per_img > score_thr).squeeze(-1)\n","\n","        scores_per_img = scores_per_img[scores_keep]\n","        bboxes_per_img = bboxes[batch_i][scores_keep]\n","        labels_per_img = clses[batch_i][scores_keep]\n","        img_shape = [output_resolution,output_resolution]\n","        bboxes_per_img[:, 0::2] = bboxes_per_img[:, 0::2].clamp(min=0, max=img_shape[1] - 1)\n","        bboxes_per_img[:, 1::2] = bboxes_per_img[:, 1::2].clamp(min=0, max=img_shape[0] - 1)\n","        \n","        bboxes_per_img = torch.cat([bboxes_per_img, scores_per_img], dim=1)\n","        labels_per_img = labels_per_img.squeeze(-1)\n","        result_list.append((bboxes_per_img, labels_per_img))\n","\n","    return result_list\n","\n","def bbox_overlaps_ttfnet(bboxes1, bboxes2, mode='iou', is_aligned=False):\n","    \"\"\"Calculate overlap between two set of bboxes.\n","    If ``is_aligned`` is ``False``, then calculate the ious between each bbox\n","    of bboxes1 and bboxes2, otherwise the ious between each aligned pair of\n","    bboxes1 and bboxes2.\n","    Args:\n","        bboxes1 (Tensor): shape (m, 4)\n","        bboxes2 (Tensor): shape (n, 4), if is_aligned is ``True``, then m and n\n","            must be equal.\n","        mode (str): \"iou\" (intersection over union) or iof (intersection over\n","            foreground).\n","    Returns:\n","        ious(Tensor): shape (m, n) if is_aligned == False else shape (m, 1)\n","    \"\"\"\n","\n","    assert mode in ['iou', 'iof']\n","\n","    rows = bboxes1.size(0)\n","    cols = bboxes2.size(0)\n","    if is_aligned:\n","        assert rows == cols\n","\n","    if rows * cols == 0:\n","        return bboxes1.new(rows, 1) if is_aligned else bboxes1.new(rows, cols)\n","\n","    if is_aligned:\n","        lt = torch.max(bboxes1[:, :2], bboxes2[:, :2])  # [rows, 2]\n","        rb = torch.min(bboxes1[:, 2:], bboxes2[:, 2:])  # [rows, 2]\n","\n","        wh = (rb - lt + 1).clamp(min=0)  # [rows, 2]\n","        overlap = wh[:, 0] * wh[:, 1]\n","        area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (\n","            bboxes1[:, 3] - bboxes1[:, 1] + 1)\n","\n","        if mode == 'iou':\n","            area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (\n","                bboxes2[:, 3] - bboxes2[:, 1] + 1)\n","            ious = overlap / (area1 + area2 - overlap)\n","        else:\n","            ious = overlap / area1\n","    else:\n","        lt = torch.max(bboxes1[:, None, :2], bboxes2[:, :2])  # [rows, cols, 2]\n","        rb = torch.min(bboxes1[:, None, 2:], bboxes2[:, 2:])  # [rows, cols, 2]\n","\n","        wh = (rb - lt + 1).clamp(min=0)  # [rows, cols, 2]\n","        overlap = wh[:, :, 0] * wh[:, :, 1]\n","        area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (\n","            bboxes1[:, 3] - bboxes1[:, 1] + 1)\n","        \n","        if mode == 'iou':\n","            area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (\n","                bboxes2[:, 3] - bboxes2[:, 1] + 1)\n","            \n","            a_xmin = torch.min(bboxes1[:,0], bboxes2[:,0])\n","            a_ymin = torch.min(bboxes1[:,1], bboxes2[:,1])\n","            a_xmax = torch.max(bboxes1[:,2], bboxes2[:,2])\n","            a_ymax = torch.max(bboxes1[:,3], bboxes2[:,3])\n","            a_box = (a_xmax - a_xmin + 1) * (a_ymax - a_ymin + 1)\n","\n","            w = torch.min(area1 / area2, area2 / area1)\n","\n","            ious = overlap / (area1[:, None] + area2 - overlap)\n","            wuocs = (w[:,None] * ((area1[:, None] + area2 - overlap) / a_box[:, None]))\n","        else:\n","            ious = overlap / (area1[:, None])\n","    \n","    return ious,wuocs"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset Class"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T17:23:38.969801Z","iopub.status.busy":"2023-04-27T17:23:38.968941Z","iopub.status.idle":"2023-04-27T17:23:39.016632Z","shell.execute_reply":"2023-04-27T17:23:39.015615Z","shell.execute_reply.started":"2023-04-27T17:23:38.969760Z"},"trusted":true},"outputs":[],"source":["class GooRealDataset(Dataset):\n","    \n","    def __init__(self,df, get_transform, get_transform_modality, input_size=input_resolution, output_size=output_resolution,\n","                 test=False, modality=modality, imshow=False):\n","    \n","        self.df_data =  df\n","        self.image_dir = image_dir\n","        self.transform = get_transform\n","        self.transform_modality = get_transform_modality\n","        self.get_object_transform = _get_object_transform()\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.imshow = imshow\n","        self.test = test\n","        self.modality = modality\n","    def __len__(self):\n","        return len(self.df_data.index)\n","    \n","    def __getitem__(self, index):\n","        gaze_inside = True\n","        \n","        row = self.df_data.iloc[index]\n","        path = PureWindowsPath(row['filename']).as_posix()\n","        filename_no_extension = os.path.splitext(os.path.basename(path))[0]\n","        \n","        subfolder_path_str = os.path.splitext(path)[0].split(\"/\")\n","        key_filename = subfolder_path_str[0]+'/'+subfolder_path_str[1]\n","        \n","        img = Image.open(os.path.join(self.image_dir, path))\n","        img = img.convert('RGB').resize((640,480))\n","        \n","        width, height = img.size\n","        \n","        #gaze_x = row.gaze_cx / width\n","        #gaze_y = row.gaze_cy / height\n","        \n","        eye_x = row.hx / width\n","        eye_y = row.hy / height\n","        \n","        x_min, y_min, x_max, y_max = np.array(row['ann']['bboxes'][-1])\n","        \n","        gaze_obj_x_min, gaze_obj_y_min, gaze_obj_x_max, gaze_obj_y_max = np.array(row['ann']['bboxes'][row.gazeIdx])\n","        \n","        gazed_object_bbox = np.array(row['ann']['bboxes'][row.gazeIdx])\n","        gazed_object_class = np.array(row.gaze_item)\n","        all_object_bboxes = np.array(row['ann']['bboxes'])[:-1]\n","        all_object_bboxes_class = np.array(row['ann']['labels'])[:-1]\n","        \n","        # For CenterNet move the gaze point to center of the gazed at object bounding box\n","        gaze_x = ((gaze_obj_x_min+gaze_obj_x_max)/2)/ width\n","        gaze_y = ((gaze_obj_y_min+gaze_obj_y_max)/2)/ height\n","        \n","        # expand face bbox a bit\n","        k = 0.05\n","        x_min -= k * abs(x_max - x_min)\n","        y_min -= k * abs(y_max - y_min)\n","        x_max += k * abs(x_max - x_min)\n","        y_max += k * abs(y_max - y_min)\n","\n","        x_min, y_min, x_max, y_max = map(float, [x_min, y_min, x_max, y_max])\n","\n","        if self.test: \n","            \n","            self.pose_dir = 'goo-real-val-pose-directory'\n","            self.depth_dir = 'goo-real-val-depth-directory'\n","        \n","        else:\n","            \n","            self.pose_dir = 'goo-real-train-pose-directory'\n","            self.depth_dir = 'goo-real-train-depth-directory'\n","           \n","        # read pose which can be either .png or .jpg file\n","        if os.path.exists(os.path.join(self.pose_dir, key_filename,filename_no_extension+'-pose.png')):\n","            pose = Image.open(os.path.join(self.pose_dir, key_filename,filename_no_extension+'-pose.png'))\n","        else:\n","            pose = Image.open(os.path.join(self.pose_dir, key_filename,filename_no_extension+'-pose.jpg'))\n","        \n","        # read depth\n","        depth = Image.open(os.path.join(self.depth_dir,key_filename,filename_no_extension+'.png'))\n","        \n","        pose = pose.resize((640,480))\n","        depth = depth.resize((640,480))\n","    \n","        if self.imshow:\n","            img.save(\"origin_img.jpg\")\n","\n","        if self.test:\n","            imsize = torch.IntTensor([width, height])\n","            if privacy:\n","                img = Image.fromarray(np.uint8(np.zeros((height, width, 3))*255))\n","        \n","        else:\n","            ## data augmentation               \n","                        \n","            # Jitter (expansion-only) bounding box size\n","            if np.random.random_sample() <= 0.5:\n","                k = np.random.random_sample() * 0.2\n","                x_min -= k * abs(x_max - x_min)\n","                y_min -= k * abs(y_max - y_min)\n","                x_max += k * abs(x_max - x_min)\n","                y_max += k * abs(y_max - y_min)\n","\n","            # Random Crop\n","            if np.random.random_sample() <= 0.5:\n","                # Calculate the minimum valid range of the crop that doesn't exclude the face and the gaze target\n","                crop_x_min = np.min([gaze_x * width, x_min, x_max])\n","                crop_y_min = np.min([gaze_y * height, y_min, y_max])\n","                crop_x_max = np.max([gaze_x * width, x_min, x_max])\n","                crop_y_max = np.max([gaze_y * height, y_min, y_max])\n","\n","                # Randomly select a random top left corner\n","                if crop_x_min >= 0:\n","                    crop_x_min = np.random.uniform(0, crop_x_min)\n","                if crop_y_min >= 0:\n","                    crop_y_min = np.random.uniform(0, crop_y_min)\n","\n","                # Find the range of valid crop width and height starting from the (crop_x_min, crop_y_min)\n","                crop_width_min = crop_x_max - crop_x_min\n","                crop_height_min = crop_y_max - crop_y_min\n","                crop_width_max = width - crop_x_min\n","                crop_height_max = height - crop_y_min\n","                # Randomly select a width and a height\n","                crop_width = np.random.uniform(crop_width_min, crop_width_max)\n","                crop_height = np.random.uniform(crop_height_min, crop_height_max)\n","\n","                # Crop it\n","                img = TF.crop(img, crop_y_min, crop_x_min, crop_height, crop_width)\n","                pose = TF.crop(pose, crop_y_min, crop_x_min, crop_height, crop_width)\n","                depth = TF.crop(depth, crop_y_min, crop_x_min, crop_height, crop_width)\n","                \n","                # Record the crop's (x, y) offset\n","                offset_x, offset_y = crop_x_min, crop_y_min\n","\n","                # convert coordinates into the cropped frame\n","                x_min, y_min, x_max, y_max = x_min - offset_x, y_min - offset_y, x_max - offset_x, y_max - offset_y\n","                # if gaze_inside:\n","                gaze_x, gaze_y = (gaze_x * width - offset_x) / float(crop_width), \\\n","                                 (gaze_y * height - offset_y) / float(crop_height)\n","                eye_x, eye_y = (eye_x * width - offset_x) / float(crop_width), \\\n","                                 (eye_y * height - offset_y) / float(crop_height)\n","                # else:\n","                #     gaze_x = -1; gaze_y = -1\n","                \n","                gaze_obj_x_min -= offset_x\n","                gaze_obj_y_min -= offset_y\n","                gaze_obj_x_max -= offset_x\n","                gaze_obj_y_max -= offset_y\n","                \n","                width, height = crop_width, crop_height\n","\n","            # Random flip\n","            if np.random.random_sample() <= 0.5:\n","                \n","                img = img.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n","                pose = pose.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n","                depth = depth.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n","                                \n","                x_max_2 = width - x_min\n","                x_min_2 = width - x_max\n","                x_max = x_max_2\n","                x_min = x_min_2\n","                gaze_x = 1 - gaze_x\n","                eye_x = 1 - eye_x\n","                \n","                # flip the GT gazed object bbox\n","                gaze_obj_x_max_2 = width - gaze_obj_x_min\n","                gaze_obj_x_min_2 = width - gaze_obj_x_max\n","                gaze_obj_x_max = gaze_obj_x_max_2\n","                gaze_obj_x_min = gaze_obj_x_min_2\n","            \n","            \n","            # Random flip vertical\n","            if np.random.random_sample() <= 0.5:\n","                \n","                img = img.transpose(Image.Transpose.FLIP_TOP_BOTTOM)\n","                pose = pose.transpose(Image.Transpose.FLIP_TOP_BOTTOM)\n","                depth = depth.transpose(Image.Transpose.FLIP_TOP_BOTTOM)\n","                                \n","                y_max_2 = height - y_min\n","                y_min_2 = height - y_max\n","                y_max = y_max_2\n","                y_min = y_min_2\n","                gaze_y = 1 - gaze_y\n","                eye_y = 1 - eye_y\n","                \n","                # flip the GT gazed object bbox\n","                gaze_obj_y_max_2 = height - gaze_obj_y_min\n","                gaze_obj_y_min_2 = height - gaze_obj_y_max\n","                gaze_obj_y_max = gaze_obj_y_max_2\n","                gaze_obj_y_min = gaze_obj_y_min_2\n","            \n","\n","                \n","            # Random color change\n","            if np.random.random_sample() <= 0.5:\n","                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n","                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n","                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n","\n","        if cone_mode=='early':\n","            cone_resolution = input_resolution\n","        else:\n","            cone_resolution = input_resolution // 4\n","        \n","        head_channel = get_head_box_channel(x_min, y_min, x_max, y_max, width, height,\n","                                                    resolution=cone_resolution, coordconv=False).unsqueeze(0)\n","\n","        # Crop the face\n","        if privacy:\n","            face = pose.crop((int(x_min), int(y_min), int(x_max), int(y_max)))\n","        else:\n","            face = img.crop((int(x_min), int(y_min), int(x_max), int(y_max)))\n","\n","        # modality dropout\n","        height, width = int(height), int(width)\n","        num_modalities = 3\n","        dropped = np.zeros(num_modalities)\n","        if not self.test and self.modality=='attention':\n","            if modality_dropout:\n","                # keep one modality\n","                if privacy:\n","                    modality_idx = 1\n","                else:\n","                    modality_idx = 0\n","                m_keep = np.random.randint(modality_idx, num_modalities)\n","\n","                if (np.random.rand() <= 0.2) and m_keep!=0:\n","                    img = Image.fromarray(np.uint8(np.random.rand(height, width, 3)*255))\n","                    dropped[0] = 1\n","                if (np.random.rand() <= 0.2) and m_keep!=1:\n","                    depth = Image.fromarray(np.uint8(np.random.rand(height, width)*255))\n","                    dropped[1] = 1\n","                if (np.random.rand() <= 0.2) and m_keep!=2:\n","                    pose = Image.fromarray(np.uint8(np.random.rand(height, width, 3)*255))\n","                    dropped[2] = 1\n","\n","            if privacy:\n","                img = Image.fromarray(np.uint8(np.zeros((height, width, 3))))\n","                dropped[0] = 1\n","        \n","        gazed_object_bbox = np.array([gaze_obj_x_min, gaze_obj_y_min, gaze_obj_x_max, gaze_obj_y_max])\n","\n","            \n","        # generate new gaze field (for human-centric branch)\n","        eye_point = np.array([eye_x, eye_y])\n","        gaze = np.array([gaze_x, gaze_y])\n","        gt_direction = np.array([-1.0, -1.0])\n","        if gaze_inside:\n","            gt_direction = gaze - eye_point            \n","            if gt_direction.mean()!=0:\n","                gt_direction = gt_direction / np.linalg.norm(gt_direction)\n","        \n","        gaze_field = generate_data_field(eye_point, width=cone_resolution, height=cone_resolution)\n","        # normalize\n","        norm = np.sqrt(np.sum(gaze_field ** 2, axis=0)).reshape([1, cone_resolution, cone_resolution])\n","        # avoid zero norm\n","        norm = np.maximum(norm, 0.1)\n","        gaze_field /= norm\n","          \n","        if self.transform is not None:\n","            img = self.transform(img)\n","            face = self.transform(face)\n","            \n","            pose = self.transform_modality(pose)\n","            depth = self.transform_modality(depth)\n","            #depth = depth / 65535    # depth maps are in 16 bit format\n","        \n","         \n","        ## for TTFNet\n","        gazed_object_box_for_cp = torch.tensor([gaze_obj_x_min/width,gaze_obj_y_min/height,gaze_obj_x_max/width,gaze_obj_y_max/height]).unsqueeze(0)\n","        heatmap_cp, box_target_cp, reg_weight_cp =  target_single_image(gazed_object_box_for_cp, feat_shape=(self.output_size, self.output_size)) \n","        \n","        # generate the heat map used for deconv prediction\n","        gaze_heatmap = torch.zeros(self.output_size, self.output_size)  # set the size of the output\n","        #gaze_heatmap_org = torch.zeros(self.output_size, self.output_size)  # set the size of the output\n","        if self.test:  # aggregated heatmap\n","            if gaze_x != -1:\n","                #gaze_heatmap_org = draw_labelmap(gaze_heatmap_org, [gaze_x * self.output_size, gaze_y * self.output_size], 3, type='Gaussian')\n","                gaze_heatmap = heatmap_cp.squeeze(0).float() ## for TTFNet\n","        else:\n","            #gaze_heatmap_org = draw_labelmap(gaze_heatmap_org, [gaze_x * self.output_size, gaze_y * self.output_size], 3, type='Gaussian')\n","            gaze_heatmap = heatmap_cp.squeeze(0).float() ## for TTFNet\n","        \n","        if gaze_inside:\n","            cont_gaze = [gaze_x, gaze_y]\n","        else:\n","            cont_gaze = [-1, -1]\n","        cont_gaze = torch.FloatTensor(cont_gaze)\n","        \n","        if self.imshow:\n","            fig = plt.figure(111)\n","            img = 255 - unnorm(img.numpy()) * 255\n","            img = np.clip(img, 0, 255)\n","            plt.imshow(np.transpose(img, (1, 2, 0)))\n","            plt.imshow(cv2.resize(gaze_heatmap, (self.input_size, self.input_size)), cmap='jet', alpha=0.3)\n","            plt.imshow(cv2.resize(1 - head_channel.squeeze(0), (self.input_size, self.input_size)), alpha=0.2)\n","            plt.savefig('viz_aug.png')\n","\n","        if self.test:\n","            return img, face, pose, depth, gaze_field, gt_direction, head_channel, gaze_heatmap, cont_gaze, imsize, path, eye_point, all_object_bboxes, all_object_bboxes_class, gazed_object_bbox, gazed_object_class\n","        else:\n","            return img, face, pose, depth, gaze_field, gt_direction, head_channel, gaze_heatmap, path, gaze_inside, dropped, box_target_cp, reg_weight_cp, gazed_object_bbox, height, width\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T17:23:39.018900Z","iopub.status.busy":"2023-04-27T17:23:39.018521Z","iopub.status.idle":"2023-04-27T17:23:39.078651Z","shell.execute_reply":"2023-04-27T17:23:39.077614Z","shell.execute_reply.started":"2023-04-27T17:23:39.018864Z"},"trusted":true},"outputs":[],"source":["class GooRealPlusSynthDataset(Dataset):\n","    \n","    def __init__(self,df, get_transform, get_transform_modality, input_size=input_resolution, output_size=output_resolution,\n","                 test=False, modality=modality, imshow=False):\n","    \n","        self.df_data =  df\n","        self.image_dir = image_dir\n","        self.transform = get_transform\n","        self.transform_modality = get_transform_modality\n","        self.get_object_transform = _get_object_transform()\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.imshow = imshow\n","        self.test = test\n","        self.modality = modality\n","    def __len__(self):\n","        return len(self.df_data.index)\n","    \n","    def __getitem__(self, index):\n","        gaze_inside = True\n","        \n","        row = self.df_data.iloc[index]\n","        \n","        # GOO Real Dataset\n","        if not row['dataset_id']:\n","            self.image_dir = 'GOO-Real-Dataset-Images-Directory'\n","            path = PureWindowsPath(row['filename']).as_posix()\n","            filename_no_extension = os.path.splitext(os.path.basename(path))[0]\n","            subfolder_path_str = os.path.splitext(path)[0].split(\"/\")\n","            key_filename = subfolder_path_str[0]+'/'+subfolder_path_str[1]\n","        \n","        # GOO Synth Test Dataset\n","        else:\n","            self.image_dir = 'GOO-Synth-Test-Dataset-Images-Directory'\n","            path = row['filename']\n","            filename_no_extension = path.split('.')[0]\n","            \n","        img = Image.open(os.path.join(self.image_dir, path))\n","        img = img.convert('RGB').resize((640,480))\n","        \n","        width, height = img.size\n","        \n","        #gaze_x = row.gaze_cx / width\n","        #gaze_y = row.gaze_cy / height\n","        \n","        eye_x = row.hx / width\n","        eye_y = row.hy / height\n","        \n","        x_min, y_min, x_max, y_max = np.array(row['ann']['bboxes'][-1])\n","        \n","        gaze_obj_x_min, gaze_obj_y_min, gaze_obj_x_max, gaze_obj_y_max = np.array(row['ann']['bboxes'][row.gazeIdx])\n","        \n","        gazed_object_bbox = np.array(row['ann']['bboxes'][row.gazeIdx])\n","        gazed_object_class = np.array(row.gaze_item)\n","        all_object_bboxes = np.array(row['ann']['bboxes'])[:-1]\n","        all_object_bboxes_class = np.array(row['ann']['labels'])[:-1]\n","        \n","        # For CenterNet move the gaze point to center of the gazed at object bounding box\n","        #if self.test:\n","        gaze_x = ((gaze_obj_x_min+gaze_obj_x_max)/2)/ width\n","        gaze_y = ((gaze_obj_y_min+gaze_obj_y_max)/2)/ height\n","        \n","        # expand face bbox a bit\n","        k = 0.05\n","        x_min -= k * abs(x_max - x_min)\n","        y_min -= k * abs(y_max - y_min)\n","        x_max += k * abs(x_max - x_min)\n","        y_max += k * abs(y_max - y_min)\n","\n","        x_min, y_min, x_max, y_max = map(float, [x_min, y_min, x_max, y_max])\n","\n","        if not row['dataset_id']:\n","            \n","            # point to val dir\n","            if self.test: \n","            \n","                self.pose_dir = 'goo-real-val-pose-directory'\n","                self.depth_dir = 'goo-real-val-depth-directory'\n","            \n","            # point to train dir\n","            else:\n","                \n","                self.pose_dir = 'goo-real-train-pose-directory'\n","                self.depth_dir = 'goo-real-train-depth-directory'\n","            \n","            # read pose\n","            if os.path.exists(os.path.join(self.pose_dir, key_filename,filename_no_extension+'-pose.png')):\n","                pose = Image.open(os.path.join(self.pose_dir, key_filename,filename_no_extension+'-pose.png'))\n","            else:\n","                pose = Image.open(os.path.join(self.pose_dir, key_filename,filename_no_extension+'-pose.jpg'))\n","            # read depth\n","            depth = Image.open(os.path.join(self.depth_dir,key_filename,filename_no_extension+'.png'))\n","        else:\n","            # point to val dir\n","            if self.test: \n","            \n","                self.pose_dir = 'goo-synth-test-val-pose-directory'\n","                self.depth_dir = 'goo-synth-test-val-depth-directory'\n","            \n","            # point to train dir\n","            else:\n","                \n","                self.pose_dir = 'goo-synth-test-train-pose-directory'\n","                self.depth_dir = 'goo-synth-test-train-depth-directory'\n","                \n","            # read pose\n","            if os.path.exists(os.path.join(self.pose_dir,filename_no_extension+'-pose.png')):\n","                pose = Image.open(os.path.join(self.pose_dir,filename_no_extension+'-pose.png'))\n","            else:\n","                pose = Image.open(os.path.join(self.pose_dir,filename_no_extension+'-pose.jpg'))\n","            # read depth\n","            depth = Image.open(os.path.join(self.depth_dir,filename_no_extension+'.png'))\n","        \n","        pose = pose.resize((640,480))\n","        depth = depth.resize((640,480))\n","        \n","        if self.imshow:\n","            img.save(\"origin_img.jpg\")\n","\n","        if self.test:\n","            imsize = torch.IntTensor([width, height])\n","            if privacy:\n","                img = Image.fromarray(np.uint8(np.zeros((height, width, 3))*255))\n","        else:\n","            ## data augmentation               \n","                        \n","            # Jitter (expansion-only) bounding box size\n","            if np.random.random_sample() <= 0.5:\n","                k = np.random.random_sample() * 0.2\n","                x_min -= k * abs(x_max - x_min)\n","                y_min -= k * abs(y_max - y_min)\n","                x_max += k * abs(x_max - x_min)\n","                y_max += k * abs(y_max - y_min)\n","\n","            # Random Crop\n","            if np.random.random_sample() <= 0.5:\n","                # Calculate the minimum valid range of the crop that doesn't exclude the face and the gaze target\n","                crop_x_min = np.min([gaze_x * width, x_min, x_max])\n","                crop_y_min = np.min([gaze_y * height, y_min, y_max])\n","                crop_x_max = np.max([gaze_x * width, x_min, x_max])\n","                crop_y_max = np.max([gaze_y * height, y_min, y_max])\n","\n","                # Randomly select a random top left corner\n","                if crop_x_min >= 0:\n","                    crop_x_min = np.random.uniform(0, crop_x_min)\n","                if crop_y_min >= 0:\n","                    crop_y_min = np.random.uniform(0, crop_y_min)\n","\n","                # Find the range of valid crop width and height starting from the (crop_x_min, crop_y_min)\n","                crop_width_min = crop_x_max - crop_x_min\n","                crop_height_min = crop_y_max - crop_y_min\n","                crop_width_max = width - crop_x_min\n","                crop_height_max = height - crop_y_min\n","                # Randomly select a width and a height\n","                crop_width = np.random.uniform(crop_width_min, crop_width_max)\n","                crop_height = np.random.uniform(crop_height_min, crop_height_max)\n","\n","                # Crop it\n","                img = TF.crop(img, crop_y_min, crop_x_min, crop_height, crop_width)\n","                pose = TF.crop(pose, crop_y_min, crop_x_min, crop_height, crop_width)\n","                depth = TF.crop(depth, crop_y_min, crop_x_min, crop_height, crop_width)\n","                \n","                # Record the crop's (x, y) offset\n","                offset_x, offset_y = crop_x_min, crop_y_min\n","\n","                # convert coordinates into the cropped frame\n","                x_min, y_min, x_max, y_max = x_min - offset_x, y_min - offset_y, x_max - offset_x, y_max - offset_y\n","                # if gaze_inside:\n","                gaze_x, gaze_y = (gaze_x * width - offset_x) / float(crop_width), \\\n","                                 (gaze_y * height - offset_y) / float(crop_height)\n","                eye_x, eye_y = (eye_x * width - offset_x) / float(crop_width), \\\n","                                 (eye_y * height - offset_y) / float(crop_height)\n","                # else:\n","                #     gaze_x = -1; gaze_y = -1\n","                \n","                gaze_obj_x_min -= offset_x\n","                gaze_obj_y_min -= offset_y\n","                gaze_obj_x_max -= offset_x\n","                gaze_obj_y_max -= offset_y\n","                \n","                width, height = crop_width, crop_height\n","\n","            # Random flip\n","            if np.random.random_sample() <= 0.5:\n","                img = img.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n","                pose = pose.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n","                depth = depth.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n","                                \n","                x_max_2 = width - x_min\n","                x_min_2 = width - x_max\n","                x_max = x_max_2\n","                x_min = x_min_2\n","                gaze_x = 1 - gaze_x\n","                eye_x = 1 - eye_x\n","                \n","                # flip the GT gazed object bbox\n","                gaze_obj_x_max_2 = width - gaze_obj_x_min\n","                gaze_obj_x_min_2 = width - gaze_obj_x_max\n","                gaze_obj_x_max = gaze_obj_x_max_2\n","                gaze_obj_x_min = gaze_obj_x_min_2\n","\n","            # Random color change\n","            if np.random.random_sample() <= 0.5:\n","                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n","                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n","                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n","\n","        if cone_mode=='early':\n","            cone_resolution = input_resolution\n","        else:\n","            cone_resolution = input_resolution // 4\n","        \n","        head_channel = get_head_box_channel(x_min, y_min, x_max, y_max, width, height,\n","                                                    resolution=cone_resolution, coordconv=False).unsqueeze(0)\n","\n","        # Crop the face\n","        if privacy:\n","            face = pose.crop((int(x_min), int(y_min), int(x_max), int(y_max)))\n","        else:\n","            face = img.crop((int(x_min), int(y_min), int(x_max), int(y_max)))\n","\n","        # modality dropout\n","        height, width = int(height), int(width)\n","        num_modalities = 3\n","        dropped = np.zeros(num_modalities)\n","        if not self.test and self.modality=='attention':\n","            if modality_dropout:\n","                # keep one modality\n","                if privacy:\n","                    modality_idx = 1\n","                else:\n","                    modality_idx = 0\n","                m_keep = np.random.randint(modality_idx, num_modalities)\n","\n","                if (np.random.rand() <= 0.2) and m_keep!=0:\n","                    img = Image.fromarray(np.uint8(np.random.rand(height, width, 3)*255))\n","                    dropped[0] = 1\n","                if (np.random.rand() <= 0.2) and m_keep!=1:\n","                    depth = Image.fromarray(np.uint8(np.random.rand(height, width)*255))\n","                    dropped[1] = 1\n","                if (np.random.rand() <= 0.2) and m_keep!=2:\n","                    pose = Image.fromarray(np.uint8(np.random.rand(height, width, 3)*255))\n","                    dropped[2] = 1\n","\n","            if privacy:\n","                img = Image.fromarray(np.uint8(np.zeros((height, width, 3))))\n","                dropped[0] = 1\n","        \n","        gazed_object_bbox = np.array([gaze_obj_x_min, gaze_obj_y_min, gaze_obj_x_max, gaze_obj_y_max])\n","\n","            \n","        # generate new gaze field (for human-centric branch)\n","        eye_point = np.array([eye_x, eye_y])\n","        gaze = np.array([gaze_x, gaze_y])\n","        gt_direction = np.array([-1.0, -1.0])\n","        if gaze_inside:\n","            gt_direction = gaze - eye_point            \n","            if gt_direction.mean()!=0:\n","                gt_direction = gt_direction / np.linalg.norm(gt_direction)\n","        \n","        gaze_field = generate_data_field(eye_point, width=cone_resolution, height=cone_resolution)\n","        # normalize\n","        norm = np.sqrt(np.sum(gaze_field ** 2, axis=0)).reshape([1, cone_resolution, cone_resolution])\n","        # avoid zero norm\n","        norm = np.maximum(norm, 0.1)\n","        gaze_field /= norm\n","          \n","        if self.transform is not None:\n","            img = self.transform(img)\n","            face = self.transform(face)\n","            \n","            pose = self.transform_modality(pose)\n","            depth = self.transform_modality(depth)\n","            #depth = depth / 65535    # depth maps are in 16 bit format\n","        \n","         \n","        ## for TTFNet\n","        gazed_object_box_for_cp = torch.tensor([gaze_obj_x_min/width,gaze_obj_y_min/height,gaze_obj_x_max/width,gaze_obj_y_max/height]).unsqueeze(0)\n","        heatmap_cp, box_target_cp, reg_weight_cp =  target_single_image(gazed_object_box_for_cp, feat_shape=(self.output_size, self.output_size)) \n","        \n","        # generate the heat map used for deconv prediction\n","        gaze_heatmap = torch.zeros(self.output_size, self.output_size)  # set the size of the output\n","        #gaze_heatmap_org = torch.zeros(self.output_size, self.output_size)  # set the size of the output\n","        if self.test:  # aggregated heatmap\n","            if gaze_x != -1:\n","                #gaze_heatmap_org = draw_labelmap(gaze_heatmap_org, [gaze_x * self.output_size, gaze_y * self.output_size], 3, type='Gaussian')\n","                gaze_heatmap = heatmap_cp.squeeze(0).float() ## for TTFNet\n","        else:\n","            #gaze_heatmap_org = draw_labelmap(gaze_heatmap_org, [gaze_x * self.output_size, gaze_y * self.output_size], 3, type='Gaussian')\n","            gaze_heatmap = heatmap_cp.squeeze(0).float() ## for TTFNet\n","        \n","        if gaze_inside:\n","            cont_gaze = [gaze_x, gaze_y]\n","        else:\n","            cont_gaze = [-1, -1]\n","        cont_gaze = torch.FloatTensor(cont_gaze)\n","        \n","        if self.imshow:\n","            fig = plt.figure(111)\n","            img = 255 - unnorm(img.numpy()) * 255\n","            img = np.clip(img, 0, 255)\n","            plt.imshow(np.transpose(img, (1, 2, 0)))\n","            plt.imshow(cv2.resize(gaze_heatmap, (self.input_size, self.input_size)), cmap='jet', alpha=0.3)\n","            plt.imshow(cv2.resize(1 - head_channel.squeeze(0), (self.input_size, self.input_size)), alpha=0.2)\n","            plt.savefig('viz_aug.png')\n","\n","        if self.test:\n","            return img, face, pose, depth, gaze_field, gt_direction, head_channel, gaze_heatmap, cont_gaze, imsize, path, eye_point, all_object_bboxes, all_object_bboxes_class, gazed_object_bbox, gazed_object_class\n","        else:\n","            return img, face, pose, depth, gaze_field, gt_direction, head_channel, gaze_heatmap, path, gaze_inside, dropped, box_target_cp, reg_weight_cp, gazed_object_bbox, height, width\n","        \n","     \n","    "]},{"cell_type":"markdown","metadata":{},"source":["### Dataloader Sanity Check"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T17:23:39.081008Z","iopub.status.busy":"2023-04-27T17:23:39.080599Z","iopub.status.idle":"2023-04-27T17:23:42.293646Z","shell.execute_reply":"2023-04-27T17:23:42.292276Z","shell.execute_reply.started":"2023-04-27T17:23:39.080972Z"},"trusted":true},"outputs":[],"source":["\n","transform = _get_transform()\n","transform_modality = _get_transform_modality()\n","    \n","if use_only_real_dataset_for_train:\n","    val_dataset = GooRealDataset(df_train, transform, transform_modality, \n","                           input_size=input_resolution, output_size=output_resolution, \n","                           test=False, modality=modality , imshow=False)\n","    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n","                                               batch_size=1,\n","                                               shuffle=True,\n","                                               num_workers=0)\n","else:\n","    val_dataset = GooRealPlusSynthDataset(df_train, transform, transform_modality, \n","                           input_size=input_resolution, output_size=output_resolution, \n","                           test=False, modality=modality , imshow=False)\n","    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n","                                               batch_size=1,\n","                                               shuffle=True,\n","                                               num_workers=0)\n","encoded_inputs = next(iter(val_loader))\n","img, face, pose, depth, gaze_field, gt_direction, head_channel, gaze_heatmap, path, gaze_inside, dropped, box_target_cp, reg_weight_cp, gazed_object_bbox, height, width = encoded_inputs\n","\n","plt.imshow(img.squeeze(0).numpy().transpose(1,2,0))\n","plt.show()\n","plt.imshow(depth.squeeze(0).numpy().transpose(1,2,0))\n","plt.show()\n","plt.imshow(pose.squeeze(0).numpy().transpose(1,2,0))\n","plt.show()\n","plt.imshow(face.squeeze(0).numpy().transpose(1,2,0))\n","plt.show()\n","plt.imshow(gaze_field.squeeze(0).numpy().transpose(1,2,0)[:,:,0])\n","plt.show()\n","plt.imshow(gaze_field.squeeze(0).numpy().transpose(1,2,0)[:,:,1])\n","plt.show()\n","print(gt_direction)\n","plt.imshow(head_channel.squeeze(0).numpy().transpose(1,2,0))\n","plt.show()\n","plt.imshow(gaze_heatmap.squeeze(0).numpy())\n","plt.show()\n","print(gaze_heatmap.min(),gaze_heatmap.max())\n","print(np.argmax(gaze_heatmap.squeeze(0).numpy()))\n","print(gaze_heatmap.shape)\n","print(path)\n","print(gaze_inside)\n","print(dropped)\n","for index in range(4):\n","    plt.imshow(box_target_cp.squeeze(0)[index])\n","    plt.show()\n","    print(box_target_cp.squeeze(0)[index].min(),box_target_cp.squeeze(0)[index].max())\n","\n","plt.imshow(reg_weight_cp.squeeze(0)[0])\n","plt.show()\n","print(gazed_object_bbox)\n","print(height, width)"]},{"cell_type":"markdown","metadata":{},"source":["## Loss Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T17:23:42.297092Z","iopub.status.busy":"2023-04-27T17:23:42.296808Z","iopub.status.idle":"2023-04-27T17:23:42.321860Z","shell.execute_reply":"2023-04-27T17:23:42.320864Z","shell.execute_reply.started":"2023-04-27T17:23:42.297065Z"},"trusted":true},"outputs":[],"source":["def ct_focal_loss(pred, gt, gamma=2.0):\n","    \"\"\"\n","    Focal loss used in CornerNet & CenterNet. Note that the values in gt (label) are in [0, 1] since\n","    gaussian is used to reduce the punishment and we treat [0, 1) as neg example.\n","    Args:\n","        pred: tensor, any shape.\n","        gt: tensor, same as pred.\n","        gamma: gamma in focal loss.\n","    Returns:\n","    \"\"\"\n","    \n","    pos_inds = gt.eq(1).float()\n","    neg_inds = gt.lt(1).float()\n","    \n","    neg_weights = torch.pow(1 - gt, 4)  # reduce punishment\n","    pos_loss = -torch.log(pred) * torch.pow(1 - pred, gamma) * pos_inds\n","    neg_loss = -torch.log(1 - pred) * torch.pow(pred, gamma) * neg_weights * neg_inds\n","\n","    num_pos = pos_inds.float().sum()\n","    pos_loss = pos_loss.sum()\n","    neg_loss = neg_loss.sum()\n","    \n","    if num_pos == 0:\n","        return neg_loss\n","    return (pos_loss + neg_loss) / num_pos\n","\n","def giou_loss(pred,\n","              target,\n","              weight,\n","              avg_factor=None):\n","    \"\"\"GIoU loss.\n","    Computing the GIoU loss between a set of predicted bboxes and target bboxes.\n","    \"\"\"\n","    pos_mask = weight > 0\n","    weight = weight[pos_mask].float()\n","    if avg_factor is None:\n","        avg_factor = torch.sum(pos_mask).float().item() + 1e-6\n","    bboxes1 = pred[pos_mask].view(-1, 4)\n","    bboxes2 = target[pos_mask].view(-1, 4)\n","    lt = torch.max(bboxes1[:, :2], bboxes2[:, :2])  # [rows, 2]\n","    rb = torch.min(bboxes1[:, 2:], bboxes2[:, 2:])  # [rows, 2]\n","    wh = (rb - lt + 1).clamp(min=0)  # [rows, 2]\n","    enclose_x1y1 = torch.min(bboxes1[:, :2], bboxes2[:, :2])\n","    enclose_x2y2 = torch.max(bboxes1[:, 2:], bboxes2[:, 2:])\n","    enclose_wh = (enclose_x2y2 - enclose_x1y1 + 1).clamp(min=0)\n","\n","    overlap = wh[:, 0] * wh[:, 1]\n","    ap = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (bboxes1[:, 3] - bboxes1[:, 1] + 1)\n","    ag = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (bboxes2[:, 3] - bboxes2[:, 1] + 1)\n","    ious = overlap / (ap + ag - overlap)\n","\n","    enclose_area = enclose_wh[:, 0] * enclose_wh[:, 1]  # i.e. C in paper\n","    u = ap + ag - overlap\n","    gious = ious - (enclose_area - u) / enclose_area\n","    iou_distances = 1 - gious\n","    return torch.sum(iou_distances * weight)[None] / avg_factor\n","\n","def diou_loss(pred,\n","              target,\n","              weight,\n","              avg_factor=None):\n","    \"\"\"DIoU loss.\n","    Computing the GIoU loss between a set of predicted bboxes and target bboxes.\n","    https://pytorch.org/vision/main/generated/torchvision.ops.distance_box_iou_loss.html\n","    \"\"\"\n","    pos_mask = weight > 0\n","    weight = weight[pos_mask].float()\n","    if avg_factor is None:\n","        avg_factor = torch.sum(pos_mask).float().item() + 1e-6\n","    bboxes1 = pred[pos_mask].view(-1, 4)\n","    bboxes2 = target[pos_mask].view(-1, 4)\n","    iou_distances = torchvision.ops.distance_box_iou_loss(bboxes1,bboxes2,reduction='None')\n","    return torch.sum(iou_distances * weight)[None] / avg_factor\n","\n","def ciou_loss(pred,\n","              target,\n","              weight,\n","              avg_factor=None):\n","    \"\"\"CIoU loss.\n","    Computing the GIoU loss between a set of predicted bboxes and target bboxes.\n","    https://pytorch.org/vision/main/generated/torchvision.ops.complete_box_iou_loss.html\n","    \"\"\"\n","    pos_mask = weight > 0\n","    weight = weight[pos_mask].float()\n","    if avg_factor is None:\n","        avg_factor = torch.sum(pos_mask).float().item() + 1e-6\n","    bboxes1 = pred[pos_mask].view(-1, 4)\n","    bboxes2 = target[pos_mask].view(-1, 4)\n","    iou_distances = torchvision.ops.complete_box_iou_loss(bboxes1,bboxes2,reduction='None')\n","    return torch.sum(iou_distances * weight)[None] / avg_factor\n","\n","def loss_calc_ttfnet(pred_hm, pred_wh, heatmap, box_target,wh_weight=5, gaze_inside=None):\n","        \"\"\"\n","        Args:\n","            pred_hm: tensor, (batch, 1, h, w).\n","            pred_wh: tensor, (batch, 4, h, w)\n","            heatmap: tensor, same as pred_hm.\n","            box_target: tensor, same as pred_wh.\n","            wh_weight: tensor, same as pred_wh.\n","        Returns:\n","            hm_loss\n","            wh_loss\n","        \"\"\"\n","        if pred_hm.shape[1]!=1:\n","            pred_hm = pred_hm.unsqueeze(dim=1)\n","        if heatmap.shape[1]!=1:\n","            heatmap = heatmap.unsqueeze(dim=1)\n","        \n","        \n","        H, W = pred_hm.shape[2:]\n","        \n","        pred_hm = torch.clamp(pred_hm, min=1e-4, max=1 - 1e-4)\n","        hm_loss = ct_focal_loss(pred_hm, heatmap)\n","        \n","        # case where not all the images have gaze target inside the image are nt covered   \n","                 \n","        mask = wh_weight.view(-1, H, W)\n","        avg_factor = mask.sum() + 1e-4\n","        shifts_x = torch.arange(0, W , 1, dtype=torch.float32, device=heatmap.device)\n","        shifts_y = torch.arange(0, H , 1, dtype=torch.float32, device=heatmap.device)\n","        shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x, indexing='ij')\n","        base_loc = torch.stack((shift_x, shift_y), dim=0)  # (2, h, w)\n","        \n","        # (batch, h, w, 4)\n","        pred_boxes = torch.cat((base_loc - pred_wh[:, [0, 1]], base_loc + pred_wh[:, [2, 3]]), dim=1).permute(0, 2, 3, 1).float()\n","        \n","        # (batch, h, w, 4)\n","        boxes = box_target.permute(0, 2, 3, 1).float()\n","        #wh_loss = giou_loss(pred_boxes, boxes, mask, avg_factor=avg_factor)\n","        #wh_loss = diou_loss(pred_boxes, boxes, mask, avg_factor=avg_factor) \n","        #wh_loss = ciou_loss(pred_boxes, boxes, mask, avg_factor=avg_factor)\n","        wh_loss = ciou_loss(pred_boxes, boxes, mask, avg_factor=None)\n","        #print(f'wh_loss: {wh_loss}')\n","        return hm_loss, wh_loss\n","\n","# Get energy aggregation loss from GaTector paper\n","def compute_heatmap_loss_by_gtbox_predheatmap(box, heatmap, width, height):\n","    \n","    # Use ground truth box and predicted heatmap to compute the energy aggregation loss\n","    # GT bbox is passed in size (640, 480)\n","    # Check the scale factor orginal = 10 *\n","    \n","    batch_size = heatmap.size()[0]\n","    power, total_power, total_eng_loss= 0., 0., 0.\n","    for i in range(batch_size):\n","        cur_box = box[i]\n","        cur_heatmap = heatmap[i]\n","        #cur_heatmap = torch.clip(heatmap[i], min=0)\n","        xmin, ymin, xmax, ymax = int((cur_box[0] / width[i]) * output_resolution), int((cur_box[1] / height[i]) * output_resolution), int((cur_box[2] / width[i]) * output_resolution), int((cur_box[3] / height[i])* output_resolution)\n","        # axis are flipped in the heatmap\n","        power = torch.sum(cur_heatmap[ymin: ymax + 1, xmin: xmax + 1])\n","        total_power = cur_heatmap.sum()\n","        eng_loss = 1 - (power / total_power)\n","        total_eng_loss = total_eng_loss + eng_loss\n","    return 1 * (total_eng_loss / batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["## Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T17:23:42.337396Z","iopub.status.busy":"2023-04-27T17:23:42.336983Z","iopub.status.idle":"2023-04-27T17:23:42.421393Z","shell.execute_reply":"2023-04-27T17:23:42.420299Z","shell.execute_reply.started":"2023-04-27T17:23:42.337356Z"},"trusted":true},"outputs":[],"source":["cos_sim_func = nn.CosineSimilarity(dim=1, eps=1e-8)\n","\n","def compute_metrics_train(pred_hm, gt_gaze, image_size, gt_eye_point, directions, eps = 1e-8):\n","    \n","    AUC = []; min_dist = []; avg_dist = []\n","    avg_ang = []\n","    # go through each data point and record AUC, min dist, avg dist\n","    for b_i in tqdm(range(len(gt_gaze))):\n","        # remove padding and recover valid ground truth points\n","        valid_gaze = gt_gaze[b_i]        \n","        valid_gaze = valid_gaze[valid_gaze != -1].view(-1,2)\n","        \n","        valid_eye_point = gt_eye_point[b_i]\n","        valid_eye_point = valid_eye_point[valid_eye_point != -1].view(-1,2)\n","        \n","        valid_pred_direction = torch.tensor(directions[b_i])\n","        \n","        # AUC: area under curve of ROC\n","        pm = pred_hm[b_i]\n","        multi_hot = multi_hot_targets(gt_gaze[b_i], image_size[b_i])\n","        scaled_heatmap = cv2.resize(pm, (image_size[b_i][0].item(), image_size[b_i][1].item()))\n","        auc_score = evaluation.auc(scaled_heatmap, multi_hot)\n","        AUC.append(auc_score)\n","        # min distance: minimum among all possible pairs of <ground truth point, predicted point>\n","        pred_x, pred_y = evaluation.argmax_pts(pm)\n","        norm_p = [pred_x/float(output_resolution), pred_y/float(output_resolution)]\n","        all_distances = []\n","        for gaze in valid_gaze:\n","            all_distances.append(evaluation.L2_dist(gaze, norm_p))\n","        min_dist.append(min(all_distances))\n","        # average distance: distance between the predicted point and human average point\n","        mean_gt_gaze = torch.mean(valid_gaze, 0)\n","        avg_distance = evaluation.L2_dist(mean_gt_gaze, norm_p)\n","        avg_dist.append(avg_distance)\n","        \n","        mean_gt_gaze_direction = mean_gt_gaze - valid_eye_point\n","        mean_pred_gaze_direction = torch.tensor(norm_p) - valid_eye_point\n","        avg_ang.append(torch.rad2deg(torch.acos(cos_sim_func(mean_gt_gaze_direction,valid_pred_direction))).item())\n","        \n","    return np.array(AUC), np.array(min_dist), np.array(avg_dist), np.abs(np.array(avg_ang))\n","\n","def get_avg_energy_by_gtbox_predheatmap(box, scaled_heatmap, width=640, height=480):\n","    \n","    # Use ground truth box and predicted heatmap to compute the energy aggregation loss\n","    # GT bbox is passed in size (640, 480)\n","    # Check the scale factor orginal = 10 *\n","    power, total_power = 0., 0.\n","    eng = 0.\n","    cur_box = box\n","    cur_heatmap = scaled_heatmap\n","    #make all the negative values as 0\n","    #cur_heatmap = np.maximum(heatmap,0)\n","    # axis are flipped in the heatmap\n","    power = np.sum(cur_heatmap[cur_box[1]: cur_box[3] + 1, cur_box[0]: cur_box[2] + 1])\n","    total_power = cur_heatmap.sum()\n","    if total_power > 0:\n","        eng = (power / total_power) * 100\n","    \n","    return np.array(eng) \n","\n","def get_avg_energy_in_gtcat_predheatmap(all_object_bboxes, scaled_pred_heatmap, gazed_object_class, all_object_bboxes_class, width=640, height=480):\n","    \n","    # Use ground truth boxes and predicted heatmap to compute the energy aggregation loss\n","    # Since GT Object BBoXes overlap hence it is possible that cat_power > total_power\n","    # And cat_eng becomes > 100%\n","    cat_power, total_power = 0., 0.\n","    cat_eng = 0.\n","    cat_boxes = all_object_bboxes[np.where(all_object_bboxes_class == gazed_object_class)[0]]\n","    cur_heatmap = scaled_pred_heatmap\n","    total_power = cur_heatmap.sum()\n","    \n","    #cur_heatmap = np.maximum(heatmap,0)\n","    # axis are flipped in the heatmap\n","    if total_power > 0:\n","        for index in range (len(cat_boxes)):\n","            cur_box = cat_boxes[index]\n","            cat_power += np.sum(cur_heatmap[cur_box[1]: cur_box[3] + 1, cur_box[0]: cur_box[2] + 1])\n","            \n","        cat_eng = np.minimum((cat_power / total_power) * 100 , 100)\n","    \n","    return cat_eng \n","\n","# Iterate through all GT Object category and calc total mean energy of this category. \n","# Predict the Object category with max total mean energy\n","def match_object_cat_after_get_predicted_bbox_from_energy(scaled_pred_heatmap,all_object_bboxes, all_object_bboxes_class, gazed_object_class):\n","    \"\"\"\n","    Use pred heatmap to find the GT object bboxes with maximum energy as gaze target\n","    \"\"\"\n","    max_energy_in_cat = 0.\n","    pred_box_class = None\n","    cur_all_object_bboxes = all_object_bboxes\n","    cur_all_object_bboxes_class = all_object_bboxes_class\n","    # All Bboxes are in original image resolution\n","    cur_pred_heatmap = scaled_pred_heatmap\n","    cur_gazed_object_class = gazed_object_class\n","    \n","    #Object categories in the scene is from 1-24\n","    for cat_ind in range(1,25):\n","        cat_boxes = all_object_bboxes[np.where(all_object_bboxes_class == cat_ind)[0]]\n","        total_mean_energy_in_cat = 0.0\n","        for index in range (len(cat_boxes)):\n","            xmin, ymin, xmax, ymax = cat_boxes[index]\n","         \n","            # axis are flipped in the heatmap\n","            no_of_pixels_in_box = (xmax+1-xmin) * (ymax+1-ymin)\n","        \n","            total_mean_energy_in_cat += np.sum(cur_pred_heatmap[ymin: ymax + 1, xmin: xmax + 1])/(no_of_pixels_in_box)\n","            \n","            \n","        if total_mean_energy_in_cat > max_energy_in_cat:\n","            max_energy_in_cat = total_mean_energy_in_cat\n","            pred_box_class = cat_ind\n","\n","    if pred_box_class is not None:\n","        if int(pred_box_class) == int(cur_gazed_object_class):\n","            return [1.] \n","        else:\n","            return [0.]\n","    else:\n","        return [0.]\n","\n","    \n","def match_object_cat_from_ttffnet_regression(cur_all_result_list_ttfnet, cur_image_size, all_object_bboxes, all_object_bboxes_class, gazed_object_class):\n","    \"\"\"\n","    Returns true if the predicted BBox IoU calculated between the predicted BBoX and any of the GT BBoXes of GT object category is more than 0.5 and \n","    the predicted gazed at object class is same as GT object class.\n","    \"\"\"\n","    for ind, cur_box in enumerate(cur_all_result_list_ttfnet):\n","        \n","        # get predicted BBox in output_resolution x output_resolution\n","        pred_gazed_object_box_from_reg = cur_box[:4]\n","        if pred_gazed_object_box_from_reg[0] == -1:\n","            break\n","        # get predicted BBox in 640x480\n","        pred_gazed_object_box_from_reg_imsize = torch.tensor([pred_gazed_object_box_from_reg[0]*cur_image_size[0]/output_resolution,\n","                                                              pred_gazed_object_box_from_reg[1]*cur_image_size[1]/output_resolution,\n","                                                              pred_gazed_object_box_from_reg[2]*cur_image_size[0]/output_resolution,\n","                                                              pred_gazed_object_box_from_reg[3]*cur_image_size[1]/output_resolution]).int()\n","        pred_gazed_object_box_from_reg_imsize = pred_gazed_object_box_from_reg_imsize.unsqueeze(dim=0)\n","        ious, wuocs = bbox_overlaps_ttfnet(torch.tensor(all_object_bboxes), pred_gazed_object_box_from_reg_imsize, mode='iou', is_aligned=False)\n","        if int(all_object_bboxes_class[torch.argmax(ious,axis=0)]) == int(gazed_object_class) and torch.argmax(ious,axis=0)>0.5:\n","            return [1.]\n","    \n","    return [0.]\n","\n","def compute_metrics(pred_hm, gt_gaze, image_size, gt_eye_point, all_object_bboxes_list, \n","                    all_object_bboxes_class_list, gazed_object_class_list, \n","                    all_result_list_ttfnet, directions, gazed_object_bbox_list, eps = 1e-8):\n","    \n","    AUC = []; min_dist = []; avg_dist = []\n","    avg_ang = []\n","    count_matching_object_class = []\n","    count_matching_object_class_ap_50 = []\n","    avg_pred_hm_energy_in_gt_bbox = []\n","    avg_pred_hm_energy_in_gt_cat = []\n","    # go through each data point and record AUC, min dist, avg dist\n","    for b_i in tqdm(range(len(gt_gaze))):\n","        # remove padding and recover valid ground truth points\n","        valid_gaze = gt_gaze[b_i]        \n","        valid_gaze = valid_gaze[valid_gaze != -1].view(-1,2)\n","        \n","        valid_eye_point = gt_eye_point[b_i]\n","        valid_eye_point = valid_eye_point[valid_eye_point != -1].view(-1,2)\n","        \n","        valid_pred_direction = torch.tensor(directions[b_i])\n","        # AUC: area under curve of ROC\n","        pm = pred_hm[b_i]\n","        multi_hot = multi_hot_targets(gt_gaze[b_i], image_size[b_i])\n","        scaled_heatmap = cv2.resize(pm, (image_size[b_i][0].item(), image_size[b_i][1].item()))\n","        \n","        auc_score = evaluation.auc(scaled_heatmap, multi_hot)\n","        AUC.append(auc_score)\n","        # min distance: minimum among all possible pairs of <ground truth point, predicted point>\n","        pred_x, pred_y = evaluation.argmax_pts(pm)\n","        norm_p = [pred_x/float(output_resolution), pred_y/float(output_resolution)]\n","        all_distances = []\n","        for gaze in valid_gaze:\n","            all_distances.append(evaluation.L2_dist(gaze, norm_p))\n","        min_dist.append(min(all_distances))\n","        # average distance: distance between the predicted point and human average point\n","        mean_gt_gaze = torch.mean(valid_gaze, 0)\n","        avg_distance = evaluation.L2_dist(mean_gt_gaze, norm_p)\n","        avg_dist.append(avg_distance)\n","        \n","        mean_gt_gaze_direction = mean_gt_gaze - valid_eye_point\n","        mean_pred_gaze_direction = torch.tensor(norm_p) - valid_eye_point\n","        avg_ang.append(torch.rad2deg(torch.acos(cos_sim_func(mean_gt_gaze_direction,valid_pred_direction))).item())\n","                            \n","        cur_all_object_bboxes_list = np.array(all_object_bboxes_list[b_i])\n","        cur_all_object_bboxes_class_list = np.array(all_object_bboxes_class_list[b_i])\n","        cur_gazed_object_class = np.array(gazed_object_class_list[b_i])\n","        count_matching_object_class.extend(\n","            match_object_cat_after_get_predicted_bbox_from_energy(\n","                scaled_heatmap,\n","                cur_all_object_bboxes_list, \n","                cur_all_object_bboxes_class_list,\n","                cur_gazed_object_class))\n","        \n","        cur_all_result_list_ttfnet = np.array(all_result_list_ttfnet[b_i])\n","        count_matching_object_class_ap_50.extend(\n","            match_object_cat_from_ttffnet_regression(\n","            cur_all_result_list_ttfnet,\n","            image_size[b_i],\n","            cur_all_object_bboxes_list,\n","            cur_all_object_bboxes_class_list,\n","            cur_gazed_object_class))\n","        \n","        avg_pred_hm_energy_in_gt_bbox.extend([\n","        get_avg_energy_by_gtbox_predheatmap(\n","            gazed_object_bbox_list[b_i],\n","            scaled_heatmap)])\n","        \n","        avg_pred_hm_energy_in_gt_cat.extend([\n","        get_avg_energy_in_gtcat_predheatmap(\n","            cur_all_object_bboxes_list,\n","            scaled_heatmap,\n","            cur_gazed_object_class,\n","            cur_all_object_bboxes_class_list)])\n","        \n","    return np.array(AUC), np.array(min_dist), np.array(avg_dist), np.abs(np.array(avg_ang)),  np.array(count_matching_object_class), np.array(count_matching_object_class_ap_50), np.array(avg_pred_hm_energy_in_gt_bbox), np.array(avg_pred_hm_energy_in_gt_cat)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Test Function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# run inference and returns the metrics\n","def test(model_weights, val_loader, batch_size=48, device=0, mode='dict', save_path=None, flag_print_metrics = True):\n","\n","    # Load model\n","    print(\"Constructing model\")\n","    if mode=='pt':\n","        pretrained_dict = torch.load(model_weights)\n","    elif mode=='dict':\n","        pretrained_dict = model_weights\n","    \n","    if pretrained_dict['modality'] == 'attention':\n","        model_base = AttentionModelCombined(cone_mode=pretrained_dict['cone_mode'], pred_inout=pretrained_dict['pred_inout'])\n","    else:\n","        model_base = BaselineModel(pretrained_dict['backbone_name'], pretrained_dict['modality'], cone_mode=pretrained_dict['cone_mode'], pred_inout=pretrained_dict['pred_inout'])\n","    \n","    model_base.cuda().to(device)\n","    \n","    if pretrained_dict['modality'] == 'attention':\n","        model = attentionModelBboxHead(model_base)\n","    else:\n","        model = baselineModelBboxHead(model_base)\n","        \n","    model.cuda().to(device)\n","    \n","    model_dict = model.state_dict()\n","    model_dict.update(pretrained_dict['model'])\n","    model.load_state_dict(model_dict)\n","    \n","    print('Evaluation in progress ...')\n","    model.train(False)\n","    gt_gaze = []; pred_hm = []; image_size = [] ; paths = []; pred_att = []; directions = []\n","    gt_eye_point = []\n","    all_object_bboxes_list = []\n","    all_object_bboxes_class_list = []\n","    gazed_object_bbox_list = []\n","    gazed_object_class_list = []\n","    all_result_list_ttfnet = []\n","    \n","    with torch.no_grad():\n","        for val_batch, (val_img, val_face, val_pose, val_depth, val_gaze_field, val_gt_direction, val_head_channel, val_gaze_heatmap, cont_gaze, imsize, path, eye_point, all_object_bboxes, all_object_bboxes_class, gazed_object_bbox,gazed_object_class) in tqdm(enumerate(val_loader), total=len(val_loader)):        \n","            val_images = val_img.cuda().to(device)\n","            val_faces = val_face.cuda().to(device)\n","            val_head_channels = val_head_channel.cuda().to(device)\n","            val_gaze_fields = val_gaze_field.cuda().to(device)\n","            val_depth_maps = val_depth.cuda().to(device)\n","            val_pose_maps = val_pose.cuda().to(device)\n","            val_gt_direction = val_gt_direction.cuda().to(device)\n","            \n","            # choose input modality\n","            if pretrained_dict['modality'] == 'image':\n","                model_input = val_images\n","            elif pretrained_dict['modality'] == 'pose':\n","                model_input = val_pose_maps\n","            elif pretrained_dict['modality'] == 'depth':\n","                model_input = val_depth_maps\n","            elif pretrained_dict['modality'] == 'attention':\n","                model_input = [val_images, val_depth_maps, val_pose_maps]\n","                \n","            if pretrained_dict['modality'] == 'attention':\n","                val_gaze_heatmap_pred, direction, val_inout_pred, val_att, val_pred_wh = model(model_input, val_faces, val_gaze_fields, val_head_channels)\n","                pred_att.extend(val_att.cpu().numpy())\n","            else:\n","                val_gaze_heatmap_pred, direction, val_inout_pred, val_pred_wh = model(model_input, val_faces, val_gaze_fields, val_head_channels)\n","            val_gaze_heatmap_pred = val_gaze_heatmap_pred.squeeze(1)\n","                        \n","            gt_gaze.extend(cont_gaze)\n","            gt_eye_point.extend(eye_point)\n","            pred_hm.extend(val_gaze_heatmap_pred.cpu().numpy())\n","            image_size.extend(imsize)\n","            paths.extend(path)\n","            directions.extend(direction.cpu().numpy())\n","            all_object_bboxes_list.extend(all_object_bboxes.cpu().numpy().astype(int)) \n","            all_object_bboxes_class_list.extend(all_object_bboxes_class.cpu().numpy().astype(int)) \n","            \n","            gazed_object_bbox_list.extend(gazed_object_bbox.cpu().numpy().astype(int))\n","            gazed_object_class_list.extend(gazed_object_class.cpu().numpy().astype(int))\n","            \n","            result_list_ttfnet = get_bboxes_ttfnet(val_gaze_heatmap_pred,val_pred_wh)\n","            \n","            if torch.numel(result_list_ttfnet[0][0]) > 0:\n","                all_result_list_ttfnet.extend([result_list_ttfnet[0][0].cpu().numpy()])\n","            else:\n","                all_result_list_ttfnet.extend([np.expand_dims(np.array([-1,-1,-1,-1,-1]), axis=0)])\n","\n","    # Get all the evaluation metrics\n","    AUC, min_dist, avg_dist, avg_ang, count_matching_object_class, count_matching_object_class_ap_50, avg_pred_hm_energy_in_gt_bbox, avg_pred_hm_energy_in_gt_cat = compute_metrics(pred_hm, gt_gaze, image_size, gt_eye_point, all_object_bboxes_list, all_object_bboxes_class_list, gazed_object_class_list, all_result_list_ttfnet, directions, gazed_object_bbox_list)\n","    if save_path is not None:\n","        output = {}\n","        if pretrained_dict['modality'] == 'attention':\n","            output['pred_att'] = pred_att\n","        output['pred_hm'] = pred_hm; output['gt_gaze'] = gt_gaze; output['paths'] = paths\n","        output['AUC'] = AUC; output['min_dist'] = min_dist; output['avg_dist'] = avg_dist; output['direction'] = directions\n","        output['avg_ang'] = avg_ang\n","        output['count_matching_object_class'] = count_matching_object_class\n","        output['count_matching_object_class_ap_50'] = count_matching_object_class_ap_50\n","        output['avg_pred_hm_energy_in_gt_bbox'] = avg_pred_hm_energy_in_gt_bbox\n","        output['avg_pred_hm_energy_in_gt_cat'] = avg_pred_hm_energy_in_gt_cat\n","        with open(os.path.join(save_path, 'output_gazefollow.pkl'), 'wb') as fp:\n","            pickle.dump(output, fp)    \n","            \n","    final_AUC = torch.mean(torch.tensor(AUC))\n","    final_min_dist = torch.mean(torch.tensor(min_dist))\n","    final_avg_dist = torch.mean(torch.tensor(avg_dist))\n","    final_avg_ang = torch.mean(torch.tensor(avg_ang))\n","    final_count_matching_object_class = torch.mean(torch.tensor(count_matching_object_class)) * 100\n","    final_count_matching_object_class_ap_50 = torch.mean(torch.tensor(count_matching_object_class_ap_50)) * 100\n","    final_avg_pred_hm_energy_in_gt_bbox = np.mean(avg_pred_hm_energy_in_gt_bbox)\n","    final_avg_pred_hm_energy_in_gt_cat = np.mean(avg_pred_hm_energy_in_gt_cat)\n","    if flag_print_metrics:\n","        if pretrained_dict['modality'] == 'attention':\n","            avg_attention_weights = [sum(x) / len(x) for x in zip(*pred_att)]\n","            print(f'Avg. Attention weights image | depth | pose: {avg_attention_weights[0]} | {avg_attention_weights[1]} | {avg_attention_weights[2]}')\n","        print(\"\\tAUC:{:.4f}\\t min dist:{:.4f}\\t avg dist:{:.4f}\\t avg ang:{:.4f}\\t Object prediction Acc.(%):{:.4f}\\t BBoX head Object prediction Acc.(%):{:.4f} \\t avg_pred_hm_energy_in_gt_bbox(%): {:.4f} \\t avg_pred_hm_energy_in_gt_cat(%): {:.4f}\".format(\n","            final_AUC,\n","            final_min_dist,\n","            final_avg_dist,\n","            final_avg_ang,\n","            final_count_matching_object_class,\n","            final_count_matching_object_class_ap_50,\n","            final_avg_pred_hm_energy_in_gt_bbox,\n","            final_avg_pred_hm_energy_in_gt_cat))\n","        \n","    return final_AUC, final_min_dist, final_avg_dist, final_avg_ang, final_count_matching_object_class, final_count_matching_object_class_ap_50, final_avg_pred_hm_energy_in_gt_bbox\n"]},{"cell_type":"markdown","metadata":{},"source":["## Training Function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T17:23:42.423470Z","iopub.status.busy":"2023-04-27T17:23:42.423102Z","iopub.status.idle":"2023-04-27T17:28:23.342746Z","shell.execute_reply":"2023-04-27T17:28:23.341210Z","shell.execute_reply.started":"2023-04-27T17:23:42.423433Z"},"trusted":true},"outputs":[],"source":["#### Train config\n","\n","# set seeds\n","np.random.seed(42)\n","torch.manual_seed(42)\n","\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","device = ('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","init_baseline_model_weights = ''  # path to baseline gaze detection model weights\n","init_extended_model_weights = ''  # path to extended model weights\n","\n","lr = 2.5e-4       # learning rate\n","batch_size = 4     # training batch size\n","val_batch_size = 1  # validation batch size\n","epochs = 40         # total number of epochs for training \n","eval_every = 1      # number of epochs after which we evaluate on the validation set\n","save_every = 1      # number of epoch after which we save the checkpoints for the model\n","log_dir = '.'\n","\n","def train():\n","    transform = _get_transform()\n","    transform_modality = _get_transform_modality()\n","    \n","    \n","    cone_mode = 'early'    # {'late', 'early'} fusion of person information\n","\n","    pred_inout = False     # For every image of the GOO dataset, the target is inside the image \n","\n","\n","    # Prepare data\n","    print(\"Loading Data\")\n","    if use_only_real_dataset_for_train:\n","        train_dataset = GooRealDataset(df_train, transform, transform_modality, \n","                                   input_size=input_resolution, output_size=output_resolution, \n","                                   modality=modality)\n","        train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                                   batch_size=batch_size,\n","                                                   shuffle=True,\n","                                                   num_workers=2)\n","\n","        val_dataset = GooRealDataset(df_val, transform, transform_modality, \n","                                 input_size=input_resolution, output_size=output_resolution,\n","                                 modality=modality, test=True)\n","        val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n","                                                   batch_size=val_batch_size,\n","                                                   shuffle=True,\n","                                                   num_workers=2)\n","    else:\n","        train_dataset = GooRealPlusSynthDataset(df_train, transform, transform_modality, \n","                                   input_size=input_resolution, output_size=output_resolution, \n","                                   modality=modality)\n","        train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                                   batch_size=batch_size,\n","                                                   shuffle=True,\n","                                                   num_workers=2)\n","\n","        val_dataset = GooRealPlusSynthDataset(df_val, transform, transform_modality, \n","                                 input_size=input_resolution, output_size=output_resolution,\n","                                 modality=modality, test=True)\n","        val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n","                                                   batch_size=val_batch_size,\n","                                                   shuffle=True,\n","                                                   num_workers=2)\n","    # Set up log dir\n","    logdir = os.path.join(log_dir,\n","                          datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n","    os.makedirs(logdir)\n","    \n","    # Load base model\n","    print(\"Constructing model\")\n","    if modality == 'attention':\n","        model = AttentionModelCombined(cone_mode='early', pred_inout=False)\n","    elif modality in ['image', 'depth', 'pose']:\n","        model = BaselineModel(backbone_name, modality, cone_mode='early', pred_inout=False)\n","    model.cuda().to(device)\n","    \n","    # if baseline model weights are available then load the weights of this model\n","    if init_baseline_model_weights:\n","        \n","        model_dict = model.state_dict()\n","        pretrained_dict = torch.load(init_baseline_model_weights)\n","        # Run this if size mismatch\n","        #model.on_load_checkpoint(pretrained_dict)\n","        pretrained_model_dict = pretrained_dict['model']      \n","        model_dict.update(pretrained_model_dict)\n","        model.load_state_dict(model_dict,strict = False)\n","        print('loaded model pretrained weights (strict = False)')\n","        \n","        #pretrained_opt_dict = pretrained_dict.get('optimizer', None)\n","        #if pretrained_opt_dict is not None:\n","        #    print('loading optimizer state dict')\n","        #    optimizer.load_state_dict(pretrained_opt_dict)\n","    \n","    # after loading the baseline model, get the extended model\n","    if modality == 'attention':\n","        extended_model = attentionModelBboxHead(model)\n","    elif modality in ['image', 'depth', 'pose']:\n","        extended_model = baselineModelBboxHead(model)\n","        \n","    \n","    extended_model.cuda().to(device)\n","    \n","    # load pretrained weights for the extended model\n","    if init_extended_model_weights:\n","        \n","        model_dict_extended = extended_model.state_dict()\n","        pretrained_dict_extended = torch.load(init_extended_model_weights)\n","        pretrained_model_dict_extended = pretrained_dict_extended['model']  \n","        model_dict_extended.update(pretrained_model_dict_extended)\n","        extended_model.load_state_dict(model_dict_extended)\n","        print('loaded model pretrained weights for extended model')\n","        #pretrained_opt_dict = pretrained_dict_extended.get('optimizer', None)\n","        #if pretrained_opt_dict is not None:\n","        #    print('loading optimizer state dict')\n","        #    optimizer.load_state_dict(pretrained_opt_dict)\n","    \n","    \n","    # Optimizer configurations\n","    # Just like in the baseline model traning script, we want to train the model backbone with a lower learning rate\n","    if modality == 'attention':\n","        reduced_lr_list = ['feature_extractor_image.backbone', 'feature_extractor_depth.backbone', 'feature_extractor_pose.backbone', 'human_centric.backbone']\n","        #reduced_lr_list = []\n","    else:\n","        reduced_lr_list = []\n","    params_non_backbone = []\n","    params_backbone = []\n","    for kv in extended_model.named_parameters():\n","        flag = 0\n","        for lname in reduced_lr_list:\n","            if lname in kv[0]:\n","                flag = 1\n","        if flag:\n","            params_backbone.append(kv[1])\n","        else:\n","            params_non_backbone.append(kv[1])\n","    \n","    optimizer = torch.optim.AdamW([{'params': params_non_backbone},{'params': params_backbone, 'lr': lr/10}], lr=lr)\n","    \n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n","    \n","    # Intiliaztions of the training loop\n","    start_ep = 0\n","    # Loss functions \n","    mse_loss = nn.MSELoss(reduce=False) # not reducing in order to ignore outside cases\n","    step = 0    \n","    loss_amp_factor = 1000 # multiplied to the loss to prevent underflow\n","    dir_loss_factor = 1\n","    ttfnet_wh_loss_amp_factor = 50\n","    energy_agg_loss_factor = 1\n","    \n","    max_steps = len(train_loader)\n","    optimizer.zero_grad()\n","    \n","    # AMP\n","    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n","    \n","    best_AUC = 0; best_min_dist = 1; best_avg_dist = 1\n","    \n","    # start of the training loop\n","    print(\"Training in progress ...\")\n","    with torch.cuda.amp.autocast(enabled=use_amp):\n","        for ep in range(start_ep, epochs):\n","            for batch, (img, face, pose, depth, gaze_field, gt_direction, head_channel, gaze_heatmap, path, gaze_inside, dropped, box_target_cp, reg_weight_cp, gazed_object_bbox, height, width) in tqdm(enumerate(train_loader),  total=len(train_loader)):\n","                extended_model.train(True)        \n","\n","                images = img.cuda().to(device)\n","                #plt.imshow(img[0,:,:,:].detach().cpu().numpy().transpose(1,2,0))\n","                #plt.show()\n","\n","                faces = face.cuda().to(device)\n","                head_channels = head_channel.cuda().to(device)\n","                gaze_fields = gaze_field.cuda().to(device)\n","                depth_maps = depth.cuda().to(device)\n","                pose_maps = pose.cuda().to(device)\n","                gaze_heatmap = gaze_heatmap.cuda().to(device)\n","                gt_direction = gt_direction.cuda().to(device)\n","                dropped = dropped.cuda().to(device)\n","                box_target_cp = box_target_cp.cuda().to(device)\n","                reg_weight_cp = reg_weight_cp.cuda().to(device)\n","                gazed_object_bbox = gazed_object_bbox.cuda().to(device)\n","                height = height.cuda().to(device)\n","                width =  width.cuda().to(device)\n","                gaze_inside = gaze_inside.cuda().to(device)\n","                # choose input modality\n","                if modality == 'image':\n","                    model_input = images\n","                elif modality == 'pose':\n","                    model_input = pose_maps\n","                elif modality == 'depth':\n","                    model_input = depth_maps\n","                elif modality == 'attention':\n","                    model_input = [images, depth_maps, pose_maps]\n","                if modality == 'attention':\n","                    gaze_heatmap_pred, direction, inout_pred, att, pred_wh = extended_model(model_input, faces, gaze_fields, head_channels)\n","                else:\n","                    gaze_heatmap_pred, direction, inout_pred, pred_wh = extended_model(model_input, faces, gaze_fields, head_channels)\n","                gaze_heatmap_pred = gaze_heatmap_pred.squeeze(1)\n","                #gaze_heatmap_pred = torch.sigmoid(gaze_heatmap_pred)\n","                 \n","                # Loss\n","                # l2 loss for predicted heatmap\n","                l2_loss = mse_loss(gaze_heatmap_pred, gaze_heatmap)*loss_amp_factor\n","                l2_loss = torch.mean(l2_loss, dim=1)\n","                l2_loss = torch.mean(l2_loss, dim=1)\n","                gaze_inside = gaze_inside.cuda(device).to(torch.float)\n","                l2_loss = torch.mul(l2_loss, gaze_inside) # zero out loss when it's out-of-frame gaze case\n","                l2_loss = torch.sum(l2_loss)/torch.sum(gaze_inside)\n","                \n","                # cross entropy loss for in vs out\n","                Xent_loss = 0\n","                #if pred_inout:\n","                #    Xent_loss = bcelogit_loss(inout_pred.squeeze(), gaze_inside.squeeze())\n","        \n","                # cosine loss on predicted direction\n","                dir_loss = 0\n","                dir_loss = 1- cos_sim_func(direction, gt_direction)\n","                dir_loss = dir_loss * gaze_inside # zero out loss when it's out-of-frame gaze case\n","                dir_loss = dir_loss.sum()*dir_loss_factor\n","                \n","                # attention loss on dropped modalities\n","                att_loss = 0\n","                if modality=='attention':\n","                    att_masked = att.squeeze() * dropped\n","                    att_loss = att_masked.sum()\n","                \n","                # energy aggregation loss\n","                # GT bbox is passed in size (640, 480)\n","                energy_agg_loss = 0\n","                energy_agg_loss = compute_heatmap_loss_by_gtbox_predheatmap(gazed_object_bbox,gaze_heatmap_pred, width, height) * energy_agg_loss_factor\n","                \n","                \n","                ttfnet_hm_loss,ttfnet_wh_loss = loss_calc_ttfnet(pred_hm=gaze_heatmap_pred, pred_wh=pred_wh, heatmap=gaze_heatmap , box_target=box_target_cp,wh_weight=reg_weight_cp)\n","                ttfnet_wh_loss = ttfnet_wh_loss_amp_factor * ttfnet_wh_loss\n","                \n","                #total_loss = l2_loss + Xent_loss + dir_loss + att_loss\n","                                \n","                total_loss = l2_loss + dir_loss + att_loss + ttfnet_hm_loss + ttfnet_wh_loss \n","                \n","                # NOTE: Xent_loss is used to train the main model.\n","                #       No Xent_loss is used to get SOTA on GazeFollow benchmark.\n","\n","                scaler.scale(total_loss).backward()\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad()\n","                step += 1\n","                \n","            print(\"Epoch:{:04d}\\tstep:{:06d}/{:06d}\\t training loss: (L2){:.4f} (Xent){:.4f} (dir){:.4f} (att){:.4f}\".format(\n","                    ep, batch+1, max_steps, l2_loss.item(), Xent_loss, dir_loss.item(), att_loss))\n","            print(\"ttfnet hm loss: {:4f}\".format(ttfnet_hm_loss.item()))\n","            print(\"ttfnet wh loss: {:4f}\".format(ttfnet_wh_loss.item()))\n","            print(\"energy agg loss: {:4f}\".format(energy_agg_loss.item())) \n","                        \n","            if (ep % eval_every == 0):\n","                print('Validation in progress ...')\n","                extended_model.train(False)\n","                checkpoint = {'model': extended_model.state_dict(),\n","                              'backbone_name': backbone_name, \n","                              'modality': modality,\n","                              'cone_mode': cone_mode,\n","                              'pred_inout': pred_inout}\n","                final_AUC, final_min_dist, final_avg_dist, final_avg_ang, final_count_matching_object_class, final_count_matching_object_class_ap_50, final_avg_pred_hm_energy_in_gt_bbox = test(checkpoint, val_loader,batch_size=val_batch_size)\n","\n","            \n","            if (ep % save_every == 0):\n","                print('saving model ...')                \n","                # save the model\n","                checkpoint = {'epoch': ep,\n","                              'optimizer': optimizer.state_dict(),\n","                              'model': extended_model.state_dict(),\n","                              'backbone_name': backbone_name, \n","                              'modality': modality,\n","                              'cone_mode': cone_mode,\n","                              'pred_inout': pred_inout}\n","                torch.save(checkpoint, os.path.join(logdir, 'epoch_'+str(ep)+'.pt'))\n","            \n","            scheduler.step()\n","\n","if __name__ == \"__main__\":\n","    train()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":4}
